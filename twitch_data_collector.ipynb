{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Twitch Insight: Data Collector & Collaboration Network Analyzer (v3 - Enhanced Prints)\n",
    "\n",
    "This notebook automates the process of collecting data from the Twitch API, store it in a local SQLite database, and perform analysis, with a special focus on understanding collaboration networks between streamers. It identifies potential collaborations by detecting `@mentions` in video titles and descriptions, and also explores community structures within these networks. The primary interface for data collection, processing, and exploration is a Jupyter Notebook.\n",
    "\n",
    "**Key Features:**\n",
    "- Fetches top streams/categories periodically.\n",
    "- Fetches channel details and video archives.\n",
    "- Processes video titles/descriptions for `@mentions`.\n",
    "- Looks up mentioned users via API if not in the database.\n",
    "- Stores collaboration data (frequency, duration, recency).\n",
    "- Processes mentions atomically per-video using DB transactions.\n",
    "- Includes a refresh cycle for updating random channels.\n",
    "- Provides data exploration for channels, videos, and the collaboration network.\n",
    "\n",
    "**Modules Used:**\n",
    "- `config.py`: Configuration (API keys, constants). Requires `.env` file.\n",
    "- `database.py`: SQLite database interactions (schema, CRUD, upserts).\n",
    "- `twitch_api.py`: Twitch Helix API communication (auth, rate limits).\n",
    "- `network_utils.py`: Mention extraction and validation."
   ],
   "id": "5e517ca68d15737a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import re  # For parsing duration\n",
    "import math  # For log scale checks\n",
    "import random  # For refresh cycle\n",
    "import networkx as nx  # For graph analysis\n",
    "from ipywidgets import Text, Button, Dropdown, Output, VBox, Layout  # For interactive viz\n",
    "from IPython.display import display, HTML  # For displaying widgets and potentially HTML\n",
    "\n",
    "# Matplotlib will try this list of fonts in order until it finds one on your system.\n",
    "# This list includes common fonts for Japanese, Chinese, and Korean on Windows/macOS.\n",
    "plt.rcParams['font.family'] = ['Meiryo', 'Malgun Gothic', 'SimHei', 'Apple SD Gothic Neo', 'sans-serif']\n",
    "\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "\n",
    "# --- Local Notebook Configuration ---\n",
    "# Set to True for detailed progress and time estimations, False for minimal output.\n",
    "VERBOSE_MODE = True\n",
    "# Set to True to use channel profile pictures as nodes in graphs (slower).\n",
    "# Set to False to use simple colored circles (faster).\n",
    "USE_IMAGE_NODES = True\n",
    "\n",
    "# Import custom modules\n",
    "try:\n",
    "    import config\n",
    "    import database\n",
    "    import twitch_api\n",
    "    import network_utils\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing custom modules: {e}\")\n",
    "    print(\"Please ensure config.py, database.py, twitch_api.py, and network_utils.py are in the same directory.\")\n",
    "    raise SystemExit(\"Stopping notebook due to missing modules.\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(module)s - %(message)s')\n",
    "\n",
    "# --- Print Configuration ---\n",
    "config.print_config()  # Call the function from config.py\n",
    "print(f\"\\n[Notebook Settings]\\n  VERBOSE_MODE: {VERBOSE_MODE}\")\n",
    "\n",
    "# --- Initialize ---\n",
    "db_conn = None\n",
    "api_client = None\n",
    "try:\n",
    "    print(\"\\nInitializing database connection...\")\n",
    "    # Use settings from config.py\n",
    "    db_conn = database.get_db_connection(config.DATABASE_NAME)\n",
    "    database.initialize_database(db_conn)\n",
    "    print(f\"Database '{config.DATABASE_NAME}' initialized.\")\n",
    "\n",
    "    # Graceful API Client Initialization\n",
    "    print(\"Initializing Twitch API client...\")\n",
    "    api_client = twitch_api.TwitchAPIClient(\n",
    "        client_id=config.TWITCH_CLIENT_ID,\n",
    "        client_secret=config.TWITCH_CLIENT_SECRET,\n",
    "        auth_url=config.TWITCH_AUTH_URL,\n",
    "        base_url=config.TWITCH_API_BASE_URL\n",
    "    )\n",
    "    if not api_client._authenticate():\n",
    "         raise ConnectionError(\"Failed to authenticate with Twitch API.\")\n",
    "    print(\"API Client initialized and authenticated.\")\n",
    "\n",
    "except ValueError as e:\n",
    "    # This will catch the credential error from the API client's __init__\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"WARNING: {e}\")\n",
    "    print(\"API Client could not be initialized. Data collection cells will be skipped.\")\n",
    "    print(\"Analysis of existing data in the database is still possible.\")\n",
    "    print(\"=\"*50)\n",
    "except Exception as e:\n",
    "    logging.critical(f\"Initialization failed: {e}\", exc_info=True)\n",
    "    print(f\"Initialization failed: {e}\")\n",
    "    if db_conn:\n",
    "        db_conn.close()\n",
    "    raise SystemExit(\"Stopping notebook due to initialization failure.\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Setup Complete. Current time: {datetime.now().strftime('%Y-%M:%S')}\")\n",
    "print(\"-\" * 30)"
   ],
   "id": "e5ca9c4d759b9375",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 2: Helper Functions\n",
    "\n",
    "def parse_duration_for_collab(duration_str):\n",
    "    \"\"\"\n",
    "    Parses a Twitch duration string (e.g., \"1h30m20s\") into total seconds.\n",
    "    \"\"\"\n",
    "    if not duration_str or not isinstance(duration_str, str):\n",
    "        return 0 # Return 0 for invalid input\n",
    "\n",
    "    total_seconds = 0\n",
    "    hours = re.search(r'(\\d+)h', duration_str)\n",
    "    minutes = re.search(r'(\\d+)m', duration_str)\n",
    "    seconds = re.search(r'(\\d+)s', duration_str)\n",
    "\n",
    "    if hours:\n",
    "        total_seconds += int(hours.group(1)) * 3600\n",
    "    if minutes:\n",
    "        total_seconds += int(minutes.group(1)) * 60\n",
    "    if seconds:\n",
    "        total_seconds += int(seconds.group(1))\n",
    "\n",
    "    return total_seconds\n",
    "\n",
    "def format_seconds_to_hm(seconds):\n",
    "    \"\"\"\n",
    "    Formats a duration in seconds into a human-readable \"Xh Ym\" string.\n",
    "    \"\"\"\n",
    "    if pd.isna(seconds) or seconds < 0:\n",
    "        return \"N/A\"\n",
    "\n",
    "    seconds = int(seconds)\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "\n",
    "    return f\"{hours}h {minutes:02}m\"\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ],
   "id": "d87a6db4e65288e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell: Advanced Graph Drawing Helper Functions\n",
    "\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from PIL import Image\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# --- Image Caching and Drawing Logic ---\n",
    "\n",
    "# Create a directory for caching images if it doesn't exist\n",
    "IMAGE_CACHE_DIR = \"image_cache\"\n",
    "if not os.path.exists(IMAGE_CACHE_DIR):\n",
    "    os.makedirs(IMAGE_CACHE_DIR)\n",
    "\n",
    "def get_cached_image(url, channel_id):\n",
    "    \"\"\"\n",
    "    Downloads an image from a URL and caches it locally.\n",
    "    Returns a Pillow Image object.\n",
    "    \"\"\"\n",
    "    if not url or not isinstance(url, str): return None\n",
    "    try:\n",
    "        # Use a consistent filename based on channel ID\n",
    "        filename = f\"{channel_id}.png\"\n",
    "        filepath = os.path.join(IMAGE_CACHE_DIR, filename)\n",
    "\n",
    "        if os.path.exists(filepath):\n",
    "            # Load from cache\n",
    "            return Image.open(filepath)\n",
    "        else:\n",
    "            # Download, save to cache, then return\n",
    "            response = requests.get(url, stream=True, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            img = Image.open(response.raw).convert(\"RGBA\")\n",
    "            img.save(filepath, 'PNG')\n",
    "            return img\n",
    "    except Exception as e:\n",
    "        # logging.warning(f\"Could not download or process image for channel {channel_id} from {url}: {e}\")\n",
    "        return None # Return None on any error\n",
    "\n",
    "def draw_network_with_images(G, pos, node_data, community_partition, labels, ax):\n",
    "    \"\"\"\n",
    "    Draws a networkx graph on a matplotlib axis, using channel profile pictures for nodes\n",
    "    and correct text labels.\n",
    "    \"\"\"\n",
    "    # Draw edges first, so they appear behind the nodes\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.3, edge_color='gray', ax=ax)\n",
    "\n",
    "    # Prepare community colors\n",
    "    unique_community_ids = sorted(list(set(community_partition.values())))\n",
    "    community_to_color_idx = {cid: i for i, cid in enumerate(unique_community_ids)}\n",
    "    cmap = plt.cm.get_cmap('tab20', len(unique_community_ids))\n",
    "\n",
    "    # Draw each node as an image with a colored border\n",
    "    for node_id in G.nodes():\n",
    "        community_id = community_partition.get(node_id, -1)\n",
    "        node_color = 'grey' if community_id == -1 else cmap(community_to_color_idx.get(community_id))\n",
    "\n",
    "        node_info = node_data.loc[node_id] if node_id in node_data.index else None\n",
    "        if node_info is None: continue\n",
    "\n",
    "        follower_count = max(1, node_info.get('follower_count', 1))\n",
    "\n",
    "        # Calculate size and zoom factors based on followers\n",
    "        size_factor = math.log10(follower_count + 1)\n",
    "        image_zoom = 0.02 * size_factor\n",
    "        border_size = 0.025 * size_factor\n",
    "\n",
    "        # 1. Draw the colored circle for the border\n",
    "        ax.add_patch(plt.Circle(pos[node_id], radius=border_size, color=node_color, zorder=1))\n",
    "\n",
    "        # 2. Get the cached profile picture\n",
    "        img = get_cached_image(node_info.get('profile_image_url'), node_id)\n",
    "\n",
    "        if img:\n",
    "            # 3. Place the image on the graph\n",
    "            imagebox = OffsetImage(img, zoom=image_zoom, resample=True)\n",
    "            imagebox.image.axes = ax\n",
    "            ab = AnnotationBbox(imagebox, pos[node_id], frameon=False, pad=0.0, zorder=2)\n",
    "            ax.add_artist(ab)\n",
    "\n",
    "    # Draw node labels using the provided `labels` dictionary\n",
    "    nx.draw_networkx_labels(G, pos, labels=labels, font_size=8, ax=ax,\n",
    "                            bbox=dict(facecolor='white', alpha=0.5, edgecolor='none', boxstyle='round,pad=0.2'))\n",
    "\n",
    "\n",
    "print(\"Advanced graph drawing helper function 'draw_network_with_images' is now defined.\")"
   ],
   "id": "8eebad6488d37da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Collection Seeding\n",
    "\n",
    "This section is responsible for seeding the database with channels and their videos by fetching top streams and channels. It can be run to initialize an empty database, or to discover new channels that are not a part of the social network of any known channel.\n"
   ],
   "id": "c18bda92add56995"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 2: Data Collection Cycle Function & Execution (Top Streams Focus)\n",
    "def run_collection_cycle(current_api_client, current_db_conn):\n",
    "    \"\"\"\n",
    "    Runs one cycle focused on fetching top streams, channels, and their videos.\n",
    "    Mention processing is handled separately.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Starting Top Stream Data Collection Cycle at {datetime.now().strftime('%H:%M:%S')} ===\")\n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    # Phase 1: Fetch Top Categories\n",
    "    phase_start_time = time.time()\n",
    "    print(\"\\n--- Phase 1: Fetching and Processing Top Categories ---\")\n",
    "    top_categories = current_api_client.get_top_games(config.NUM_TOP_CATEGORIES)\n",
    "    if not top_categories: print(\"Could not fetch top categories. Cycle aborted.\"); return False\n",
    "    database.save_categories(current_db_conn, top_categories)\n",
    "    print(f\"Phase 1: Processed {len(top_categories)} top categories in {time.time() - phase_start_time:.2f}s.\")\n",
    "\n",
    "    # Phase 2: Fetch Top Streams & Identify Channels\n",
    "    phase_start_time = time.time()\n",
    "    print(\"\\n--- Phase 2: Fetching Top Streams & Identifying Channels from Categories ---\")\n",
    "    channels_to_process = set()\n",
    "    categories_to_scan = database.get_categories_to_scan(current_db_conn, config.NUM_TOP_CATEGORIES)\n",
    "    total_categories_to_scan = len(categories_to_scan)\n",
    "    print(f\"Found {total_categories_to_scan} categories prioritized for scanning.\")\n",
    "    category_processing_times = []\n",
    "\n",
    "    for i, category_row in enumerate(categories_to_scan):\n",
    "        cat_start_time = time.time()\n",
    "        category_id = category_row['id']\n",
    "        category_name = category_row['name']\n",
    "        print(f\" ({i + 1}/{total_categories_to_scan}) Processing category: {category_name}...\")\n",
    "        streams = current_api_client.get_streams_for_game(category_id, config.NUM_STREAMS_PER_CATEGORY)\n",
    "        if streams:\n",
    "            stream_channel_ids = set()\n",
    "            for stream in streams:\n",
    "                if 'user_id' in stream and 'user_login' in stream and 'user_name' in stream:\n",
    "                    if database.save_channel_basic(current_db_conn, {\n",
    "                        'id': stream['user_id'], 'login': stream['user_login'], 'display_name': stream['user_name']\n",
    "                    }): stream_channel_ids.add(stream['user_id'])\n",
    "            channels_to_process.update(stream_channel_ids)\n",
    "        database.update_category_scan_time(current_db_conn, category_row['id'])\n",
    "\n",
    "        cat_duration = time.time() - cat_start_time\n",
    "        category_processing_times.append(cat_duration)\n",
    "        if VERBOSE_MODE and category_processing_times and total_categories_to_scan > 0:\n",
    "            avg_time_per_cat = sum(category_processing_times) / len(category_processing_times)\n",
    "            est_remaining_cat_secs = (total_categories_to_scan - (i + 1)) * avg_time_per_cat\n",
    "            if est_remaining_cat_secs > 0 and i < total_categories_to_scan - 1:\n",
    "                est_cat_mins, est_cat_s = divmod(int(est_remaining_cat_secs), 60)\n",
    "                print(f\" -> Processed in {cat_duration:.2f}s. Est. remaining for categories: {est_cat_mins}m {est_cat_s}s\")\n",
    "            else:\n",
    "                print(f\" -> Processed in {cat_duration:.2f}s.\")\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    print(\n",
    "        f\"\\nPhase 2: Identified {len(channels_to_process)} unique channels. Took {time.time() - phase_start_time:.2f}s.\")\n",
    "\n",
    "    # Phase 3: Fetch/Update Channel Details (MODIFIED for one-by-one fetching)\n",
    "    phase_start_time = time.time()\n",
    "    print(\"\\n--- Phase 3: Fetching/Updating Channel Details (including Followers) ---\")\n",
    "    processed_channels_details = 0\n",
    "    channels_needing_details_update = [\n",
    "        chan_id for chan_id in list(channels_to_process)\n",
    "        if database.check_channel_needs_update(current_db_conn, chan_id, config.REFETCH_CHANNEL_DETAILS_DAYS)\n",
    "    ]\n",
    "    total_to_update = len(channels_needing_details_update)\n",
    "    print(f\"{total_to_update} channels require detail fetching/updating.\")\n",
    "    detail_fetch_times = []\n",
    "\n",
    "    if total_to_update > 0:\n",
    "        for i, channel_id in enumerate(channels_needing_details_update):\n",
    "            item_start_time = time.time()\n",
    "            print(f\" ({i + 1}/{total_to_update}) Fetching details for channel ID: {channel_id}...\")\n",
    "\n",
    "            # 1. Get user details (like login, description)\n",
    "            user_details_list = current_api_client.get_user_details(user_ids=[channel_id])\n",
    "\n",
    "            if user_details_list:\n",
    "                user_data = user_details_list[0] # Get the first (and only) result\n",
    "\n",
    "                # 2. Get follower count (separate API call)\n",
    "                follower_count = current_api_client.get_channel_follower_count(broadcaster_id=channel_id)\n",
    "                if follower_count is not None:\n",
    "                    user_data['follower_count'] = follower_count\n",
    "\n",
    "                # 3. Save combined data\n",
    "                try:\n",
    "                    database.save_channel_details(current_db_conn, user_data)\n",
    "                    processed_channels_details += 1\n",
    "                except Exception as e:\n",
    "                    print(f\" -> DB Error saving details for {user_data.get('login', channel_id)}: {e}\")\n",
    "\n",
    "            elif user_details_list is None:\n",
    "                print(f\" -> API call failed for details of channel {channel_id}. Skipping.\")\n",
    "\n",
    "            # Time estimation logic\n",
    "            item_duration = time.time() - item_start_time\n",
    "            detail_fetch_times.append(item_duration)\n",
    "            if VERBOSE_MODE and detail_fetch_times:\n",
    "                avg_time = sum(detail_fetch_times) / len(detail_fetch_times)\n",
    "                est_rem_secs = (total_to_update - (i + 1)) * avg_time\n",
    "                if est_rem_secs > 0:\n",
    "                    est_mins, est_s = divmod(int(est_rem_secs), 60)\n",
    "                    print(f\" -> Processed in {item_duration:.2f}s. Est. remaining: {est_mins}m {est_s}s\")\n",
    "\n",
    "            time.sleep(0.3) # API courtesy between channels\n",
    "\n",
    "    print(f\"Phase 3: Finished. Attempted save for {processed_channels_details} channels. Took {time.time() - phase_start_time:.2f}s.\")\n",
    "\n",
    "    # Phase 4: Fetch/Update Channel Videos\n",
    "    phase_start_time = time.time()\n",
    "    print(\"\\n--- Phase 4: Checking for and Fetching New Videos ---\")\n",
    "    channels_for_video_fetch = list(channels_to_process)\n",
    "    total_channels_for_video = len(channels_for_video_fetch)\n",
    "    print(f\"Checking for new videos for {total_channels_for_video} channels from this cycle...\")\n",
    "    processed_channels_videos = 0; new_videos_found_total = 0\n",
    "    video_fetch_times = []\n",
    "\n",
    "    if total_channels_for_video > 0:\n",
    "        for i, channel_id in enumerate(channels_for_video_fetch):\n",
    "            ch_video_start_time = time.time()\n",
    "            channel_info_for_log_cursor = current_db_conn.execute(\"SELECT login FROM Channels WHERE id = ?\", (channel_id,))\n",
    "            channel_info_for_log = channel_info_for_log_cursor.fetchone()\n",
    "            channel_log_name = channel_info_for_log['login'] if channel_info_for_log else channel_id\n",
    "\n",
    "            print(f\" ({i + 1}/{total_channels_for_video}) Checking videos for channel: {channel_log_name}...\")\n",
    "            latest_stored_date = database.get_latest_video_date_for_channel(current_db_conn, channel_id)\n",
    "            # This call fetches videos for channels just found in top streams\n",
    "            new_videos = current_api_client.get_channel_videos(\n",
    "                channel_id,\n",
    "                video_type='archive',\n",
    "                # Limit how many new videos to grab for a newly seen channel.\n",
    "                # Helps prevent a single new channel from dominating the cycle time.\n",
    "                limit=100,\n",
    "                after_date=latest_stored_date\n",
    "            )\n",
    "\n",
    "            if new_videos:\n",
    "                if VERBOSE_MODE: print(f\" -> Found {len(new_videos)} new archive videos.\")\n",
    "                database.save_videos(current_db_conn, new_videos); new_videos_found_total += len(new_videos)\n",
    "            elif new_videos is None:\n",
    "                print(f\" -> API call failed fetching videos for {channel_log_name}.\")\n",
    "\n",
    "            if new_videos is not None: database.update_channel_video_fetch_time(current_db_conn, channel_id)\n",
    "            processed_channels_videos += 1\n",
    "\n",
    "            ch_video_duration = time.time() - ch_video_start_time\n",
    "            video_fetch_times.append(ch_video_duration)\n",
    "            if VERBOSE_MODE and video_fetch_times:\n",
    "                avg_time_per_ch_video = sum(video_fetch_times) / len(video_fetch_times)\n",
    "                est_remaining_vid_secs = (total_channels_for_video - (i + 1)) * avg_time_per_ch_video\n",
    "                if est_remaining_vid_secs > 0 and i < total_channels_for_video - 1:\n",
    "                    est_vid_mins, est_vid_s = divmod(int(est_remaining_vid_secs), 60)\n",
    "                    print(f\" -> Processed in {ch_video_duration:.2f}s. Est. remaining for video checks: {est_vid_mins}m {est_vid_s}s\")\n",
    "                else:\n",
    "                    print(f\" -> Processed in {ch_video_duration:.2f}s.\")\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    print(f\"\\nPhase 4: Finished. Checked {processed_channels_videos} channels, found {new_videos_found_total} new videos. Took {time.time() - phase_start_time:.2f}s.\")\n",
    "    print(f\"=== Top Stream Data Collection Cycle Finished in {time.time() - overall_start_time:.2f}s ===\")\n",
    "    return True\n",
    "\n",
    "# # Run the collection cycle (you can comment this out after initial runs or run it selectively)\n",
    "# if api_client:\n",
    "#     print(f\"\\n--- Executing Data Collection Cycle (Top Streams) at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "#     run_collection_cycle(api_client, db_conn)\n",
    "#     print(\"-\" * 30)\n",
    "#     print(\"Data Collection Cycle (Top Streams) Done for this run.\")\n",
    "#     print(\"-\" * 30)\n",
    "# else:\n",
    "#     print(\"\\nAPI Client not available. Skipping Data Collection Cycle.\")"
   ],
   "id": "15d635ce2a507458",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Mention Processing\n",
    "\n",
    "This section processes videos to extract mentions (indicating collaborations) from their titles. New channels discovered through this process will be registered in the database, but we will not yet fetch these new channels' videos. Run this repeatedly until all videos in the database have been processed.\n"
   ],
   "id": "5d8740c021271ab1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 3: Mention Processing Function (Atomic Per-Video)\n",
    "\n",
    "# Helper function to parse duration\n",
    "def parse_duration_for_collab(duration_str):\n",
    "    \"\"\"\n",
    "    Parses a Twitch duration string (e.g., \"1h30m20s\") into total seconds.\n",
    "    \"\"\"\n",
    "    if not duration_str or not isinstance(duration_str, str):\n",
    "        return 0 # Return 0 for invalid input\n",
    "\n",
    "    total_seconds = 0\n",
    "    hours = re.search(r'(\\d+)h', duration_str)\n",
    "    minutes = re.search(r'(\\d+)m', duration_str)\n",
    "    seconds = re.search(r'(\\d+)s', duration_str)\n",
    "\n",
    "    if hours:\n",
    "        total_seconds += int(hours.group(1)) * 3600\n",
    "    if minutes:\n",
    "        total_seconds += int(minutes.group(1)) * 60\n",
    "    if seconds:\n",
    "        total_seconds += int(seconds.group(1))\n",
    "\n",
    "    return total_seconds\n",
    "\n",
    "\n",
    "def process_video_mentions_batch(video_batch, current_api_client, current_db_conn):\n",
    "    \"\"\"\n",
    "    Processes a batch of videos to find mentions, update collaborations.\n",
    "    Discovers new channels via API for unknown mentions.\n",
    "    Attempts atomic processing per video using DB transactions.\n",
    "    \"\"\"\n",
    "    func_start_time = time.time()\n",
    "    processed_count_in_batch = 0;\n",
    "    newly_found_channels_in_batch = 0;\n",
    "    updated_edges_in_batch = 0;\n",
    "    all_unknown_logins_in_batch = set();\n",
    "    temp_video_data = {}\n",
    "\n",
    "    if VERBOSE_MODE: print(f\"  Batch Start: {len(video_batch)} videos to process.\")\n",
    "\n",
    "    # Pass 1: Extract mentions and identify all unique unknown logins for the batch\n",
    "    for video_row in video_batch:\n",
    "         video_id = video_row['id']; title = video_row['title'] or ''; desc = video_row['description'] or ''\n",
    "         text_to_scan = f\"{title} {desc}\"\n",
    "         mentioned_logins = network_utils.extract_mentions(text_to_scan)\n",
    "         temp_video_data[video_id] = {\n",
    "             'owner_id': video_row['channel_id'], 'mentions': mentioned_logins,\n",
    "             'published_at': video_row['published_at'], 'duration': video_row['duration']\n",
    "         }\n",
    "         if mentioned_logins:\n",
    "             try:\n",
    "                 _, not_found_now = network_utils.find_mentioned_channel_ids(mentioned_logins, current_db_conn);\n",
    "                 all_unknown_logins_in_batch.update(not_found_now)\n",
    "             except Exception as e:\n",
    "                 logging.error(f\"Error checking mentions in DB during Pass 1 for video {video_id}: {e}\")\n",
    "\n",
    "    # Pass 2: Fetch details for unknown mentioned logins via batched API calls\n",
    "    newly_discovered_ids_this_pass = {}\n",
    "    if all_unknown_logins_in_batch:\n",
    "        unknown_logins_list = list(all_unknown_logins_in_batch)\n",
    "        if VERBOSE_MODE: print(f\"  Pass 2: Checking {len(unknown_logins_list)} unique unknown logins via API...\")\n",
    "        num_api_batches = (len(unknown_logins_list) + 99) // 100\n",
    "        for i in range(num_api_batches):\n",
    "            batch_logins = unknown_logins_list[i * 100:(i + 1) * 100]\n",
    "            if VERBOSE_MODE: print(f\"   -> API Batch {i + 1}/{num_api_batches} for {len(batch_logins)} logins...\")\n",
    "            time.sleep(0.2)\n",
    "            user_details_list = current_api_client.get_user_details(user_logins=batch_logins);\n",
    "            api_call_succeeded = user_details_list is not None\n",
    "            if api_call_succeeded and user_details_list:\n",
    "                for user_data in user_details_list:\n",
    "                     try:\n",
    "                         database.save_channel_details(current_db_conn, user_data);\n",
    "                         login_lower = user_data['login'].lower();\n",
    "                         user_id = user_data['id']\n",
    "                         newly_discovered_ids_this_pass[login_lower] = user_id;\n",
    "                         newly_found_channels_in_batch += 1\n",
    "                     except Exception as e:\n",
    "                         print(f\"Error saving newly discovered channel {user_data.get('login')}: {e}\")\n",
    "            time.sleep(0.1)\n",
    "        if VERBOSE_MODE: print(f\"  Pass 2 Complete. Discovered and saved {newly_found_channels_in_batch} new channels.\")\n",
    "\n",
    "    # Pass 3: Process each video within its own database transaction\n",
    "    for idx, (video_id, video_data) in enumerate(temp_video_data.items()):\n",
    "        channel_id_A = video_data['owner_id'];\n",
    "        mentioned_logins = video_data['mentions']\n",
    "        published_at = video_data['published_at'];\n",
    "        duration_str = video_data['duration']\n",
    "        is_processed_successfully_this_video = False\n",
    "\n",
    "        try:\n",
    "            published_at_dt = published_at\n",
    "            if not isinstance(published_at_dt, datetime):\n",
    "                published_at_dt = pd.to_datetime(published_at, errors='coerce', utc=True)\n",
    "\n",
    "            if pd.isna(published_at_dt):\n",
    "                logging.warning(f\"Invalid timestamp for video {video_id}. Skipping edges. Marking processed.\")\n",
    "                current_db_conn.execute('BEGIN')\n",
    "                database.mark_video_mentions_processed(current_db_conn, video_id)\n",
    "                current_db_conn.commit()\n",
    "                processed_count_in_batch += 1\n",
    "                is_processed_successfully_this_video = True\n",
    "\n",
    "            elif not mentioned_logins:\n",
    "                current_db_conn.execute('BEGIN')\n",
    "                database.mark_video_mentions_processed(current_db_conn, video_id)\n",
    "                current_db_conn.commit()\n",
    "                processed_count_in_batch += 1\n",
    "                is_processed_successfully_this_video = True\n",
    "            else:\n",
    "                current_known_ids, _ = network_utils.find_mentioned_channel_ids(mentioned_logins, current_db_conn)\n",
    "                for login, user_id_new in newly_discovered_ids_this_pass.items():\n",
    "                     if login in mentioned_logins:\n",
    "                         current_known_ids[login] = user_id_new\n",
    "\n",
    "                duration_sec = parse_duration_for_collab(duration_str)\n",
    "                edges_for_this_video = 0\n",
    "                mentions_to_add_list = []\n",
    "\n",
    "                current_db_conn.execute('BEGIN IMMEDIATE')\n",
    "                for login, channel_id_B in current_known_ids.items():\n",
    "                    if channel_id_A != channel_id_B:\n",
    "                        database.upsert_collaboration_edge(current_db_conn, channel_id_A, channel_id_B, published_at_dt, duration_sec)\n",
    "                        mentions_to_add_list.append((channel_id_A, channel_id_B, video_id, published_at_dt))\n",
    "                        edges_for_this_video += 1\n",
    "\n",
    "                if mentions_to_add_list:\n",
    "                    database.add_mentions(current_db_conn, mentions_to_add_list)\n",
    "\n",
    "                database.mark_video_mentions_processed(current_db_conn, video_id)\n",
    "                current_db_conn.commit()\n",
    "\n",
    "                processed_count_in_batch += 1\n",
    "                updated_edges_in_batch += edges_for_this_video\n",
    "                is_processed_successfully_this_video = True\n",
    "\n",
    "        except Exception as e:\n",
    "            if not is_processed_successfully_this_video:\n",
    "                 try:\n",
    "                     current_db_conn.rollback()\n",
    "                 except sqlite3.Error as rb_err:\n",
    "                     logging.error(f\"Error during rollback for video {video_id}: {rb_err}\")\n",
    "                 logging.error(f\"Error processing video {video_id} within transaction\", exc_info=True)\n",
    "\n",
    "    return processed_count_in_batch, newly_found_channels_in_batch, updated_edges_in_batch"
   ],
   "id": "975b454ea1069823",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # %%\n",
    "# # Cell 4: Mention Processing Loop\n",
    "# if api_client:\n",
    "#     print(f\"\\n--- Starting Mention Processing Phase at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "#     BATCH_SIZE = config.MENTION_PROC_BATCH_SIZE\n",
    "#     MAX_BATCHES_PER_RUN = config.MENTION_PROC_MAX_BATCHES\n",
    "#     total_videos_processed_run = 0\n",
    "#     total_new_channels_run = 0\n",
    "#     total_edges_updated_run = 0\n",
    "#     batches_processed_run = 0\n",
    "#     mention_loop_start_time = time.time()\n",
    "#     batch_processing_times_mention_loop = []\n",
    "#\n",
    "#     # --- Indication of total unprocessed videos at the START ---\n",
    "#     try:\n",
    "#         unproc_cursor = db_conn.cursor()\n",
    "#         unproc_cursor.execute(\"SELECT COUNT(*) FROM Videos WHERE mentions_processed_at IS NULL\")\n",
    "#         initial_total_unprocessed_videos = unproc_cursor.fetchone()[0]\n",
    "#         print(f\"Estimated total unprocessed videos at start of run: {initial_total_unprocessed_videos}\")\n",
    "#         if initial_total_unprocessed_videos > 0:\n",
    "#             initial_total_expected_batches = (initial_total_unprocessed_videos + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "#             print(f\"Expecting around {initial_total_expected_batches} batches in total (this run will process up to {MAX_BATCHES_PER_RUN}).\")\n",
    "#     except sqlite3.Error as e_count:\n",
    "#         print(f\"Could not get initial count of unprocessed videos: {e_count}\")\n",
    "#         initial_total_expected_batches = MAX_BATCHES_PER_RUN  # Fallback for ETA\n",
    "#\n",
    "#     while batches_processed_run < MAX_BATCHES_PER_RUN:\n",
    "#         batch_loop_start_time = time.time()\n",
    "#         print(f\"\\nFetching mention processing batch {batches_processed_run + 1}/{MAX_BATCHES_PER_RUN} (Batch size: {BATCH_SIZE})...\")\n",
    "#         videos_to_process = database.get_unprocessed_videos_batch(db_conn, BATCH_SIZE)\n",
    "#\n",
    "#         if not videos_to_process:\n",
    "#             print(\"No more videos found needing mention processing.\")\n",
    "#             break\n",
    "#\n",
    "#         print(f\"Processing mentions for {len(videos_to_process)} videos...\")\n",
    "#         try:\n",
    "#             # Call the function implementing the atomic logic\n",
    "#             count, new_chans, edges = process_video_mentions_batch(videos_to_process, api_client, db_conn)\n",
    "#\n",
    "#             total_videos_processed_run += count\n",
    "#             total_new_channels_run += new_chans\n",
    "#             total_edges_updated_run += edges\n",
    "#             batches_processed_run += 1\n",
    "#             batch_duration = time.time() - batch_loop_start_time\n",
    "#             batch_processing_times_mention_loop.append(batch_duration)\n",
    "#\n",
    "#             # Print the summary for the batch\n",
    "#             print(f\"Batch {batches_processed_run} complete in {batch_duration:.2f}s. Processed: {count} videos, Found: {new_chans} new channels, Upserted: {edges} edges.\")\n",
    "#\n",
    "#             if VERBOSE_MODE and batches_processed_run < MAX_BATCHES_PER_RUN and batch_processing_times_mention_loop and len(videos_to_process) == BATCH_SIZE:\n",
    "#                 avg_time_per_batch = sum(batch_processing_times_mention_loop) / len(batch_processing_times_mention_loop)\n",
    "#                 remaining_batches_in_run = MAX_BATCHES_PER_RUN - batches_processed_run\n",
    "#\n",
    "#                 # Estimate based on remaining in *this run* or *total expected* if available and less\n",
    "#                 batches_for_eta = remaining_batches_in_run\n",
    "#                 if 'initial_total_expected_batches' in locals() and initial_total_expected_batches > batches_processed_run:\n",
    "#                     batches_for_eta = min(remaining_batches_in_run, initial_total_expected_batches - batches_processed_run)\n",
    "#\n",
    "#                 est_remaining_run_secs = batches_for_eta * avg_time_per_batch\n",
    "#                 if est_remaining_run_secs > 0:\n",
    "#                     est_run_mins, est_run_s = divmod(int(est_remaining_run_secs), 60)\n",
    "#                     print(f\"Est. time remaining for *this run* (up to {MAX_BATCHES_PER_RUN} batches, or fewer if DB empties): {est_run_mins}m {est_run_s}s\")\n",
    "#\n",
    "#             time.sleep(max(0.2, 1.0 - (0.05 * count / BATCH_SIZE if BATCH_SIZE > 0 else 1)))  # Dynamic delay\n",
    "#\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error during mention processing batch: {e}\")\n",
    "#             logging.error(\"Error in mention processing loop\", exc_info=True)\n",
    "#             print(\"Stopping mention processing due to error.\")\n",
    "#             break\n",
    "#\n",
    "#     # Final summary prints for the run\n",
    "#     print(f\"\\n--- Mention Processing Phase Finished (for this run) in {time.time() - mention_loop_start_time:.2f}s ---\")\n",
    "#     print(f\"Total videos marked as processed in this run: {total_videos_processed_run}\")\n",
    "#     print(f\"Total new channels discovered via mentions in this run: {total_new_channels_run}\")\n",
    "#     print(f\"Total collaboration edge instances upserted in this run: {total_edges_updated_run}\")\n",
    "#\n",
    "#     # Indication of remaining unprocessed videos at the END\n",
    "#     try:\n",
    "#         final_unproc_cursor = db_conn.cursor()\n",
    "#         final_unproc_cursor.execute(\"SELECT COUNT(*) FROM Videos WHERE mentions_processed_at IS NULL\")\n",
    "#         final_unprocessed_videos = final_unproc_cursor.fetchone()[0]\n",
    "#         print(f\"\\nVideos remaining to be processed in database: {final_unprocessed_videos}\")\n",
    "#     except sqlite3.Error as e_count_end:\n",
    "#         print(f\"Could not get final count of unprocessed videos: {e_count_end}\")\n",
    "#\n",
    "#     print(\"-\" * 30)\n",
    "#\n",
    "# else:\n",
    "#     print(\"\\nAPI Client not available. Skipping Mention Processing Phase.\")"
   ],
   "id": "9b8a33530f48500e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Refresh Cycle\n",
    "\n",
    "This section periodically updates details and recent videos for a prioritized subset of channels in the database. It focuses on channels that have not been updated recently (\"stalest\" channels), ensuring the dataset remains current without re-fetching all channels every cycle. The refresh cycle:\n",
    "\n",
    "* Selects a configurable number of the stalest channels based on last update timestamps.\n",
    "* Updates each channel's details (including follower count).\n",
    "* Fetches and stores new videos for each refreshed channel.\n",
    "* Helps maintain up-to-date collaboration and content data for ongoing analysis.\n",
    "\n",
    "Run this cycle regularly to keep your database fresh and to discover new videos as channels continue to stream and as we discover new channels. Run it repeatedly until refreshes do not produce significant updates, at which point you can go back to the Mention Processing section to process the new videos."
   ],
   "id": "89a825b0f8352143"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell 5: Refresh Function (Highly Optimized with Batching and Error Handling)\n",
    "\n",
    "def run_refresh_cycle(current_api_client, current_db_conn, num_channels_to_refresh):\n",
    "    \"\"\"\n",
    "    Refreshes details and fetches recent videos for a prioritized subset of\n",
    "    the \"stalest\" channels. Uses batching for all possible API calls to maximize efficiency\n",
    "    and correctly handles channels that fail the API lookup.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Prioritized Channel Refresh Cycle at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "    overall_refresh_start_time = time.time()\n",
    "    processed_count = 0; new_videos_found_total = 0\n",
    "    channel_refresh_times = []\n",
    "\n",
    "    try:\n",
    "        # 1. Get a prioritized list of channels to refresh\n",
    "        print(f\"Fetching up to {num_channels_to_refresh} of the stalest channels from the database...\")\n",
    "        channels_to_refresh_rows = database.get_stale_channels_for_refresh(current_db_conn, num_channels_to_refresh)\n",
    "        actual_to_refresh_count = len(channels_to_refresh_rows)\n",
    "\n",
    "        if not channels_to_refresh_rows:\n",
    "            print(\"No channels found to refresh.\")\n",
    "            return 0, 0\n",
    "\n",
    "        channel_ids_to_refresh = [row['id'] for row in channels_to_refresh_rows]\n",
    "        print(f\"Prioritized list created. Preparing to refresh {actual_to_refresh_count} channels...\")\n",
    "\n",
    "        # --- 2. Batch fetch all possible data first ---\n",
    "        print(\"\\nBatch fetching user details and channel info (tags)...\")\n",
    "        user_details_map = {}\n",
    "        tags_map = {}\n",
    "\n",
    "        for i in range(0, len(channel_ids_to_refresh), 100):\n",
    "            batch_ids = channel_ids_to_refresh[i:i+100]\n",
    "            if VERBOSE_MODE: print(f\" -> Fetching batch {i//100 + 1} ({len(batch_ids)} channels)...\")\n",
    "\n",
    "            # Batch call for user details (description, etc.)\n",
    "            user_details_batch = current_api_client.get_user_details(user_ids=batch_ids)\n",
    "            if user_details_batch:\n",
    "                for user in user_details_batch:\n",
    "                    user_details_map[user['id']] = user\n",
    "\n",
    "            # Batch call for channel info (tags, etc.)\n",
    "            channels_info_batch = current_api_client.get_channels_info(broadcaster_ids=batch_ids)\n",
    "            if channels_info_batch:\n",
    "                for channel in channels_info_batch:\n",
    "                    tags_map[channel['broadcaster_id']] = channel.get('tags', [])\n",
    "\n",
    "            time.sleep(0.2)\n",
    "\n",
    "        print(f\" -> Received details for {len(user_details_map)} channels and tags for {len(tags_map)} channels.\")\n",
    "\n",
    "        # --- 3. Iterate through each channel for the remaining individual API calls ---\n",
    "        for i, channel_id in enumerate(channel_ids_to_refresh):\n",
    "            channel_refresh_start_time = time.time()\n",
    "\n",
    "            user_data = user_details_map.get(channel_id)\n",
    "\n",
    "            if not user_data:\n",
    "                print(f\"\\n ({i + 1}/{actual_to_refresh_count}) Details for channel {channel_id} not found in batch fetch (channel may be banned/deleted).\")\n",
    "                print(\" -> Marking as processed to avoid re-checking in the near future.\")\n",
    "\n",
    "                # Mark both timestamps as updated so it's moved to the back of the queue\n",
    "                database.update_channel_video_fetch_time(current_db_conn, channel_id)\n",
    "                database.update_channel_detail_fetch_time(current_db_conn, channel_id)\n",
    "                processed_count += 1\n",
    "                continue # Skip to the next channel in the list\n",
    "\n",
    "            channel_log_name = user_data.get('login', channel_id)\n",
    "            print(f\"\\n ({i + 1}/{actual_to_refresh_count}) Processing channel: {channel_log_name}...\")\n",
    "\n",
    "            # Add pre-fetched tags to the user data object\n",
    "            user_data['tags'] = tags_map.get(channel_id)\n",
    "\n",
    "            # API Call 1: Get follower count (must be individual)\n",
    "            follower_count = current_api_client.get_channel_follower_count(broadcaster_id=channel_id)\n",
    "            if follower_count is not None:\n",
    "                user_data['follower_count'] = follower_count\n",
    "\n",
    "            # Save the combined details (details, tags, followers)\n",
    "            try:\n",
    "                database.save_channel_details(current_db_conn, user_data)\n",
    "            except Exception as e:\n",
    "                print(f\"  -> DB Error saving details for channel {channel_id}: {e}\")\n",
    "\n",
    "            # API Call 2: Fetch new videos (must be individual)\n",
    "            latest_stored_date = database.get_latest_video_date_for_channel(current_db_conn, channel_id)\n",
    "            new_videos = current_api_client.get_channel_videos(channel_id, video_type='archive', limit=50, after_date=latest_stored_date)\n",
    "\n",
    "            if new_videos:\n",
    "                if VERBOSE_MODE: print(f\" -> Found {len(new_videos)} new archive videos.\")\n",
    "                database.save_videos(current_db_conn, new_videos)\n",
    "                new_videos_found_total += len(new_videos)\n",
    "\n",
    "            if new_videos is not None:\n",
    "                database.update_channel_video_fetch_time(current_db_conn, channel_id)\n",
    "\n",
    "            processed_count += 1\n",
    "\n",
    "            # Time estimation logic\n",
    "            channel_refresh_duration = time.time() - channel_refresh_start_time\n",
    "            channel_refresh_times.append(channel_refresh_duration)\n",
    "            if VERBOSE_MODE and channel_refresh_times:\n",
    "                avg_time_per_refresh = sum(channel_refresh_times) / len(channel_refresh_times)\n",
    "                remaining_refreshes = actual_to_refresh_count - (i + 1)\n",
    "                est_remaining_refresh_secs = remaining_refreshes * avg_time_per_refresh\n",
    "                if est_remaining_refresh_secs > 0 and i < actual_to_refresh_count - 1:\n",
    "                    est_ref_mins, est_ref_s = divmod(int(est_remaining_refresh_secs), 60)\n",
    "                    print(f\" -> Refreshed in {channel_refresh_duration:.2f}s. Est. time remaining: {est_ref_mins}m {est_ref_s}s\")\n",
    "\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        print(f\"\\n--- Channel Refresh Cycle Finished in {time.time() - overall_refresh_start_time:.2f}s ---\")\n",
    "        print(f\"Attempted refresh for {processed_count} channels.\")\n",
    "        print(f\"Found {new_videos_found_total} new videos in total during refresh.\")\n",
    "\n",
    "        return new_videos_found_total, processed_count\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during refresh cycle: {e}\")\n",
    "        logging.error(\"Error in refresh cycle\", exc_info=True)\n",
    "        return 0, 0"
   ],
   "id": "5bc8ba422b2e5eb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Run Channel Refresh Cycle\n",
    "# if api_client:\n",
    "#     print(f\"\\n--- Executing Channel Refresh Cycle at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "#     run_refresh_cycle(api_client, db_conn, num_channels_to_refresh=config.REFRESH_CYCLE_CHANNELS)\n",
    "#     print(\"-\" * 30)\n",
    "#     print(\"Channel Refresh Phase Done (for this run).\")\n",
    "#     print(\"-\" * 30)\n",
    "# else:\n",
    "#     print(\"\\nAPI Client not available. Skipping Channel Refresh Cycle.\")\n"
   ],
   "id": "924394808b835266",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Automated Data Collection Main Loop\n",
    "\n",
    "The following cell defines and runs the main automation function, `run_automated_main_loop`. This function is designed to intelligently run the different data collection cycles until the database is considered \"up-to-date\" and stable. It serves as a \"run and walk away\" alternative to manually running each cycle.\n",
    "\n",
    "The loop operates in a sequence of \"meta-cycles,\" each following a specific priority order:\n",
    "\n",
    "1.  **Phase 1: Process All Mentions**\n",
    "    * The loop's first priority is to process the `mentions` for all videos currently in the database that haven't been checked yet.\n",
    "    * It will run the mention processing cycle repeatedly until the queue of unprocessed videos is empty. This ensures that all existing data is fully integrated into the collaboration network before new data is fetched.\n",
    "\n",
    "2.  **Phase 2: Refresh Until Stable**\n",
    "    * Once the mention queue is clear, the loop begins running the `run_refresh_cycle` to update stale channels and fetch their newest videos.\n",
    "    * It continues to run this refresh cycle as long as the updates are considered \"significant.\" A significant update is defined by finding a number of new videos that is greater than a certain percentage of the channels refreshed in that cycle (e.g., more than 10%, controlled by `REFRESH_SIGNIFICANCE_THRESHOLD`).\n",
    "    * When a refresh cycle produces very few new videos, the database is considered stable, and this phase concludes.\n",
    "\n",
    "3.  **Phase 3: Final Collection & Loop Decision**\n",
    "    * **If the Refresh Phase was busy** (meaning it ran more than once and added a lot of new videos), the main loop will restart from Phase 1. This is crucial because the new videos need to have their mentions processed.\n",
    "    * **If the Refresh Phase was quiet** (meaning it ran only once or not at all), the script assumes the database is stable. It performs one final `run_collection_cycle` to seed the database with the latest top streams and then gracefully stops the automation.\n",
    "\n",
    "This entire process is wrapped with safety limits (`MAX_META_CYCLES`, etc.) to prevent any potential for infinite loops.\n",
    "\n",
    "Running this may take a very long time, so it is commented-out by default"
   ],
   "id": "61d041a43f07dc2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell: Automated Data Collection Main Loop\n",
    "\n",
    "def run_automated_main_loop(api_client, db_conn):\n",
    "    \"\"\"\n",
    "    Runs the data collection cycles iteratively until the database is in a\n",
    "    stable, up-to-date state.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"AUTOMATED DATA COLLECTION LOOP STARTED at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Automation Parameters ---\n",
    "    REFRESH_SIGNIFICANCE_THRESHOLD = 4\n",
    "    MAX_META_CYCLES = 10\n",
    "    MAX_MENTION_LOOPS_PER_CYCLE = 200\n",
    "    MAX_REFRESH_LOOPS_PER_CYCLE = 20\n",
    "\n",
    "    meta_cycles_run = 0\n",
    "    while meta_cycles_run < MAX_META_CYCLES:\n",
    "        meta_cycles_run += 1\n",
    "        print(f\"\\n--- Starting Meta-Cycle {meta_cycles_run}/{MAX_META_CYCLES} ---\")\n",
    "\n",
    "        # --- PHASE 1: Process all existing mentions until queue is empty ---\n",
    "        print(\"\\n>>> Phase 1: Processing mentions for unprocessed videos...\")\n",
    "        mention_loops_run = 0\n",
    "        total_mentions_processed_this_phase = 0\n",
    "        total_new_channels_this_phase = 0\n",
    "        total_edges_upserted_this_phase = 0\n",
    "\n",
    "        while mention_loops_run < MAX_MENTION_LOOPS_PER_CYCLE:\n",
    "            mention_loops_run += 1\n",
    "            unprocessed_count = db_conn.execute(\"SELECT COUNT(*) FROM Videos WHERE mentions_processed_at IS NULL\").fetchone()[0]\n",
    "\n",
    "            if unprocessed_count == 0:\n",
    "                print(\"All mentions processed. Moving to next phase.\")\n",
    "                break\n",
    "\n",
    "            print(f\"\\nRunning mention processing batch {mention_loops_run} (Unprocessed videos remaining: {unprocessed_count})...\")\n",
    "            videos_to_process = database.get_unprocessed_videos_batch(db_conn, config.MENTION_PROC_BATCH_SIZE)\n",
    "\n",
    "            if not videos_to_process:\n",
    "                print(\"Warning: Unprocessed count was > 0 but no videos were fetched. Breaking mention loop.\")\n",
    "                break\n",
    "\n",
    "            batch_start_time = time.time()\n",
    "\n",
    "            # --- Capture return values and print summary ---\n",
    "            count, new_chans, edges = process_video_mentions_batch(videos_to_process, api_client, db_conn)\n",
    "            batch_duration = time.time() - batch_start_time\n",
    "\n",
    "            print(f\" -> Batch complete in {batch_duration:.2f}s. Processed: {count} videos, Found: {new_chans} new channels, Upserted: {edges} edges.\")\n",
    "\n",
    "            # Update phase totals\n",
    "            total_mentions_processed_this_phase += count\n",
    "            total_new_channels_this_phase += new_chans\n",
    "            total_edges_upserted_this_phase += edges\n",
    "\n",
    "        else: # This 'else' belongs to the 'while' loop\n",
    "            print(\"Warning: Mention processing reached max loops. Moving on to prevent infinite loop.\")\n",
    "\n",
    "        print(f\"\\n--- Mention Processing Phase Summary ---\")\n",
    "        # Adjust run count if loop broke on first check\n",
    "        final_mention_loops_run = mention_loops_run - 1 if unprocessed_count == 0 else mention_loops_run\n",
    "        print(f\"Ran {final_mention_loops_run} batch(es).\")\n",
    "        print(f\"Total Videos Processed: {total_mentions_processed_this_phase}\")\n",
    "        print(f\"Total New Channels Found: {total_new_channels_this_phase}\")\n",
    "        print(f\"Total Edges Upserted: {total_edges_upserted_this_phase}\")\n",
    "\n",
    "\n",
    "        # --- PHASE 2: Refresh stale channels until no significant updates are found ---\n",
    "        print(\"\\n>>> Phase 2: Refreshing stale channels until database stabilizes...\")\n",
    "        refresh_cycles_run = 0\n",
    "        while refresh_cycles_run < MAX_REFRESH_LOOPS_PER_CYCLE:\n",
    "            refresh_cycles_run += 1\n",
    "            print(f\"\\nRunning refresh cycle {refresh_cycles_run}...\")\n",
    "\n",
    "            new_videos, channels_refreshed = run_refresh_cycle(\n",
    "                api_client, db_conn, config.REFRESH_CYCLE_CHANNELS\n",
    "            )\n",
    "\n",
    "            if channels_refreshed == 0:\n",
    "                print(\"Refresh cycle found no channels to process. Database is stable.\")\n",
    "                break\n",
    "\n",
    "            significance_limit = channels_refreshed * REFRESH_SIGNIFICANCE_THRESHOLD\n",
    "            if new_videos < significance_limit:\n",
    "                print(f\"Refresh cycle found only {new_videos} new videos, which is below the significance threshold of {significance_limit:.1f}. Concluding refresh phase.\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Found {new_videos} new videos. Updates were significant, another refresh cycle may be needed.\")\n",
    "        else:\n",
    "            print(\"Warning: Refresh phase reached max loops. Moving on to prevent infinite loop.\")\n",
    "\n",
    "\n",
    "        # --- PHASE 3: Decide whether to loop back or finish ---\n",
    "        print(\"\\n>>> Phase 3: Evaluating next step...\")\n",
    "        if refresh_cycles_run > 1:\n",
    "            print(\"Significant updates were made during refresh. Looping back to process mentions from newly added videos.\")\n",
    "            continue\n",
    "        else:\n",
    "            print(\"Database is stable. Running one final top-stream collection cycle...\")\n",
    "            run_collection_cycle(api_client, db_conn)\n",
    "\n",
    "            print(\"\\nAutomation complete. The database is now in a stable, up-to-date state.\")\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        print(f\"\\nAutomation stopped after reaching maximum meta-cycles ({MAX_META_CYCLES}). This is a safeguard against infinite loops.\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(f\"AUTOMATED DATA COLLECTION LOOP FINISHED at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "# --- Execute the Automated Loop ---\n",
    "# You can run this cell to start the full automated process (may take many hours)\n",
    "if api_client:\n",
    "    # Execute the Automated Loop\n",
    "    run_automated_main_loop(api_client, db_conn)\n",
    "else:\n",
    "    print(\"\\nAPI Client not available. Skipping Automated Data Collection Loop.\")"
   ],
   "id": "f88099548266cb16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Exploration\n",
    "\n",
    "Load the collected data from the database into pandas DataFrames and perform basic analysis and visualization.\n"
   ],
   "id": "27a245d7daeec98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell 7: Load Data from Database\n",
    "print(f\"\\n--- Loading Data for Exploration at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "try:\n",
    "    channel_date_cols = ['created_at', 'first_seen', 'last_fetched_details', 'last_fetched_videos']\n",
    "    video_date_cols = ['published_at', 'created_at_api', 'fetched_at', 'mentions_processed_at']\n",
    "    category_date_cols = ['last_scanned_top_streams']\n",
    "\n",
    "    print(\"Loading Channels table...\")\n",
    "    channels_df = pd.read_sql_query(\"SELECT * FROM Channels\", db_conn)\n",
    "    print(\"Loading Videos table...\")\n",
    "    videos_df = pd.read_sql_query(\"SELECT * FROM Videos\", db_conn)\n",
    "    print(\"Loading Categories table...\")\n",
    "    categories_df = pd.read_sql_query(\"SELECT * FROM Categories\", db_conn)\n",
    "\n",
    "    # --- NEW: Load the Mentions table ---\n",
    "    print(\"Loading Mentions table...\")\n",
    "    mentions_df = pd.read_sql_query(\"SELECT * FROM Mentions\", db_conn)\n",
    "\n",
    "    # Convert timestamp columns\n",
    "    print(\"Converting timestamp columns...\")\n",
    "    for col in channel_date_cols:\n",
    "        if col in channels_df.columns: channels_df[col] = pd.to_datetime(channels_df[col], errors='coerce', utc=True)\n",
    "    for col in video_date_cols:\n",
    "        if col in videos_df.columns: videos_df[col] = pd.to_datetime(videos_df[col], errors='coerce', utc=True)\n",
    "    for col in category_date_cols:\n",
    "         if col in categories_df.columns: categories_df[col] = pd.to_datetime(categories_df[col], errors='coerce', utc=True)\n",
    "    # Convert timestamp in the new mentions_df\n",
    "    if 'mention_timestamp' in mentions_df.columns:\n",
    "        mentions_df['mention_timestamp'] = pd.to_datetime(mentions_df['mention_timestamp'], errors='coerce', utc=True)\n",
    "\n",
    "\n",
    "    # Convert numeric columns, coercing errors\n",
    "    print(\"Converting numeric columns and parsing durations...\")\n",
    "    channels_df['follower_count'] = pd.to_numeric(channels_df['follower_count'], errors='coerce')\n",
    "    channels_df['display_name'] = channels_df['display_name'].fillna(channels_df['login'])\n",
    "    videos_df['duration_seconds'] = videos_df['duration'].apply(parse_duration_for_collab).fillna(0).astype(int)\n",
    "\n",
    "    print(f\"Loaded {len(channels_df)} channels, {len(videos_df)} videos, {len(categories_df)} categories, and {len(mentions_df)} mentions.\")\n",
    "    print(\"--- Data Loading Complete ---\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from database: {e}\")\n",
    "    raise SystemExit(\"Stopping notebook due to data loading failure.\")"
   ],
   "id": "889d3742ca50c522",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 8: Display Sample Data\n",
    "print(\"\\nSample Channels Data:\")\n",
    "display(channels_df.head())\n",
    "print(\"\\nSample Videos Data:\")\n",
    "display(\n",
    "    videos_df[['id', 'channel_id', 'title', 'published_at', 'view_count', 'duration_seconds', 'mentions_processed_at',\n",
    "               'game_name']].head())\n",
    "print(\"\\nSample Categories Data:\")\n",
    "display(categories_df.head())\n"
   ],
   "id": "8246499271cdb108",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 9: Basic Statistics\n",
    "print(\"\\n--- Basic Statistics ---\")\n",
    "print(\"\\nChannel Statistics:\")\n",
    "print(f\"Total Channels: {len(channels_df)}\")\n",
    "print(\"\\nBroadcaster Types:\")\n",
    "print(channels_df['broadcaster_type'].value_counts(dropna=False))\n",
    "print(\"\\nChannel Follower Count Summary:\")\n",
    "print(channels_df['follower_count'].describe())\n",
    "print(f\"Channels missing details: {channels_df['last_fetched_details'].isnull().sum()}\")\n",
    "\n",
    "print(\"\\nVideo Statistics:\")\n",
    "print(f\"Total Videos: {len(videos_df)}\")\n",
    "print(\"\\nVideo Types:\")\n",
    "print(videos_df['type'].value_counts(dropna=False))\n",
    "print(\"\\nVideo View Count Summary:\")\n",
    "print(videos_df['view_count'].describe())\n",
    "print(\"\\nVideo Duration (seconds) Summary:\")\n",
    "print(videos_df['duration_seconds'].describe())\n",
    "print(f\"Videos with mentions processed: {videos_df['mentions_processed_at'].notnull().sum()}\")\n",
    "print(f\"Videos pending mention processing: {videos_df['mentions_processed_at'].isnull().sum()}\")"
   ],
   "id": "6416a94fbeb6693c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Visualization (Channels & Videos)\n",
   "id": "7aa877841300cbbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 10: Visualizations - Channels\n",
    "print(\"\\n--- Channel Visualizations ---\")\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# --- UPDATED: Histogram of Channel Follower Counts ---\n",
    "print(\"Generating Channel Follower Count Histogram...\")\n",
    "plt.figure(figsize=(10, 5));\n",
    "# Use the 'follower_count' column now, and drop any rows where it might be NaN\n",
    "followers_positive = channels_df.dropna(subset=['follower_count'])\n",
    "followers_positive = followers_positive[followers_positive['follower_count'] > 0]['follower_count']\n",
    "\n",
    "if not followers_positive.empty:\n",
    "    # Determine if log scale is needed by checking the data range\n",
    "    should_use_log = (followers_positive.max() / followers_positive.min() > 100) if followers_positive.min() > 0 else False\n",
    "\n",
    "    # Conditionally set the plotting arguments\n",
    "    plot_kwargs = {'bins': 40, 'kde': False}\n",
    "    if should_use_log:\n",
    "        plot_kwargs['log_scale'] = True # Set to True, not a variable that could be False\n",
    "        title = 'Distribution of Channel Follower Counts (Log Scale)'\n",
    "        xlabel = 'Total Follower Count (Log Scale)'\n",
    "    else:\n",
    "        # Do not pass log_scale when it's not needed\n",
    "        title = 'Distribution of Channel Follower Counts'\n",
    "        xlabel = 'Total Follower Count'\n",
    "\n",
    "    sns.histplot(followers_positive, **plot_kwargs)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Number of Channels')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No positive channel follower data to plot histogram yet.\")\n",
    "\n",
    "print(\"Generating Broadcaster Types Bar Chart...\")\n",
    "plt.figure(figsize=(8, 5));\n",
    "channel_types = channels_df['broadcaster_type'].fillna('N/A').replace('', 'N/A')\n",
    "sns.countplot(y=channel_types, order=channel_types.value_counts().index, palette='viridis');\n",
    "plt.title('Channel Broadcaster Types')\n",
    "plt.xlabel('Number of Channels')\n",
    "plt.ylabel('Broadcaster Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "8fddecb20676a379",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 11: Visualizations - Videos\n",
    "print(\"\\n--- Video Visualizations ---\")\n",
    "\n",
    "print(\"Generating Video View Count Histogram...\")\n",
    "plt.figure(figsize=(10, 5));\n",
    "views_positive_vid = videos_df.dropna(subset=['view_count'])\n",
    "views_positive_vid = views_positive_vid[views_positive_vid['view_count'] > 0]['view_count']\n",
    "\n",
    "if not views_positive_vid.empty:\n",
    "    # Determine if log scale is needed\n",
    "    should_use_log_vid = (views_positive_vid.max() / views_positive_vid.min() > 100) if views_positive_vid.min() > 0 else False\n",
    "\n",
    "    # Conditionally set the plotting arguments\n",
    "    plot_kwargs_vid = {'bins': 40, 'kde': False}\n",
    "    if should_use_log_vid:\n",
    "        plot_kwargs_vid['log_scale'] = True\n",
    "        title_vid = 'Distribution of Video View Counts (Log Scale)'\n",
    "        xlabel_vid = 'Video View Count (Log Scale)'\n",
    "    else:\n",
    "        title_vid = 'Distribution of Video View Counts'\n",
    "        xlabel_vid = 'Video View Count'\n",
    "\n",
    "    sns.histplot(views_positive_vid, **plot_kwargs_vid)\n",
    "    plt.title(title_vid)\n",
    "    plt.xlabel(xlabel_vid)\n",
    "    plt.ylabel('Number of Videos')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No positive video view data to plot histogram.\")\n",
    "\n",
    "print(\"Generating Video Types Bar Chart...\")\n",
    "plt.figure(figsize=(8, 5));\n",
    "video_types = videos_df['type'].fillna('N/A');\n",
    "sns.countplot(y=video_types, order=video_types.value_counts().index, palette='magma');\n",
    "plt.title('Video Types');\n",
    "plt.xlabel('Number of Videos');\n",
    "plt.ylabel('Type');\n",
    "plt.tight_layout();\n",
    "plt.show()\n",
    "\n",
    "# --- Video Publication Time Series ---\n",
    "print(\"Generating Video Publication Time Series...\")\n",
    "plt.figure(figsize=(12, 6));\n",
    "video_pub_dates = videos_df.dropna(subset=['published_at'])\n",
    "\n",
    "if not video_pub_dates.empty and len(video_pub_dates) > 1:\n",
    "    # --- Calculate the 5th percentile date to set the start range ---\n",
    "    start_date = video_pub_dates['published_at'].quantile(0.05)\n",
    "    print(f\"Displaying time series from {start_date.strftime('%Y-%m-%d')} onwards (showing 95% of the data).\")\n",
    "\n",
    "    # Filter the DataFrame to this new date range\n",
    "    plotting_df = video_pub_dates[video_pub_dates['published_at'] >= start_date]\n",
    "\n",
    "    # --- Use the filtered plotting_df for the rest of the logic ---\n",
    "    if not plotting_df.empty:\n",
    "        time_range_days = (plotting_df['published_at'].max() - plotting_df['published_at'].min()).days if len(plotting_df) > 1 else 0\n",
    "        resample_freq = 'ME' if time_range_days > 90 else 'D' # Resample by Month or Day\n",
    "        plot_title = 'Number of Videos Published (' + ('Monthly' if resample_freq == 'ME' else 'Daily') + ')'\n",
    "\n",
    "        # Plot the resampled data\n",
    "        plotting_df.set_index('published_at')['id'].resample(resample_freq).count().plot(marker='.' if resample_freq == 'D' else 'o', linestyle='-');\n",
    "\n",
    "        plt.title(plot_title);\n",
    "        plt.ylabel('Number of Videos');\n",
    "        plt.xlabel('Publication Date');\n",
    "        plt.tight_layout();\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No video data remains after filtering by start date.\")\n",
    "else:\n",
    "    print(\"Not enough video publication date data to plot a meaningful time series.\")"
   ],
   "id": "9f967ce2b74eb455",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Note on \"Streaming Together\" Feature\n",
    "\n",
    "While Twitch has features like \"Squad Streams\" or \"Guest Star\" allowing multiple creators to stream simultaneously on one channel, the participant data for these features **does not appear to be reliably available** via the standard Twitch API for *past* streams or VODs (as of June 2025).\n",
    "\n",
    "Therefore, the collaboration detection in this notebook relies primarily on identifying `@mentions` within video titles and descriptions.\n"
   ],
   "id": "8d17c13ab5a5f533"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Collaboration Network Exploration (Filtered)\n",
    "\n",
    "Exploring the collaboration network. Data is filtered IN MEMORY based on\n",
    "thresholds in `config.py` BEFORE analysis to improve performance.\n",
    "The underlying database remains complete.\n"
   ],
   "id": "cd855c40a6302025"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 12: Load and Filter Data for Network Analysis\n",
    "print(f\"\\n--- Loading and Filtering Data for Network Analysis at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "channels_df_net = pd.DataFrame()  # For channels passing all filters\n",
    "collab_df_net = pd.DataFrame()   # For edges passing all filters\n",
    "G_filtered = nx.Graph()          # The filtered graph for analysis\n",
    "degree_df_filtered = pd.DataFrame() # For degree stats\n",
    "\n",
    "try:\n",
    "    # --- 1. Load base data ---\n",
    "    collab_df_full = pd.read_sql_query(\"SELECT * FROM Collaborations\", db_conn)\n",
    "    collab_df_full['collaboration_count'] = pd.to_numeric(collab_df_full['collaboration_count'], errors='coerce').fillna(0).astype(int)\n",
    "    collab_df_full['total_collaboration_duration_seconds'] = pd.to_numeric(collab_df_full['total_collaboration_duration_seconds'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "\n",
    "    # --- 2. Filter Channels ---\n",
    "    print(f\"Filtering channels by follower count (>= {config.NETWORK_MIN_FOLLOWER_COUNT})...\")\n",
    "    channels_df_filtered_fc = channels_df[channels_df['follower_count'] >= config.NETWORK_MIN_FOLLOWER_COUNT]\n",
    "    print(f\" -> Channels after follower count filter: {len(channels_df_filtered_fc)}\")\n",
    "\n",
    "    if not videos_df.empty:\n",
    "        print(f\"Filtering channels by video count (>= {config.NETWORK_MIN_CHANNEL_VIDEO_COUNT})...\")\n",
    "        video_counts_per_channel = videos_df['channel_id'].value_counts()\n",
    "        channels_with_enough_videos = video_counts_per_channel[video_counts_per_channel >= config.NETWORK_MIN_CHANNEL_VIDEO_COUNT].index.tolist()\n",
    "        channels_df_net = channels_df_filtered_fc[channels_df_filtered_fc['id'].isin(channels_with_enough_videos)]\n",
    "        print(f\" -> Channels after video count filter: {len(channels_df_net)}\")\n",
    "    else:\n",
    "        print(\"Warning: videos_df not available for filtering by video count. Using only follower count filter for channels.\")\n",
    "        channels_df_net = channels_df_filtered_fc\n",
    "\n",
    "    valid_channel_ids_for_network = set(channels_df_net['id'])\n",
    "    print(f\"Total channels passing node filters: {len(valid_channel_ids_for_network)}\")\n",
    "\n",
    "    # --- 3. Filter Collaborations (Edges) ---\n",
    "    if not collab_df_full.empty:\n",
    "        print(f\"Filtering collaboration edges by count (>= {config.NETWORK_MIN_COLLABORATION_COUNT})...\")\n",
    "        collab_df_filtered_count = collab_df_full[collab_df_full['collaboration_count'] >= config.NETWORK_MIN_COLLABORATION_COUNT]\n",
    "        print(f\" -> Edges after count filter: {len(collab_df_filtered_count)}\")\n",
    "\n",
    "        print(\"Filtering edges to ensure both connected channels passed node filters...\")\n",
    "        collab_df_net = collab_df_filtered_count[\n",
    "            collab_df_filtered_count['channel_id_1'].isin(valid_channel_ids_for_network) &\n",
    "            collab_df_filtered_count['channel_id_2'].isin(valid_channel_ids_for_network)\n",
    "        ].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "        print(f\" -> Edges after node validity filter: {len(collab_df_net)}\")\n",
    "\n",
    "        # --- Cap outlier durations before creating graph ---\n",
    "        duration_threshold_weeks = config.NETWORK_DURATION_OUTLIER_WEEKS\n",
    "        duration_threshold_seconds = duration_threshold_weeks * 7 * 24 * 3600\n",
    "\n",
    "        outlier_edges = collab_df_net['total_collaboration_duration_seconds'] > duration_threshold_seconds\n",
    "        outlier_count = outlier_edges.sum()\n",
    "\n",
    "        if outlier_count > 0:\n",
    "            # The f-string now uses the config variable directly for accurate reporting\n",
    "            print(f\"Capping duration for {outlier_count} edge(s) with duration > {duration_threshold_weeks} week(s) for network analysis.\")\n",
    "\n",
    "            # Use .loc to safely modify the DataFrame\n",
    "            collab_df_net.loc[outlier_edges, 'total_collaboration_duration_seconds'] = duration_threshold_seconds\n",
    "\n",
    "    else:\n",
    "        print(\"Full collaboration data (collab_df_full) is empty. Filtered collaboration data will be empty.\")\n",
    "        collab_df_net = pd.DataFrame()\n",
    "\n",
    "    # --- 4. Create Filtered NetworkX Graph ---\n",
    "    if not collab_df_net.empty:\n",
    "        print(\"Creating NetworkX graph G_filtered from filtered data...\")\n",
    "        G_filtered = nx.from_pandas_edgelist(\n",
    "            collab_df_net,\n",
    "            'channel_id_1',\n",
    "            'channel_id_2',\n",
    "            edge_attr=['collaboration_count', 'total_collaboration_duration_seconds', 'latest_collaboration_timestamp']\n",
    "        )\n",
    "        G_filtered.add_nodes_from(valid_channel_ids_for_network)\n",
    "        print(f\"Filtered graph G_filtered created with {G_filtered.number_of_nodes()} nodes and {G_filtered.number_of_edges()} edges.\")\n",
    "    else:\n",
    "        print(\"No edges passed all filters. Filtered graph G_filtered will be empty or contain only isolated nodes.\")\n",
    "        if valid_channel_ids_for_network: G_filtered.add_nodes_from(valid_channel_ids_for_network)\n",
    "        print(f\"Filtered graph G_filtered has {G_filtered.number_of_nodes()} nodes and {G_filtered.number_of_edges()} edges.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or filtering data for network analysis: {e}\")\n",
    "    logging.error(\"Error in network data prep:\", exc_info=True)\n",
    "    if 'G_filtered' not in locals(): G_filtered = nx.Graph()\n",
    "\n",
    "print(\"--- Network Data Preparation Complete ---\")"
   ],
   "id": "e24a067f29d07663",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell: Deeper Collaboration Analysis\n",
    "\n",
    "if 'mentions_df' in locals() and not mentions_df.empty and 'channels_df' in locals() and not channels_df.empty:\n",
    "    print(\"\\n--- Deeper Collaboration Statistics ---\")\n",
    "\n",
    "    # --- 1. What percent of channels have 0 collaborations? ---\n",
    "    print(\"\\n[Network Participation]\")\n",
    "    total_channels = len(channels_df)\n",
    "    source_channels = set(mentions_df['source_channel_id'])\n",
    "    target_channels = set(mentions_df['target_channel_id'])\n",
    "    collaborating_channel_ids = source_channels.union(target_channels)\n",
    "    num_collaborating_channels = len(collaborating_channel_ids)\n",
    "    num_zero_collabs = total_channels - num_collaborating_channels\n",
    "    percent_zero_collabs = (num_zero_collabs / total_channels) * 100 if total_channels > 0 else 0\n",
    "\n",
    "    print(f\"Total channels in database: {total_channels}\")\n",
    "    print(f\"Channels with at least one collaboration (as source or target): {num_collaborating_channels}\")\n",
    "    print(f\"Channels with zero collaborations: {num_zero_collabs} ({percent_zero_collabs:.1f}%)\")\n",
    "\n",
    "    # --- 2. Among channels with collaborations, what percent of their videos are collaborations? ---\n",
    "    print(\"\\n[Collaboration Content Strategy]\")\n",
    "    total_video_counts = videos_df[videos_df['channel_id'].isin(collaborating_channel_ids)]['channel_id'].value_counts()\n",
    "    collab_video_counts = mentions_df.groupby('source_channel_id')['video_id'].nunique()\n",
    "\n",
    "    collab_stats_df = pd.DataFrame({'total_videos': total_video_counts, 'collab_videos': collab_video_counts}).fillna(0)\n",
    "    collab_stats_df = collab_stats_df[collab_stats_df['total_videos'] > 0] # Avoid division by zero\n",
    "    collab_stats_df['collab_video_percentage'] = (collab_stats_df['collab_videos'] / collab_stats_df['total_videos']) * 100\n",
    "\n",
    "    median_percentage = collab_stats_df['collab_video_percentage'].median()\n",
    "    mean_percentage = collab_stats_df['collab_video_percentage'].mean()\n",
    "    print(f\"Among channels that collaborate, the median channel has {median_percentage:.2f}% of their videos containing mentions.\")\n",
    "    print(f\"The mean percentage of videos containing mentions is {mean_percentage:.2f}%.\")\n",
    "\n",
    "    # --- 3. Distribution of number of collaborators in a single collaboration (video) ---\n",
    "    print(\"\\n[Collaboration Size Distribution]\")\n",
    "    collabs_per_video = mentions_df.groupby('video_id')['target_channel_id'].nunique()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.countplot(x=collabs_per_video, order=collabs_per_video.value_counts().index, palette='magma')\n",
    "    ax.set_title('Distribution of Collaborators per Video')\n",
    "    ax.set_xlabel('Number of Unique Channels Mentioned in a Single Video')\n",
    "    ax.set_ylabel('Number of Videos')\n",
    "    # Limit x-axis for readability if there are extreme outliers\n",
    "    if collabs_per_video.max() > 15:\n",
    "        ax.set_xlim(-1, 15)\n",
    "        ax.text(0.98, 0.98, 'Note: X-axis limited to 15 for readability', transform=ax.transAxes, ha='right', va='top')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- 4 & 5. Top channels by total collaborations and unique collaborators ---\n",
    "    # Prepare data for both stats\n",
    "    mentions_made = mentions_df['source_channel_id'].value_counts().rename('mentions_made')\n",
    "    mentions_received = mentions_df['target_channel_id'].value_counts().rename('mentions_received')\n",
    "\n",
    "    if 'G_full_for_viz' in locals() and G_full_for_viz.number_of_nodes() > 0:\n",
    "        full_degree = pd.Series(dict(G_full_for_viz.degree()), name='unique_collaborators')\n",
    "    else: # Fallback if full graph not built yet\n",
    "        G_full_temp = nx.from_pandas_edgelist(collab_df_full, 'channel_id_1', 'channel_id_2')\n",
    "        full_degree = pd.Series(dict(G_full_temp.degree()), name='unique_collaborators')\n",
    "\n",
    "    # Combine into a single DataFrame\n",
    "    leaderboard_df = pd.concat([mentions_made, mentions_received, full_degree], axis=1).fillna(0).astype(int)\n",
    "    leaderboard_df['total_mentions'] = leaderboard_df['mentions_made'] + leaderboard_df['mentions_received']\n",
    "\n",
    "    # Merge with channel names\n",
    "    leaderboard_df = leaderboard_df.merge(channels_df[['id', 'login', 'display_name', 'follower_count']], left_index=True, right_on='id', how='left')\n",
    "\n",
    "    print(\"\\n[Collaboration Leaderboards (Unfiltered)]\")\n",
    "    print(\"\\n--- Top 10 Channels by Total Mentions (Made + Received) ---\")\n",
    "    display(leaderboard_df.sort_values('total_mentions', ascending=False).head(10)[\n",
    "        ['login', 'display_name', 'follower_count', 'total_mentions', 'mentions_made', 'mentions_received']\n",
    "    ])\n",
    "\n",
    "    print(\"\\n--- Top 10 Channels by Unique Collaborators (Degree) ---\")\n",
    "    display(leaderboard_df.sort_values('unique_collaborators', ascending=False).head(10)[\n",
    "        ['login', 'display_name', 'follower_count', 'unique_collaborators', 'total_mentions']\n",
    "    ])\n",
    "else:\n",
    "    print(\"\\nMentions data or Channels data not loaded. Skipping deeper collaboration analysis.\")"
   ],
   "id": "8fa13f90b3984e3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell: Data Exploration - Channel Tags\n",
    "\n",
    "print(\"\\n--- Channel Tag Analysis (on Filtered Channel Set) ---\")\n",
    "\n",
    "if 'channels_df_net' in locals() and not channels_df_net.empty and 'tags' in channels_df_net.columns:\n",
    "    # The 'tags' column is a JSON string, so we need to parse it.\n",
    "    # We'll also handle potential errors or empty/null entries.\n",
    "    def parse_tags(tags_json):\n",
    "        if pd.isna(tags_json):\n",
    "            return []\n",
    "        try:\n",
    "            return json.loads(tags_json)\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            return []\n",
    "\n",
    "    # Explode the DataFrame so each tag gets its own row\n",
    "    tags_series = channels_df_net['tags'].apply(parse_tags)\n",
    "    all_tags = tags_series.explode().dropna()\n",
    "\n",
    "    if not all_tags.empty:\n",
    "        print(f\"Found {all_tags.nunique()} unique tags across {len(channels_df_net)} channels.\")\n",
    "\n",
    "        top_n_tags = 20\n",
    "        tag_counts = all_tags.value_counts().nlargest(top_n_tags)\n",
    "\n",
    "        print(f\"\\nTop {top_n_tags} Most Common Tags:\")\n",
    "\n",
    "        # Visualize the top tags\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.barplot(x=tag_counts.values, y=tag_counts.index, palette='mako')\n",
    "        plt.title(f'Top {top_n_tags} Most Common Channel Tags in Filtered Network')\n",
    "        plt.xlabel('Number of Channels')\n",
    "        plt.ylabel('Tag')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No tags found to analyze in the filtered channel set.\")\n",
    "else:\n",
    "    print(\"Filtered channel data (`channels_df_net`) not available or 'tags' column is missing.\")"
   ],
   "id": "586c7863ee4fa6da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 13: Display Sample Filtered Collaboration Data\n",
    "print(\"\\nSample Filtered Collaboration Edges (collab_df_net):\")\n",
    "if not collab_df_net.empty:\n",
    "    display(collab_df_net.head())\n",
    "else:\n",
    "    print(\"No collaboration data in collab_df_net (all edges filtered out or none exist).\")\n"
   ],
   "id": "503c6eba174063ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 14: Filtered Collaboration Network Statistics\n",
    "if 'G_filtered' in locals() and G_filtered.number_of_nodes() > 0:\n",
    "    print(\"\\n--- Filtered Collaboration Network Statistics ---\")\n",
    "    print(f\"Total Edges in Filtered Network: {G_filtered.number_of_edges()}\")\n",
    "    print(f\"Total Nodes in Filtered Network: {G_filtered.number_of_nodes()}\")\n",
    "\n",
    "    if G_filtered.number_of_nodes() > 0:\n",
    "        degree_sequence = [d for n, d in G_filtered.degree()]\n",
    "        degree_df_filtered = pd.DataFrame({'channel_id': list(G_filtered.nodes()), 'degree': degree_sequence})\n",
    "\n",
    "        print(\"\\nDegree Distribution Summary (Filtered Network):\")\n",
    "        print(degree_df_filtered['degree'].describe())\n",
    "\n",
    "        # Merge with channel names for context\n",
    "        channels_for_labels_df = channels_df[['id', 'login', 'display_name']].rename(columns={'id': 'channel_id'})\n",
    "        degree_df_filtered = pd.merge(degree_df_filtered, channels_for_labels_df, on='channel_id', how='left')\n",
    "\n",
    "        print(\"\\nTop 10 Channels by Degree (Filtered Network):\")\n",
    "        print(degree_df_filtered.nlargest(10, 'degree'))\n",
    "    else:\n",
    "        print(\"Filtered graph has no nodes to calculate degree.\")\n",
    "\n",
    "    # Summary for edges in collab_df_net\n",
    "    if not collab_df_net.empty:\n",
    "        print(\"\\nCollaboration Count per Edge Summary (Filtered Network):\")\n",
    "        print(collab_df_net['collaboration_count'].describe())\n",
    "\n",
    "        # --- Human-Readable Duration Summary ---\n",
    "        print(\"\\nTotal Collaboration Duration per Edge Summary (Filtered Network):\")\n",
    "        duration_stats = collab_df_net['total_collaboration_duration_seconds'].describe()\n",
    "\n",
    "        # Create and print a formatted summary\n",
    "        formatted_stats = {\n",
    "            'count': f\"{duration_stats['count']:.0f}\",\n",
    "            'mean': format_seconds_to_hm(duration_stats['mean']),\n",
    "            'std dev': f\"~{format_seconds_to_hm(duration_stats['std'])}\",\n",
    "            'min': format_seconds_to_hm(duration_stats['min']),\n",
    "            '25%': format_seconds_to_hm(duration_stats['25%']),\n",
    "            '50% (median)': format_seconds_to_hm(duration_stats['50%']),\n",
    "            '75%': format_seconds_to_hm(duration_stats['75%']),\n",
    "            'max': format_seconds_to_hm(duration_stats['max'])\n",
    "        }\n",
    "\n",
    "        for key, value in formatted_stats.items():\n",
    "            # Left-align the key, right-align the value for clean output\n",
    "            print(f\"{key:<15} {value:>15}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No edges in collab_df_net to summarize.\")\n",
    "else:\n",
    "    print(\"\\nFiltered graph G_filtered is empty. No statistics to display.\")"
   ],
   "id": "dc914e19f65da2d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 15: Filtered Collaboration Network Visualizations\n",
    "if 'G_filtered' in locals() and G_filtered.number_of_nodes() > 0 and 'degree_df_filtered' in locals() and not degree_df_filtered.empty:\n",
    "    print(\"\\n--- Filtered Collaboration Network Visualizations ---\")\n",
    "\n",
    "    # --- 1. Degree Distribution Histogram ---\n",
    "    print(\"Generating Degree Distribution Histogram for the filtered network...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    degrees_filtered = degree_df_filtered[degree_df_filtered['degree'] > 0]['degree']\n",
    "    if not degrees_filtered.empty:\n",
    "        # Log-log scale is common for degree distributions to check for power-law behavior\n",
    "        sns.histplot(degrees_filtered, log_scale=True, bins=30)\n",
    "        plt.title('Degree Distribution of Filtered Collaboration Network (Log-Log Scale)')\n",
    "        plt.xlabel('Degree (Number of Unique Collaborators) - Log Scale')\n",
    "        plt.ylabel('Number of Channels - Log Scale')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No nodes with degree > 0 found to plot.\")\n",
    "\n",
    "    # --- 2. Edge Weight (Collaboration Count) Plot ---\n",
    "    if not collab_df_net.empty:\n",
    "        print(\"\\nGenerating Edge Weight (Collaboration Count) Plot...\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        collab_counts_filtered = collab_df_net[collab_df_net['collaboration_count'] > 0]['collaboration_count']\n",
    "\n",
    "        if not collab_counts_filtered.empty:\n",
    "            # A countplot is more direct and robust for this type of discrete integer data\n",
    "            ax = sns.countplot(x=collab_counts_filtered, palette='viridis', order = sorted(collab_counts_filtered.unique()))\n",
    "\n",
    "            # Add text labels on top of each bar\n",
    "            for p in ax.patches:\n",
    "                ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                            ha='center', va='center', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "            plt.title('Distribution of Collaboration Counts per Edge (Filtered Network)')\n",
    "            plt.xlabel('Number of Collaborations on a Single Edge')\n",
    "\n",
    "            # Conditionally apply log scale to y-axis only if counts are high\n",
    "            max_count = collab_counts_filtered.value_counts().max()\n",
    "            if max_count > 10:\n",
    "                plt.yscale('log')\n",
    "                plt.ylabel('Number of Edges (Log Scale)')\n",
    "            else:\n",
    "                plt.ylabel('Number of Edges (Linear Scale)')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No collaborations with count > 0 found to plot.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nFiltered graph or degree data insufficient for visualization. Skipping.\")"
   ],
   "id": "2c402ce1f749e696",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Community Detection in Collaboration Network",
   "id": "a74cb851323c91af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell: Helper function for Community Naming via Characteristic Tags\n",
    "\n",
    "import numpy as np # For log calculation\n",
    "\n",
    "def get_community_labels(channels_df, partition):\n",
    "    \"\"\"\n",
    "    Generates a name for each community based on its most \"characteristic\" tags.\n",
    "    A characteristic tag is one that is frequent within the community but relatively\n",
    "    rare in the overall network (TF-IDF inspired).\n",
    "    \"\"\"\n",
    "    print(\"Generating community names based on characteristic tags...\")\n",
    "    community_labels = {}\n",
    "\n",
    "    # Get a mapping of community ID to its list of member channel IDs\n",
    "    communities = {}\n",
    "    for channel_id, community_id in partition.items():\n",
    "        if community_id not in communities:\n",
    "            communities[community_id] = []\n",
    "        communities[community_id].append(channel_id)\n",
    "\n",
    "    # Prepare tag data\n",
    "    channels_with_tags = channels_df[channels_df['id'].isin(partition.keys())].copy()\n",
    "    channels_with_tags['tags_list'] = channels_with_tags['tags'].apply(\n",
    "        lambda x: json.loads(x) if pd.notna(x) else []\n",
    "    )\n",
    "\n",
    "    # Calculate global tag frequencies (how many channels have a given tag)\n",
    "    all_tags_exploded = channels_with_tags['tags_list'].explode()\n",
    "    global_tag_counts = all_tags_exploded.value_counts()\n",
    "\n",
    "    total_channels_in_network = len(channels_with_tags)\n",
    "\n",
    "    for cid, members in communities.items():\n",
    "        if cid == -1: continue # Skip outlier community\n",
    "\n",
    "        community_channels = channels_with_tags[channels_with_tags['id'].isin(members)]\n",
    "\n",
    "        # Get all tags within this specific community\n",
    "        community_tags_exploded = community_channels['tags_list'].explode()\n",
    "\n",
    "        if community_tags_exploded.dropna().empty:\n",
    "            community_labels[cid] = f\"Community {cid} (No Tags)\"\n",
    "            continue\n",
    "\n",
    "        # Calculate local frequency (TF) for tags within this community\n",
    "        local_tag_counts = community_tags_exploded.value_counts()\n",
    "\n",
    "        # Calculate characteristic score for each tag in the community\n",
    "        tag_scores = {}\n",
    "        for tag, local_count in local_tag_counts.items():\n",
    "            global_count = global_tag_counts.get(tag, 1) # Get global count, default to 1 to avoid division by zero\n",
    "\n",
    "            # TF-IDF-like score: (frequency in community) * log(rarity in whole network)\n",
    "            tf = local_count / len(community_tags_exploded)\n",
    "            idf = np.log(total_channels_in_network / global_count)\n",
    "            tag_scores[tag] = tf * idf\n",
    "\n",
    "        # Get the top 3 tags with the highest characteristic score\n",
    "        if tag_scores:\n",
    "            top_tags = sorted(tag_scores.items(), key=lambda item: item[1], reverse=True)[:3]\n",
    "            community_name = \", \".join([tag for tag, score in top_tags])\n",
    "            community_labels[cid] = community_name\n",
    "        else:\n",
    "            community_labels[cid] = f\"Community {cid}\"\n",
    "\n",
    "    print(\"Community names generated.\")\n",
    "    return community_labels"
   ],
   "id": "e134ade34f1c901a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell: Community Detection Setup and Execution\n",
    "\n",
    "import community as community_louvain # python-louvain library\n",
    "\n",
    "communities_detected = False\n",
    "partition = {} # Ensure partition is defined\n",
    "communities = {} # Ensure communities is defined\n",
    "community_labels = {} # Ensure labels dict is defined\n",
    "\n",
    "if 'G_filtered' in locals() and G_filtered.number_of_nodes() > 0:\n",
    "    print(f\"\\n--- Performing Louvain Community Detection on FILTERED graph at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "    print(f\"Using 'collaboration_count' as edge weight for community detection.\")\n",
    "\n",
    "    # Ensure 'weight' attribute is set for Louvain\n",
    "    for u, v, data in G_filtered.edges(data=True):\n",
    "        if 'weight' not in data:\n",
    "            G_filtered.edges[u,v]['weight'] = data.get('collaboration_count', 1)\n",
    "\n",
    "    start_time_community = time.time()\n",
    "    try:\n",
    "        partition = community_louvain.best_partition(G_filtered, weight='weight', random_state=42)\n",
    "        modularity = community_louvain.modularity(partition, G_filtered, weight='weight')\n",
    "\n",
    "        # Group nodes by community ID\n",
    "        for node, community_id in partition.items():\n",
    "            if community_id not in communities: communities[community_id] = []\n",
    "            communities[community_id].append(node)\n",
    "\n",
    "        num_communities = len(communities)\n",
    "        print(f\"Louvain Community Detection complete in {time.time() - start_time_community:.2f}s.\")\n",
    "        print(f\"Found {num_communities} communities.\")\n",
    "        print(f\"Modularity of the partition: {modularity:.4f}\")\n",
    "\n",
    "        # This uses the helper function which should now be in a preceding cell.\n",
    "        if 'get_community_labels' in locals():\n",
    "            community_labels = get_community_labels(channels_df_net, partition)\n",
    "        else:\n",
    "            print(\"\\nWarning: get_community_labels function not defined yet. Skipping tag-based naming.\")\n",
    "\n",
    "        # Analyze and print top communities with their new names\n",
    "        sorted_communities_by_size = sorted(communities.items(), key=lambda item: len(item[1]), reverse=True)\n",
    "\n",
    "        print(\"\\n--- Top 5 Largest Communities ---\")\n",
    "        for i in range(min(5, len(sorted_communities_by_size))):\n",
    "            cid, nodes = sorted_communities_by_size[i]\n",
    "\n",
    "            # Get the new tag-based name for this community\n",
    "            community_name = community_labels.get(cid, f\"Community {cid}\")\n",
    "\n",
    "            # Get channel names for a few members\n",
    "            member_names = [\n",
    "                channels_for_labels_df.loc[channels_for_labels_df['channel_id'] == node_id, 'login'].iloc[0]\n",
    "                for node_id in nodes[:3]\n",
    "                if not channels_for_labels_df[channels_for_labels_df['channel_id'] == node_id].empty\n",
    "            ]\n",
    "\n",
    "            # Print the enhanced summary\n",
    "            print(f\"\\n  Community {cid} (Size rank {i+1}): {len(nodes)} members.\")\n",
    "            print(f\"    -> Characteristic Tags: {community_name}\")\n",
    "            print(f\"    -> Example Members: {', '.join(member_names[:3])}...\")\n",
    "\n",
    "        communities_detected = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during community detection: {e}\")\n",
    "        logging.error(\"Community detection error\", exc_info=True)\n",
    "else:\n",
    "    print(\"\\nFiltered graph G_filtered not found or is empty. Skipping community detection.\")"
   ],
   "id": "23b293ec0907b0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell: Influential Channel Analysis\n",
    "\n",
    "if 'G_filtered' in locals() and G_filtered.number_of_nodes() > 0:\n",
    "    print(\"\\n--- Analyzing Influential Channels in the Filtered Network ---\")\n",
    "\n",
    "    # --- 1. Calculate PageRank Score for the Overall Graph ---\n",
    "    # We now only calculate PageRank, which is much faster.\n",
    "    print(\"Calculating PageRank for the overall graph...\")\n",
    "    start_time_centrality = time.time()\n",
    "\n",
    "    # Using collaboration_count as weight makes the scores more meaningful\n",
    "    pagerank_scores = nx.pagerank(G_filtered, weight='collaboration_count')\n",
    "\n",
    "    print(f\" -> PageRank calculation complete in {time.time() - start_time_centrality:.2f}s.\")\n",
    "\n",
    "    # --- 2. Combine Scores into a single, rich DataFrame ---\n",
    "    centrality_df = degree_df_filtered.copy() # Start with degree and names\n",
    "    centrality_df['pagerank'] = centrality_df['channel_id'].map(pagerank_scores)\n",
    "\n",
    "    # Merge with the main channels_df to get follower counts\n",
    "    centrality_df = pd.merge(\n",
    "        centrality_df,\n",
    "        channels_df[['id', 'follower_count']],\n",
    "        left_on='channel_id',\n",
    "        right_on='id',\n",
    "        how='left'\n",
    "    ).drop(columns=['id'])\n",
    "\n",
    "    # --- 3. Report Top Channels - Overall Network ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"--- Top 10 Most Influential Channels (Overall) by PageRank ---\")\n",
    "    print(\"PageRank measures influence based on connections to other influential channels.\")\n",
    "    # Display without the 'betweenness' column\n",
    "    display(centrality_df.sort_values('pagerank', ascending=False).head(10)[\n",
    "        ['login', 'display_name', 'degree', 'follower_count', 'pagerank']\n",
    "    ])\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- 4. Report Top Channels - Within Each Top Community ---\n",
    "    print(\"\\n\\n--- Most Influential Channels within Top Communities (by PageRank) ---\")\n",
    "    if 'sorted_communities_by_size' in locals() and community_labels:\n",
    "        # Iterate through the top 5 communities we found and named earlier\n",
    "        for i in range(min(5, len(sorted_communities_by_size))):\n",
    "            cid, nodes = sorted_communities_by_size[i]\n",
    "            community_name = community_labels.get(cid, f\"Community {cid}\")\n",
    "\n",
    "            print(f\"\\n- Community {i+1}: '{community_name}' ({len(nodes)} members)\")\n",
    "\n",
    "            # Create a subgraph containing only the nodes of this community\n",
    "            community_subgraph = G_filtered.subgraph(nodes)\n",
    "\n",
    "            if community_subgraph.number_of_nodes() > 1:\n",
    "                # Calculate PageRank *within this community only*\n",
    "                pagerank_community = nx.pagerank(community_subgraph, weight='collaboration_count')\n",
    "\n",
    "                # Get the centrality_df rows for members of this community\n",
    "                community_centrality_df = centrality_df[centrality_df['channel_id'].isin(nodes)].copy()\n",
    "\n",
    "                # Map the *intra-community* PageRank score\n",
    "                community_centrality_df['pagerank_in_community'] = community_centrality_df['channel_id'].map(pagerank_community)\n",
    "\n",
    "                # Display the top 3 most influential channels *for this community*\n",
    "                display(community_centrality_df.sort_values('pagerank_in_community', ascending=False).head(3)[\n",
    "                    ['login', 'display_name', 'degree', 'follower_count', 'pagerank_in_community']\n",
    "                ])\n",
    "            else:\n",
    "                print(\"  (Community is too small to analyze for internal influence)\")\n",
    "    else:\n",
    "        print(\"Community data not available to perform per-community analysis.\")\n",
    "else:\n",
    "    print(\"\\nFiltered graph is empty. Skipping centrality analysis.\")"
   ],
   "id": "266a915a261265f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualizing Communities",
   "id": "cd49e319fa29ca8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell: Visualize Subgraph with Community Colors (with Labels and Legend)\n",
    "\n",
    "from matplotlib.lines import Line2D # Needed for custom legend\n",
    "import numpy as np # Needed for centroid calculation\n",
    "\n",
    "if communities_detected and 'G_filtered' in locals() and G_filtered.number_of_nodes() > 0:\n",
    "    print(\"\\n--- NetworkX Subgraph Visualization with Community Colors & Labels ---\")\n",
    "    try:\n",
    "        # --- 1. Generate Community Labels using our helper function ---\n",
    "        community_labels = get_community_labels(channels_df_net, partition)\n",
    "\n",
    "        # --- 2. Create the subgraph for visualization ---\n",
    "        top_nodes_filtered = degree_df_filtered.nlargest(config.NETWORK_VIZ_TOP_N_CHANNELS_BY_DEGREE, 'degree')['channel_id'].tolist()\n",
    "        subgraph_nodes_set = set(top_nodes_filtered)\n",
    "        max_subgraph_nodes = config.NETWORK_VIZ_MAX_SUBGRAPH_NODES\n",
    "\n",
    "        for node in list(subgraph_nodes_set):\n",
    "            if G_filtered.has_node(node) and len(subgraph_nodes_set) < max_subgraph_nodes:\n",
    "                neighbors = list(G_filtered.neighbors(node));\n",
    "                subgraph_nodes_set.update(neighbors[:max_subgraph_nodes - len(subgraph_nodes_set)])\n",
    "\n",
    "        subgraph = G_filtered.subgraph(list(subgraph_nodes_set))\n",
    "        print(f\"Creating subgraph visualization with {subgraph.number_of_nodes()} nodes...\")\n",
    "\n",
    "        if subgraph.number_of_nodes() > 0:\n",
    "            # --- 3. Use community-clustered layout ---\n",
    "            layout_graph = subgraph.copy()\n",
    "            for u, v in layout_graph.edges():\n",
    "                if partition.get(u) == partition.get(v): layout_graph.edges[u,v]['weight'] = 5\n",
    "                else: layout_graph.edges[u,v]['weight'] = 0.1\n",
    "            pos_subgraph = nx.spring_layout(layout_graph, weight='weight', k=0.4, iterations=50, seed=42)\n",
    "\n",
    "            # --- 4. Prepare Visual Properties (Colors, Sizes, etc.) ---\n",
    "            # THIS IS THE ONE TRUE COLOR MAP AND INDEXING\n",
    "            unique_community_ids_in_subgraph = sorted(list(set(partition[node] for node in subgraph.nodes() if node in partition)))\n",
    "            community_to_color_idx = {cid: i for i, cid in enumerate(unique_community_ids_in_subgraph)}\n",
    "            num_colors = len(unique_community_ids_in_subgraph)\n",
    "            cmap = plt.cm.get_cmap('tab20', num_colors) if num_colors <= 20 else plt.cm.get_cmap('viridis', num_colors)\n",
    "\n",
    "            node_colors = [cmap(community_to_color_idx.get(partition.get(node), -1)) for node in subgraph.nodes()]\n",
    "            node_data_subgraph = channels_df[channels_df['id'].isin(subgraph.nodes())].set_index('id')\n",
    "            node_sizes = [math.log10(max(1, node_data_subgraph.loc[node,'follower_count'] if node in node_data_subgraph.index else 1)+1) * 300 + 100 for node in subgraph.nodes()]\n",
    "            node_labels = pd.merge(pd.DataFrame({'channel_id': list(subgraph.nodes())}), channels_for_labels_df, on='channel_id', how='left').set_index('channel_id')['login'].to_dict()\n",
    "            edge_widths = [math.log10(max(0, d.get('collaboration_count',0)) + 1) * 1.5 + 0.5 for u, v, d in subgraph.edges(data=True)]\n",
    "\n",
    "            # --- 5. Drawing the Graph ---\n",
    "            plt.figure(figsize=(20, 16))\n",
    "            ax = plt.gca()\n",
    "\n",
    "            nx.draw_networkx_nodes(subgraph, pos_subgraph, node_size=node_sizes, node_color=node_colors, alpha=0.9, ax=ax)\n",
    "            nx.draw_networkx_edges(subgraph, pos_subgraph, width=edge_widths, alpha=0.2, edge_color='gray', ax=ax)\n",
    "            nx.draw_networkx_labels(subgraph, pos_subgraph, labels=node_labels, font_size=8, ax=ax)\n",
    "\n",
    "            # --- 6. Draw the community labels (centroids) on the graph ---\n",
    "            community_centroids = {}\n",
    "            for cid in unique_community_ids_in_subgraph:\n",
    "                if cid != -1:\n",
    "                    community_nodes_in_subgraph = [node for node in subgraph.nodes() if partition.get(node) == cid]\n",
    "                    if community_nodes_in_subgraph:\n",
    "                        x_coords = [pos_subgraph[node][0] for node in community_nodes_in_subgraph]\n",
    "                        y_coords = [pos_subgraph[node][1] for node in community_nodes_in_subgraph]\n",
    "                        community_centroids[cid] = (np.mean(x_coords), np.mean(y_coords))\n",
    "\n",
    "            for cid, pos in community_centroids.items():\n",
    "                label_text = community_labels.get(cid, \"\")\n",
    "                ax.text(pos[0], pos[1] + 0.08, s=label_text,\n",
    "                        horizontalalignment='center',\n",
    "                        fontdict={'size': 10, 'color': 'black', 'weight': 'bold'},\n",
    "                        bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', boxstyle='round,pad=0.4'))\n",
    "\n",
    "            plt.title('Collaboration Subgraph with Community Clusters', fontsize=16)\n",
    "            plt.axis('off'); plt.tight_layout(); plt.show()\n",
    "\n",
    "            # --- 7. Generate a separate, clean legend ---\n",
    "            # THIS SECTION IS NOW CORRECTED\n",
    "            communities_in_subgraph_sorted = sorted(\n",
    "                unique_community_ids_in_subgraph,\n",
    "                key=lambda cid: len([n for n in subgraph.nodes() if partition.get(n) == cid]),\n",
    "                reverse=True\n",
    "            )\n",
    "\n",
    "            legend_elements = []\n",
    "            # We now use the SAME cmap and community_to_color_idx from step 4\n",
    "            for cid in communities_in_subgraph_sorted:\n",
    "                if cid == -1:\n",
    "                    label = \"Outliers\"\n",
    "                    color = 'grey'\n",
    "                else:\n",
    "                    label = community_labels.get(cid, f\"Community {cid}\")\n",
    "                    # Use the correct color mapping\n",
    "                    color = cmap(community_to_color_idx.get(cid))\n",
    "\n",
    "                legend_elements.append(Line2D([0], [0], marker='o', color='w',\n",
    "                                              label=f'{label}',\n",
    "                                              markerfacecolor=color, markersize=12))\n",
    "            if legend_elements:\n",
    "                fig_legend, ax_legend = plt.subplots(figsize=(10, len(legend_elements) * 0.5))\n",
    "                ax_legend.legend(handles=legend_elements, title=\"Community Top Tags\", loc='center', fontsize=12, title_fontsize=14)\n",
    "                ax_legend.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during community visualization: {e}\")\n",
    "        logging.error(\"Community visualization error\", exc_info=True)\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo communities detected or filtered graph is empty. Skipping community visualization.\")"
   ],
   "id": "d9ddeff2aec48950",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Collaboration Topic Modeling with BERTopic\n",
    "\n",
    "Here, we derive context by analyzing the content of video titles. We use BERTopic to automatically discover topics from the titles of videos belonging to channels in our filtered collaboration network. This helps us understand what collaborating streamers talk about or play, without relying on pre-defined categories.\n"
   ],
   "id": "6e24edeb30abcc84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell: BERTopic Pre-processing: Assign Topic ID to All Relevant Videos\n",
    "\n",
    "# This cell trains the BERTopic model on a large sample of video titles,\n",
    "# optionally reduces outliers, and then assigns a topic ID to each video.\n",
    "\n",
    "# Initialize global-like variables to hold the results\n",
    "topic_model = None\n",
    "videos_with_topics_df = None\n",
    "\n",
    "print(\"\\n--- Starting Collaboration Topic Modeling with BERTopic ---\")\n",
    "\n",
    "# --- 1. Main Gate: Check if we have a filtered graph to work with ---\n",
    "if 'G_filtered' in locals() and G_filtered.number_of_nodes() > 0:\n",
    "    # --- 2. If graph exists, proceed with data preparation and modeling ---\n",
    "    try:\n",
    "        from bertopic import BERTopic\n",
    "        import nltk\n",
    "        from nltk.corpus import stopwords\n",
    "        import re\n",
    "\n",
    "        # Set to True to run the outlier reduction step, False to skip it.\n",
    "        run_outlier_reduction = True\n",
    "\n",
    "        # Setup NLTK stopwords\n",
    "        try:\n",
    "            stopwords.words('english')\n",
    "        except LookupError:\n",
    "            print(\"NLTK stopwords not found. Downloading...\")\n",
    "            nltk.download('stopwords')\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        mention_regex = re.compile(r'@([a-zA-Z0-9_]{4,25})')\n",
    "\n",
    "        def clean_title(title):\n",
    "            if not isinstance(title, str): return \"\"\n",
    "            text = mention_regex.sub('', title)\n",
    "            text = re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n",
    "            text = text.lower()\n",
    "            text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "            return text\n",
    "\n",
    "        print(\"Preparing data for topic modeling...\")\n",
    "        nodes_in_network = list(G_filtered.nodes())\n",
    "        videos_for_modeling_df = videos_df[videos_df['channel_id'].isin(nodes_in_network)].copy()\n",
    "        print(f\"Found {len(videos_for_modeling_df)} videos from {len(nodes_in_network)} channels in the filtered network.\")\n",
    "\n",
    "        print(f\"Filtering for English language videos...\")\n",
    "        english_videos_df = videos_for_modeling_df[videos_for_modeling_df['language'] == 'en'].copy()\n",
    "        print(f\" -> Found {len(english_videos_df)} English videos to model.\")\n",
    "\n",
    "        print(\"Cleaning video titles (removing stopwords and mentions)...\")\n",
    "        english_videos_df['cleaned_title'] = english_videos_df['title'].dropna().apply(clean_title)\n",
    "\n",
    "        docs_df = english_videos_df[english_videos_df['cleaned_title'].str.len() > 0].copy()\n",
    "        docs = docs_df['cleaned_title'].tolist()\n",
    "\n",
    "        # --- 3. Check if we have enough documents to proceed ---\n",
    "        if len(docs) < 50:\n",
    "            print(f\"Not enough video titles found ({len(docs)}) after cleaning to perform topic modeling.\")\n",
    "\n",
    "        else:\n",
    "            # --- 4. This is where the actual modeling happens ---\n",
    "            MAX_DOCS_FOR_MODELING = 50000\n",
    "            if len(docs) > MAX_DOCS_FOR_MODELING:\n",
    "                print(f\"Sampling {MAX_DOCS_FOR_MODELING} titles for topic modeling to ensure performance...\")\n",
    "                docs_df = docs_df.sample(n=MAX_DOCS_FOR_MODELING, random_state=42)\n",
    "                docs = docs_df['cleaned_title'].tolist()\n",
    "\n",
    "            print(f\"Training BERTopic model on {len(docs)} cleaned video titles. This may take several minutes...\")\n",
    "\n",
    "            topic_model = BERTopic(\n",
    "                min_topic_size=20,\n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "            topics, _ = topic_model.fit_transform(docs)\n",
    "\n",
    "            topics_to_assign = topics # Default to the original topics from the model\n",
    "\n",
    "            if run_outlier_reduction:\n",
    "                print(\"\\n--- Applying Outlier Reduction (Toggle is ON) ---\")\n",
    "                initial_outlier_count = (pd.Series(topics) == -1).sum()\n",
    "                print(f\"Initial outlier count: {initial_outlier_count} ({(initial_outlier_count/len(topics)*100):.1f}%)\")\n",
    "\n",
    "                if initial_outlier_count > 0 and initial_outlier_count < len(topics):\n",
    "                    print(\"Reducing outliers using the 'c-tf-idf' strategy...\")\n",
    "                    new_topics = topic_model.reduce_outliers(docs, topics, strategy=\"c-tf-idf\")\n",
    "\n",
    "                    final_outlier_count = (pd.Series(new_topics) == -1).sum()\n",
    "                    print(f\"Outlier count after reduction: {final_outlier_count} ({(final_outlier_count/len(topics)*100):.1f}%)\")\n",
    "                    print(f\"Successfully re-assigned {initial_outlier_count - final_outlier_count} outlier documents.\")\n",
    "\n",
    "                    # Use the new topic assignments for the rest of the analysis\n",
    "                    topics_to_assign = new_topics\n",
    "                else:\n",
    "                    print(\"No outliers to reduce, or all documents were outliers.\")\n",
    "            else:\n",
    "                print(\"\\n--- Skipping Outlier Reduction (Toggle is OFF) ---\")\n",
    "\n",
    "\n",
    "            print(\"\\nMapping topic results back to videos...\")\n",
    "            docs_df['topic_id'] = topics_to_assign # Use the final topic assignments\n",
    "            videos_with_topics_df = docs_df\n",
    "\n",
    "            print(\"\\n--- BERTopic Analysis Complete ---\")\n",
    "\n",
    "            print(\"\\nTop discovered topics from cleaned titles:\")\n",
    "            display(topic_model.get_topic_info())\n",
    "\n",
    "            print(\"\\nVisualizing top topics (barchart):\")\n",
    "            display(topic_model.visualize_barchart(top_n_topics=10))\n",
    "\n",
    "            print(\"\\nVisualizing inter-topic distance map:\")\n",
    "            display(topic_model.visualize_topics())\n",
    "\n",
    "    # --- 5. Handle potential errors for the entire block ---\n",
    "    except ImportError:\n",
    "        print(\"\\nCould not perform topic modeling. Please install required libraries:\")\n",
    "        print(\"pip install bertopic sentence-transformers scikit-learn nltk\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during topic modeling: {e}\")\n",
    "        logging.error(\"BERTopic analysis failed\", exc_info=True)\n",
    "else:\n",
    "    print(\"\\nFiltered graph G_filtered not found or is empty. Skipping topic modeling.\")"
   ],
   "id": "e4b6a8bbb9db61d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell: Diagnostic for BERTopic Results\n",
    "\n",
    "if 'topic_model' in locals() and topic_model is not None:\n",
    "    print(\"--- BERTopic Model Diagnostics ---\")\n",
    "\n",
    "    # Get the overview of topics\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "\n",
    "    # Check the size of the outlier topic (-1)\n",
    "    outlier_info = topic_info[topic_info['Topic'] == -1]\n",
    "    if not outlier_info.empty:\n",
    "        total_docs = topic_info['Count'].sum()\n",
    "        outlier_count = outlier_info['Count'].iloc[0]\n",
    "        outlier_percentage = (outlier_count / total_docs) * 100\n",
    "        print(f\"\\nOutlier Topic (-1) contains {outlier_count} videos ({outlier_percentage:.1f}% of the total).\")\n",
    "\n",
    "    print(\"\\nOverview of the top 10 most frequent topics (excluding outliers):\")\n",
    "    display(topic_info[topic_info['Topic'] != -1].head(10))\n",
    "\n",
    "else:\n",
    "    print(\"BERTopic model has not been trained yet.\")"
   ],
   "id": "73f4c30a2859734c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Interactive Collaboration Network Snippet (Searchable)\n",
    "\n",
    "Type part of a channel name (login or display name) and click \"Search & Visualize\" to see its immediate collaboration network.\n",
    "\n",
    "- If multiple channels match your search, select the specific one from the second dropdown.\n",
    "- Node size is proportional to the channel's total follower count (log scale).\n",
    "- Edge thickness is proportional to the number of collaboration instances found (log scale).\n",
    "- Edge labels show the most frequent game category for the collaboration and its percentage.\n",
    "\n",
    "*(Note: Embedding channel profile pictures directly within nodes is complex with this plotting method and is omitted.)*"
   ],
   "id": "cba11b95edda95a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell: Setup for Searchable Interactive Visualization\n",
    "from ipywidgets import Text, Button, Dropdown, Output, VBox, HBox, Layout\n",
    "from IPython.display import display\n",
    "import networkx as nx\n",
    "import math\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# This variable will hold the ID of the last channel successfully visualized\n",
    "# so we can re-plot when the degree of separation is changed.\n",
    "currently_visualized_channel_id = None\n",
    "\n",
    "# --- Ensure data is loaded from previous cells ---\n",
    "# This check ensures that if you only run the bottom part of the notebook, data is available.\n",
    "if 'channels_df' not in locals() or channels_df.empty:\n",
    "    print(\"Warning: Full 'channels_df' not loaded. Reloading for search widget...\")\n",
    "    try:\n",
    "        channels_df = pd.read_sql_query(\"SELECT id, login, display_name, follower_count FROM Channels\", db_conn)\n",
    "        channels_df['follower_count'] = pd.to_numeric(channels_df['follower_count'], errors='coerce').fillna(1)\n",
    "        channels_df['display_name'] = channels_df['display_name'].fillna(channels_df['login'])\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to reload channel data: {e}\")\n",
    "        channels_df = pd.DataFrame(columns=['id', 'login', 'display_name', 'follower_count'])\n",
    "\n",
    "if 'collab_df_full' not in locals() or collab_df_full.empty:\n",
    "    print(\"Warning: Full collaboration data not loaded. Reloading...\")\n",
    "    try:\n",
    "        collab_df_full = pd.read_sql_query(\"SELECT * FROM Collaborations\", db_conn)\n",
    "        collab_df_full['collaboration_count'] = pd.to_numeric(collab_df_full['collaboration_count'], errors='coerce').fillna(0).astype(int)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to reload collaboration data: {e}\")\n",
    "        collab_df_full = pd.DataFrame(columns=['channel_id_1', 'channel_id_2', 'collaboration_count'])\n",
    "\n",
    "if 'videos_with_topics_df' not in locals():\n",
    "     print(\"Warning: BERTopic data ('videos_with_topics_df') not found. Edge labels will be empty.\")\n",
    "     videos_with_topics_df = None # Ensure variable exists to prevent errors\n",
    "\n",
    "# --- Create Widgets ---\n",
    "search_input = Text(\n",
    "    description=\"Channel Name:\",\n",
    "    placeholder=\"Enter login or display name (min 3 chars)\",\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "search_button = Button(\n",
    "    description=\"Search\",\n",
    "    button_style='info',\n",
    "    tooltip='Search for the channel and display its network',\n",
    ")\n",
    "\n",
    "# --- NEW: Dropdown for degrees of separation ---\n",
    "degree_selector = Dropdown(\n",
    "    options=[\n",
    "        ('1 (Direct Neighbors)', 1),\n",
    "        ('2 (Neighbors of Neighbors)', 2),\n",
    "        ('3', 3),\n",
    "        ('4', 4)\n",
    "    ],\n",
    "    value=1, # Default to 1 degree\n",
    "    description='Degrees:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "results_dropdown = Dropdown(\n",
    "    description=\"Select Match:\",\n",
    "    options=[(\"---\", None)],\n",
    "    disabled=True,\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': 'max-content'}\n",
    ")\n",
    "\n",
    "message_output = Output(layout={'margin': '10px 0 0 0'})\n",
    "plot_output = Output()"
   ],
   "id": "b9c3ac68400abf02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell: Helper function for BERTopic Context (Updated with Title Fallback)\n",
    "\n",
    "def get_shared_topic_context(channel_id_A, channel_id_B, mentions_df, videos_with_topics_df, all_videos_df, topic_model):\n",
    "    \"\"\"\n",
    "    Finds the context for a collaboration edge.\n",
    "    - Tries to find the most prominent shared topic (ignoring outliers as top choice).\n",
    "    - If the only context is 'outlier', it falls back to finding the title of the latest collaboration video.\n",
    "    Returns a tuple: (label_string, label_type)\n",
    "    \"\"\"\n",
    "    if topic_model is None or videos_with_topics_df is None or mentions_df.empty:\n",
    "        return \"\", \"none\"\n",
    "\n",
    "    try:\n",
    "        # 1. Find all video IDs for this specific edge from the Mentions table\n",
    "        edge_mentions = mentions_df[\n",
    "            ((mentions_df['source_channel_id'] == channel_id_A) & (mentions_df['target_channel_id'] == channel_id_B)) |\n",
    "            ((mentions_df['source_channel_id'] == channel_id_B) & (mentions_df['target_channel_id'] == channel_id_A))\n",
    "        ]\n",
    "\n",
    "        if edge_mentions.empty: return \"\", \"none\"\n",
    "        video_ids_for_edge = edge_mentions['video_id'].unique().tolist()\n",
    "\n",
    "        # 2. Look up the pre-assigned topics for these specific videos\n",
    "        edge_video_topics = videos_with_topics_df[videos_with_topics_df['id'].isin(video_ids_for_edge)]\n",
    "\n",
    "        final_topic_id_to_use = None\n",
    "        if not edge_video_topics.empty:\n",
    "            topic_frequencies = edge_video_topics['topic_id'].value_counts()\n",
    "            if not topic_frequencies.empty:\n",
    "                top_topic_id = topic_frequencies.index[0]\n",
    "                if top_topic_id != -1:\n",
    "                    final_topic_id_to_use = top_topic_id\n",
    "                elif len(topic_frequencies) > 1:\n",
    "                    final_topic_id_to_use = topic_frequencies.index[1]\n",
    "\n",
    "        # 3. If a valid topic was found, return its name\n",
    "        if final_topic_id_to_use is not None:\n",
    "            topic_words = topic_model.get_topic(final_topic_id_to_use)\n",
    "            if topic_words:\n",
    "                label = \", \".join([word for word, prob in topic_words[:3]])\n",
    "                return label, \"topic\"\n",
    "\n",
    "        # 4. Fallback: If no suitable topic, find the latest video title\n",
    "        latest_mention = edge_mentions.sort_values('mention_timestamp', ascending=False).iloc[0]\n",
    "        latest_video_id = latest_mention['video_id']\n",
    "\n",
    "        # Look up the title in the main videos_df\n",
    "        video_row = all_videos_df[all_videos_df['id'] == latest_video_id]\n",
    "        if not video_row.empty:\n",
    "            title = video_row['title'].iloc[0]\n",
    "            # Truncate long titles for display\n",
    "            label = (title[:35] + '...') if len(title) > 38 else title\n",
    "            return f\"'{label}'\", \"title\" # Add quotes to signify it's a title\n",
    "\n",
    "        return \"\", \"none\" # Final fallback\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error getting shared topic for {channel_id_A}-{channel_id_B}: {e}\")\n",
    "        return \"\", \"none\"\n",
    "\n",
    "print(\"Helper function 'get_shared_topic_context' is now updated with title fallback.\")"
   ],
   "id": "b7811adf9cc66f5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell: Visualization Function for Interactive Widget\n",
    "\n",
    "def visualize_channel_neighborhood(selected_channel_id, degrees):\n",
    "    \"\"\"\n",
    "    Queries data and draws the network neighborhood for the selected channel\n",
    "    up to a specified degree of separation. Uses BERTopic for topic labels\n",
    "    and falls back to the latest video title for outlier collaborations.\n",
    "    \"\"\"\n",
    "    global currently_visualized_channel_id\n",
    "\n",
    "    with plot_output:\n",
    "        plot_output.clear_output(wait=True);\n",
    "        with message_output: message_output.clear_output()\n",
    "        if not selected_channel_id: return\n",
    "\n",
    "        selected_channel_row = channels_df[channels_df['id'] == selected_channel_id]\n",
    "        if selected_channel_row.empty:\n",
    "            with message_output: print(f\"Selected channel ID {selected_channel_id} not found in loaded channel data.\")\n",
    "            return\n",
    "        selected_channel_name_for_title = selected_channel_row['display_name'].iloc[0]\n",
    "\n",
    "        print(f\"Generating {degrees}-degree network for: {selected_channel_name_for_title}...\")\n",
    "\n",
    "        # Build a full graph object from the complete collaboration data to traverse it\n",
    "        global G_full_for_viz\n",
    "        if 'G_full_for_viz' not in globals() or not isinstance(G_full_for_viz, nx.Graph):\n",
    "            print(\" -> Creating full collaboration graph for traversal (one-time)...\")\n",
    "            G_full_for_viz = nx.from_pandas_edgelist(\n",
    "                collab_df_full, 'channel_id_1', 'channel_id_2',\n",
    "                edge_attr=['collaboration_count']\n",
    "            )\n",
    "\n",
    "        if not G_full_for_viz.has_node(selected_channel_id):\n",
    "            with message_output: print(f\"No collaboration data found for channel: {selected_channel_name_for_title}\")\n",
    "            return\n",
    "\n",
    "        # Create the subgraph using the specified radius (degrees)\n",
    "        subgraph = nx.ego_graph(G_full_for_viz, selected_channel_id, radius=degrees)\n",
    "\n",
    "        # Performance Warning for large subgraphs\n",
    "        if subgraph.number_of_nodes() > config.NETWORK_VIZ_MAX_SUBGRAPH_NODES:\n",
    "            with message_output:\n",
    "                display(HTML(f\"<p style='color: orange;'><b>Warning:</b> The {degrees}-degree neighborhood has {subgraph.number_of_nodes()} nodes, which is more than the display limit of {config.NETWORK_VIZ_MAX_SUBGRAPH_NODES}. The graph may be slow and hard to read.</p>\"))\n",
    "\n",
    "        # --- Visualization logic ---\n",
    "        plt.figure(figsize=(16, 12));\n",
    "        ax = plt.gca()\n",
    "        pos_viz = nx.spring_layout(subgraph, k=0.5, iterations=50, seed=42)\n",
    "\n",
    "        # Get data for nodes in the subgraph for both drawing paths\n",
    "        node_data_viz = channels_df[channels_df['id'].isin(list(subgraph.nodes()))].set_index('id')\n",
    "        missing_nodes = [n for n in list(subgraph.nodes()) if n not in node_data_viz.index]\n",
    "        if missing_nodes:\n",
    "            print(f\"Warning: Missing channel data for nodes: {missing_nodes}\")\n",
    "            dummy_data = pd.DataFrame({'follower_count': 1, 'display_name': 'UNKNOWN'}, index=missing_nodes)\n",
    "            node_data_viz = pd.concat([node_data_viz, dummy_data])\n",
    "\n",
    "        # Create the labels dictionary for both drawing paths\n",
    "        node_labels_viz = {node: node_data_viz.loc[node, 'display_name'] for node in subgraph.nodes()}\n",
    "\n",
    "        if USE_IMAGE_NODES:\n",
    "            print(\"Rendering graph with profile pictures (this may take a moment)...\")\n",
    "\n",
    "            # Create a dummy partition for this neighborhood to color borders: center node vs neighbors\n",
    "            partition_viz = {node: 1 for node in subgraph.nodes()}\n",
    "            partition_viz[selected_channel_id] = 0\n",
    "\n",
    "            # Pass the `node_labels_viz` dictionary to the helper function\n",
    "            draw_network_with_images(subgraph, pos_viz, node_data_viz, partition_viz, node_labels_viz, ax)\n",
    "\n",
    "        else: # Fallback to simple nodes\n",
    "            print(\"Rendering graph with simple nodes...\")\n",
    "            node_sizes_viz = [math.log10(max(1, node_data_viz.loc[node].get('follower_count', 1)) + 1) * 200 + 150 for node in subgraph.nodes()]\n",
    "            # Color center node differently from its neighbors\n",
    "            node_colors_viz = ['tomato' if node == selected_channel_id else 'skyblue' for node in subgraph.nodes()]\n",
    "\n",
    "            nx.draw_networkx_nodes(subgraph, pos_viz, node_size=node_sizes_viz, node_color=node_colors_viz, alpha=0.9, ax=ax)\n",
    "            # Use the labels dictionary here as well\n",
    "            nx.draw_networkx_labels(subgraph, pos_viz, labels=node_labels_viz, font_size=9, ax=ax)\n",
    "\n",
    "        # Draw edges and edge labels (common to both drawing methods)\n",
    "        edge_widths_viz = [math.log10(max(0, d.get('weight', 0)) + 1) * 1.5 + 0.5 for u, v, d in subgraph.edges(data=True)]\n",
    "        nx.draw_networkx_edges(subgraph, pos_viz, width=edge_widths_viz, alpha=0.4, edge_color='gray', ax=ax)\n",
    "\n",
    "        topic_edge_labels = {}\n",
    "        title_edge_labels = {}\n",
    "\n",
    "        if VERBOSE_MODE: print(\"Deriving edge context from BERTopic model and video titles...\")\n",
    "        for u, v in subgraph.edges():\n",
    "            label, label_type = get_shared_topic_context(\n",
    "                u, v,\n",
    "                mentions_df=mentions_df,\n",
    "                videos_with_topics_df=videos_with_topics_df,\n",
    "                all_videos_df=videos_df, # Pass full df for title lookup\n",
    "                topic_model=topic_model\n",
    "            )\n",
    "\n",
    "            if label_type == \"topic\":\n",
    "                topic_edge_labels[(u, v)] = label\n",
    "            elif label_type == \"title\":\n",
    "                title_edge_labels[(u, v)] = label\n",
    "\n",
    "        nx.draw_networkx_edge_labels(\n",
    "            subgraph, pos_viz,\n",
    "            edge_labels=topic_edge_labels,\n",
    "            font_size=7, font_color='darkgreen',\n",
    "            bbox=dict(facecolor='white', alpha=0.4, edgecolor='none', boxstyle='round,pad=0.2')\n",
    "        )\n",
    "        nx.draw_networkx_edge_labels(\n",
    "            subgraph, pos_viz,\n",
    "            edge_labels=title_edge_labels,\n",
    "            font_size=7, font_color='purple',\n",
    "            bbox=dict(facecolor='white', alpha=0.4, edgecolor='none', boxstyle='round,pad=0.2')\n",
    "        )\n",
    "\n",
    "        plt.title(f\"{degrees}-Degree Collaboration Network for: {selected_channel_name_for_title}\", fontsize=16);\n",
    "        plt.axis('off');\n",
    "        plt.tight_layout();\n",
    "        plt.show()\n",
    "\n",
    "        # When visualization is successful, store the channel ID\n",
    "        currently_visualized_channel_id = selected_channel_id\n",
    "        print(\"Done.\")"
   ],
   "id": "468062d3efeafd49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell: Search and Selection Logic\n",
    "\n",
    "def handle_search_click(b):\n",
    "    \"\"\"Function called when the search button is clicked.\"\"\"\n",
    "    global currently_visualized_channel_id\n",
    "    search_term = search_input.value.strip()\n",
    "    # Reset state\n",
    "    results_dropdown.options = [(\"---\", None)];\n",
    "    results_dropdown.value = None;\n",
    "    results_dropdown.disabled = True\n",
    "    with message_output: message_output.clear_output()\n",
    "    with plot_output: plot_output.clear_output()\n",
    "    currently_visualized_channel_id = None\n",
    "\n",
    "    if len(search_term) < 3:\n",
    "        with message_output: print(\"Please enter at least 3 characters to search.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Searching for channels matching: '{search_term}'...\")\n",
    "    search_pattern = f\"%{search_term.lower()}%\"\n",
    "    cursor = db_conn.cursor()\n",
    "    # SQL to sort by follower count, showing more relevant results first\n",
    "    sql = \"SELECT id, login, display_name FROM Channels WHERE LOWER(login) LIKE ? OR LOWER(display_name) LIKE ? ORDER BY follower_count DESC LIMIT 50\"\n",
    "    try:\n",
    "        cursor.execute(sql, (search_pattern, search_pattern));\n",
    "        matches = cursor.fetchall()\n",
    "    except sqlite3.Error as e:\n",
    "        with message_output: print(f\"Database search error: {e}\"); return\n",
    "\n",
    "    if not matches:\n",
    "        with message_output: print(f\"No channels found matching '{search_term}'.\")\n",
    "    elif len(matches) == 1:\n",
    "        match = matches[0];\n",
    "        channel_id = match['id']\n",
    "        with message_output: print(f\"Found 1 match: {match['display_name']} ({match['login']}). Visualizing...\")\n",
    "        visualize_channel_neighborhood(channel_id, degree_selector.value)\n",
    "    else:\n",
    "        match_options = [(\"--- Select a Match ---\", None)] + [(f\"{row['display_name']} ({row['login']})\", row['id']) for row in matches]\n",
    "        results_dropdown.options = match_options;\n",
    "        results_dropdown.disabled = False\n",
    "        with message_output: print(f\"Found {len(matches)} matches. Please select one from the dropdown below.\")\n",
    "\n",
    "\n",
    "def handle_match_selection(change):\n",
    "    \"\"\"Function called when a channel is selected from the results dropdown.\"\"\"\n",
    "    selected_id = change.get('new')\n",
    "    if selected_id:\n",
    "        visualize_channel_neighborhood(selected_id, degree_selector.value)\n",
    "    else:\n",
    "        with plot_output: plot_output.clear_output()\n",
    "\n",
    "# --- Handler for changing the degree of separation ---\n",
    "def handle_degree_change(change):\n",
    "    \"\"\"Function called when the degree selector dropdown changes.\"\"\"\n",
    "    new_degree = change.get('new')\n",
    "    # If a channel is already selected/visualized, re-run the visualization with the new degree\n",
    "    if currently_visualized_channel_id:\n",
    "        visualize_channel_neighborhood(currently_visualized_channel_id, new_degree)\n",
    "\n",
    "# Link events to handlers\n",
    "search_button.on_click(handle_search_click)\n",
    "results_dropdown.observe(handle_match_selection, names='value')\n",
    "degree_selector.observe(handle_degree_change, names='value')"
   ],
   "id": "3c41caac0554ba8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell: Display Interactive Widgets\n",
    "print(\"\\n--- Displaying Interactive Network Visualization Widget ---\")\n",
    "\n",
    "# Arrange search input and button horizontally\n",
    "search_box = HBox([search_input, search_button])\n",
    "# Arrange degree and results dropdowns horizontally\n",
    "selector_box = HBox([degree_selector, results_dropdown])\n",
    "\n",
    "# Arrange all controls vertically for a clean layout\n",
    "controls = VBox([\n",
    "    search_box,\n",
    "    selector_box,\n",
    "    message_output,\n",
    "    plot_output\n",
    "    ], layout=Layout(width='100%'))\n",
    "\n",
    "display(controls)"
   ],
   "id": "4d7e1aebd0860436",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 24: Final Closing Remarks & Cleanup\n",
    "print(f\"\\n--- Notebook Processing Finished at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "print(\n",
    "    \"You can re-run cells like 'Run Data Collection Cycle', 'Mention Processing Loop', or 'Run Channel Refresh Cycle' to gather more data.\")\n",
    "print(\"Consider closing the database connection manually if you are completely finished.\")\n",
    "# Example manual close (uncomment to run):\n",
    "# if 'db_conn' in locals() and db_conn is not None:\n",
    "#     try:\n",
    "#         db_conn.close()\n",
    "#         print(\"Database connection closed.\")\n",
    "#         db_conn = None # Clear variable\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error closing database connection: {e}\")"
   ],
   "id": "1ad0552c3886910e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7efe208a95e78d20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cd53de7a39980f8e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
