{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Twitch Insight: Data Collector & Collaboration Network Analyzer (v3 - Enhanced Prints)\n",
    "\n",
    "This notebook automates the process of collecting data from the Twitch API, store it in a local SQLite database, and perform analysis, with a special focus on understanding collaboration networks between streamers. It identifies potential collaborations by detecting `@mentions` in video titles and descriptions, and also explores community structures within these networks. The primary interface for data collection, processing, and exploration is a Jupyter Notebook.\n",
    "\n",
    "**Key Features:**\n",
    "- Fetches top streams/categories periodically.\n",
    "- Fetches channel details and video archives.\n",
    "- Processes video titles/descriptions for `@mentions`.\n",
    "- Looks up mentioned users via API if not in the database.\n",
    "- Stores collaboration data (frequency, duration, recency).\n",
    "- Processes mentions atomically per-video using DB transactions.\n",
    "- Includes a refresh cycle for updating random channels.\n",
    "- Provides data exploration for channels, videos, and the collaboration network.\n",
    "\n",
    "**Modules Used:**\n",
    "- `config.py`: Configuration (API keys, constants). Requires `.env` file.\n",
    "- `database.py`: SQLite database interactions (schema, CRUD, upserts).\n",
    "- `twitch_api.py`: Twitch Helix API communication (auth, rate limits).\n",
    "- `network_utils.py`: Mention extraction and validation."
   ],
   "id": "5e517ca68d15737a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import logging\n",
    "import time\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import re  # For parsing duration\n",
    "import math  # For log scale checks\n",
    "import random  # For refresh cycle\n",
    "import networkx as nx  # For graph analysis\n",
    "from ipywidgets import Text, Button, Dropdown, Output, VBox, Layout  # For interactive viz\n",
    "from IPython.display import display, HTML  # For displaying widgets and potentially HTML\n",
    "\n",
    "# --- Local Notebook Configuration ---\n",
    "# Set to True for detailed progress and time estimations, False for minimal output.\n",
    "VERBOSE_MODE = True\n",
    "\n",
    "# Import custom modules\n",
    "try:\n",
    "    import config\n",
    "    import database\n",
    "    import twitch_api\n",
    "    import network_utils\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing custom modules: {e}\")\n",
    "    print(\"Please ensure config.py, database.py, twitch_api.py, and network_utils.py are in the same directory.\")\n",
    "    raise SystemExit(\"Stopping notebook due to missing modules.\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(module)s - %(message)s')\n",
    "\n",
    "# --- Print Configuration ---\n",
    "config.print_config()  # Call the function from config.py\n",
    "print(f\"\\n[Notebook Settings]\\n  VERBOSE_MODE: {VERBOSE_MODE}\")\n",
    "\n",
    "# --- Initialize ---\n",
    "db_conn = None\n",
    "api_client = None\n",
    "try:\n",
    "    print(\"\\nInitializing database connection...\")\n",
    "    # Use settings from config.py\n",
    "    db_conn = database.get_db_connection(config.DATABASE_NAME)\n",
    "    database.initialize_database(db_conn)\n",
    "    print(f\"Database '{config.DATABASE_NAME}' initialized.\")\n",
    "\n",
    "    print(\"Initializing Twitch API client...\")\n",
    "    api_client = twitch_api.TwitchAPIClient(\n",
    "        client_id=config.TWITCH_CLIENT_ID,\n",
    "        client_secret=config.TWITCH_CLIENT_SECRET,\n",
    "        auth_url=config.TWITCH_AUTH_URL,\n",
    "        base_url=config.TWITCH_API_BASE_URL\n",
    "    )\n",
    "    # Trigger authentication early to check credentials\n",
    "    if not api_client._authenticate():  # Use internal method carefully or add a public check method\n",
    "        raise ConnectionError(\"Failed to authenticate with Twitch API.\")\n",
    "    print(\"API Client initialized and authenticated.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.critical(f\"Initialization failed: {e}\", exc_info=True)\n",
    "    print(f\"Initialization failed: {e}\")\n",
    "    if db_conn:\n",
    "        db_conn.close()\n",
    "    raise SystemExit(\"Stopping notebook due to initialization failure.\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Setup Complete. Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"-\" * 30)\n",
    "\n"
   ],
   "id": "e5ca9c4d759b9375",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 2: Helper Functions\n",
    "\n",
    "def parse_duration_for_collab(duration_str):\n",
    "    \"\"\"\n",
    "    Parses a Twitch duration string (e.g., \"1h30m20s\") into total seconds.\n",
    "    \"\"\"\n",
    "    if not duration_str or not isinstance(duration_str, str):\n",
    "        return 0 # Return 0 for invalid input\n",
    "\n",
    "    total_seconds = 0\n",
    "    hours = re.search(r'(\\d+)h', duration_str)\n",
    "    minutes = re.search(r'(\\d+)m', duration_str)\n",
    "    seconds = re.search(r'(\\d+)s', duration_str)\n",
    "\n",
    "    if hours:\n",
    "        total_seconds += int(hours.group(1)) * 3600\n",
    "    if minutes:\n",
    "        total_seconds += int(minutes.group(1)) * 60\n",
    "    if seconds:\n",
    "        total_seconds += int(seconds.group(1))\n",
    "\n",
    "    return total_seconds\n",
    "\n",
    "def format_seconds_to_hm(seconds):\n",
    "    \"\"\"\n",
    "    Formats a duration in seconds into a human-readable \"Xh Ym\" string.\n",
    "    \"\"\"\n",
    "    if pd.isna(seconds) or seconds < 0:\n",
    "        return \"N/A\"\n",
    "\n",
    "    seconds = int(seconds)\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "\n",
    "    return f\"{hours}h {minutes:02}m\"\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ],
   "id": "d87a6db4e65288e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Collection Seeding\n",
    "\n",
    "This section is responsible for seeding the database with channels and their videos by fetching top streams and channels. It can be run to initialize an empty database, or to discover new channels that are not a part of the social network of any known channel.\n"
   ],
   "id": "c18bda92add56995"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 2: Data Collection Cycle Function & Execution (Top Streams Focus)\n",
    "def run_collection_cycle(current_api_client, current_db_conn):\n",
    "    \"\"\"\n",
    "    Runs one cycle focused on fetching top streams, channels, and their videos.\n",
    "    Mention processing is handled separately.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Starting Top Stream Data Collection Cycle at {datetime.now().strftime('%H:%M:%S')} ===\")\n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    # Phase 1: Fetch Top Categories\n",
    "    phase_start_time = time.time()\n",
    "    print(\"\\n--- Phase 1: Fetching and Processing Top Categories ---\")\n",
    "    top_categories = current_api_client.get_top_games(config.NUM_TOP_CATEGORIES)\n",
    "    if not top_categories: print(\"Could not fetch top categories. Cycle aborted.\"); return False\n",
    "    database.save_categories(current_db_conn, top_categories)\n",
    "    print(f\"Phase 1: Processed {len(top_categories)} top categories in {time.time() - phase_start_time:.2f}s.\")\n",
    "\n",
    "    # Phase 2: Fetch Top Streams & Identify Channels\n",
    "    phase_start_time = time.time()\n",
    "    print(\"\\n--- Phase 2: Fetching Top Streams & Identifying Channels from Categories ---\")\n",
    "    channels_to_process = set()\n",
    "    categories_to_scan = database.get_categories_to_scan(current_db_conn, config.NUM_TOP_CATEGORIES)\n",
    "    total_categories_to_scan = len(categories_to_scan)\n",
    "    print(f\"Found {total_categories_to_scan} categories prioritized for scanning.\")\n",
    "    category_processing_times = []\n",
    "\n",
    "    for i, category_row in enumerate(categories_to_scan):\n",
    "        cat_start_time = time.time()\n",
    "        category_id = category_row['id']\n",
    "        category_name = category_row['name']\n",
    "        print(f\" ({i + 1}/{total_categories_to_scan}) Processing category: {category_name}...\")\n",
    "        streams = current_api_client.get_streams_for_game(category_id, config.NUM_STREAMS_PER_CATEGORY)\n",
    "        if streams:\n",
    "            stream_channel_ids = set()\n",
    "            for stream in streams:\n",
    "                if 'user_id' in stream and 'user_login' in stream and 'user_name' in stream:\n",
    "                    if database.save_channel_basic(current_db_conn, {\n",
    "                        'id': stream['user_id'], 'login': stream['user_login'], 'display_name': stream['user_name']\n",
    "                    }): stream_channel_ids.add(stream['user_id'])\n",
    "            channels_to_process.update(stream_channel_ids)\n",
    "        database.update_category_scan_time(current_db_conn, category_row['id'])\n",
    "\n",
    "        cat_duration = time.time() - cat_start_time\n",
    "        category_processing_times.append(cat_duration)\n",
    "        if VERBOSE_MODE and category_processing_times and total_categories_to_scan > 0:\n",
    "            avg_time_per_cat = sum(category_processing_times) / len(category_processing_times)\n",
    "            est_remaining_cat_secs = (total_categories_to_scan - (i + 1)) * avg_time_per_cat\n",
    "            if est_remaining_cat_secs > 0 and i < total_categories_to_scan - 1:\n",
    "                est_cat_mins, est_cat_s = divmod(int(est_remaining_cat_secs), 60)\n",
    "                print(f\" -> Processed in {cat_duration:.2f}s. Est. remaining for categories: {est_cat_mins}m {est_cat_s}s\")\n",
    "            else:\n",
    "                print(f\" -> Processed in {cat_duration:.2f}s.\")\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    print(\n",
    "        f\"\\nPhase 2: Identified {len(channels_to_process)} unique channels. Took {time.time() - phase_start_time:.2f}s.\")\n",
    "\n",
    "    # Phase 3: Fetch/Update Channel Details (MODIFIED for one-by-one fetching)\n",
    "    phase_start_time = time.time()\n",
    "    print(\"\\n--- Phase 3: Fetching/Updating Channel Details (including Followers) ---\")\n",
    "    processed_channels_details = 0\n",
    "    channels_needing_details_update = [\n",
    "        chan_id for chan_id in list(channels_to_process)\n",
    "        if database.check_channel_needs_update(current_db_conn, chan_id, config.REFETCH_CHANNEL_DETAILS_DAYS)\n",
    "    ]\n",
    "    total_to_update = len(channels_needing_details_update)\n",
    "    print(f\"{total_to_update} channels require detail fetching/updating.\")\n",
    "    detail_fetch_times = []\n",
    "\n",
    "    if total_to_update > 0:\n",
    "        for i, channel_id in enumerate(channels_needing_details_update):\n",
    "            item_start_time = time.time()\n",
    "            print(f\" ({i + 1}/{total_to_update}) Fetching details for channel ID: {channel_id}...\")\n",
    "\n",
    "            # 1. Get user details (like login, description)\n",
    "            user_details_list = current_api_client.get_user_details(user_ids=[channel_id])\n",
    "\n",
    "            if user_details_list:\n",
    "                user_data = user_details_list[0] # Get the first (and only) result\n",
    "\n",
    "                # 2. Get follower count (separate API call)\n",
    "                follower_count = current_api_client.get_channel_follower_count(broadcaster_id=channel_id)\n",
    "                if follower_count is not None:\n",
    "                    user_data['follower_count'] = follower_count\n",
    "\n",
    "                # 3. Save combined data\n",
    "                try:\n",
    "                    database.save_channel_details(current_db_conn, user_data)\n",
    "                    processed_channels_details += 1\n",
    "                except Exception as e:\n",
    "                    print(f\" -> DB Error saving details for {user_data.get('login', channel_id)}: {e}\")\n",
    "\n",
    "            elif user_details_list is None:\n",
    "                print(f\" -> API call failed for details of channel {channel_id}. Skipping.\")\n",
    "\n",
    "            # Time estimation logic\n",
    "            item_duration = time.time() - item_start_time\n",
    "            detail_fetch_times.append(item_duration)\n",
    "            if VERBOSE_MODE and detail_fetch_times:\n",
    "                avg_time = sum(detail_fetch_times) / len(detail_fetch_times)\n",
    "                est_rem_secs = (total_to_update - (i + 1)) * avg_time\n",
    "                if est_rem_secs > 0:\n",
    "                    est_mins, est_s = divmod(int(est_rem_secs), 60)\n",
    "                    print(f\" -> Processed in {item_duration:.2f}s. Est. remaining: {est_mins}m {est_s}s\")\n",
    "\n",
    "            time.sleep(0.3) # API courtesy between channels\n",
    "\n",
    "    print(f\"Phase 3: Finished. Attempted save for {processed_channels_details} channels. Took {time.time() - phase_start_time:.2f}s.\")\n",
    "\n",
    "    # Phase 4: Fetch/Update Channel Videos\n",
    "    phase_start_time = time.time()\n",
    "    print(\"\\n--- Phase 4: Checking for and Fetching New Videos ---\")\n",
    "    channels_for_video_fetch = list(channels_to_process)\n",
    "    total_channels_for_video = len(channels_for_video_fetch)\n",
    "    print(f\"Checking for new videos for {total_channels_for_video} channels from this cycle...\")\n",
    "    processed_channels_videos = 0; new_videos_found_total = 0\n",
    "    video_fetch_times = []\n",
    "\n",
    "    if total_channels_for_video > 0:\n",
    "        for i, channel_id in enumerate(channels_for_video_fetch):\n",
    "            ch_video_start_time = time.time()\n",
    "            channel_info_for_log_cursor = current_db_conn.execute(\"SELECT login FROM Channels WHERE id = ?\", (channel_id,))\n",
    "            channel_info_for_log = channel_info_for_log_cursor.fetchone()\n",
    "            channel_log_name = channel_info_for_log['login'] if channel_info_for_log else channel_id\n",
    "\n",
    "            print(f\" ({i + 1}/{total_channels_for_video}) Checking videos for channel: {channel_log_name}...\")\n",
    "            latest_stored_date = database.get_latest_video_date_for_channel(current_db_conn, channel_id)\n",
    "            # This call fetches videos for channels just found in top streams\n",
    "            new_videos = current_api_client.get_channel_videos(\n",
    "                channel_id,\n",
    "                video_type='archive',\n",
    "                # Limit how many new videos to grab for a newly seen channel.\n",
    "                # Helps prevent a single new channel from dominating the cycle time.\n",
    "                limit=100,\n",
    "                after_date=latest_stored_date\n",
    "            )\n",
    "\n",
    "            if new_videos:\n",
    "                if VERBOSE_MODE: print(f\" -> Found {len(new_videos)} new archive videos.\")\n",
    "                database.save_videos(current_db_conn, new_videos); new_videos_found_total += len(new_videos)\n",
    "            elif new_videos is None:\n",
    "                print(f\" -> API call failed fetching videos for {channel_log_name}.\")\n",
    "\n",
    "            if new_videos is not None: database.update_channel_video_fetch_time(current_db_conn, channel_id)\n",
    "            processed_channels_videos += 1\n",
    "\n",
    "            ch_video_duration = time.time() - ch_video_start_time\n",
    "            video_fetch_times.append(ch_video_duration)\n",
    "            if VERBOSE_MODE and video_fetch_times:\n",
    "                avg_time_per_ch_video = sum(video_fetch_times) / len(video_fetch_times)\n",
    "                est_remaining_vid_secs = (total_channels_for_video - (i + 1)) * avg_time_per_ch_video\n",
    "                if est_remaining_vid_secs > 0 and i < total_channels_for_video - 1:\n",
    "                    est_vid_mins, est_vid_s = divmod(int(est_remaining_vid_secs), 60)\n",
    "                    print(f\" -> Processed in {ch_video_duration:.2f}s. Est. remaining for video checks: {est_vid_mins}m {est_vid_s}s\")\n",
    "                else:\n",
    "                    print(f\" -> Processed in {ch_video_duration:.2f}s.\")\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    print(f\"\\nPhase 4: Finished. Checked {processed_channels_videos} channels, found {new_videos_found_total} new videos. Took {time.time() - phase_start_time:.2f}s.\")\n",
    "    print(f\"=== Top Stream Data Collection Cycle Finished in {time.time() - overall_start_time:.2f}s ===\")\n",
    "    return True\n",
    "\n",
    "# Run the collection cycle (you can comment this out after initial runs or run it selectively)\n",
    "print(f\"\\n--- Executing Data Collection Cycle (Top Streams) at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "run_collection_cycle(api_client, db_conn)\n",
    "print(\"-\" * 30)\n",
    "print(\"Data Collection Cycle (Top Streams) Done for this run.\")\n",
    "print(\"-\" * 30)"
   ],
   "id": "15d635ce2a507458",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (675/1314) Checking videos for channel: tingting57...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Mention Processing\n",
    "\n",
    "\n"
   ],
   "id": "5d8740c021271ab1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 3: Mention Processing Function (Atomic Per-Video)\n",
    "\n",
    "# Helper function to parse duration\n",
    "def parse_duration_for_collab(duration_str):\n",
    "    \"\"\"\n",
    "    Parses a Twitch duration string (e.g., \"1h30m20s\") into total seconds.\n",
    "    \"\"\"\n",
    "    if not duration_str or not isinstance(duration_str, str):\n",
    "        return 0 # Return 0 for invalid input\n",
    "\n",
    "    total_seconds = 0\n",
    "    hours = re.search(r'(\\d+)h', duration_str)\n",
    "    minutes = re.search(r'(\\d+)m', duration_str)\n",
    "    seconds = re.search(r'(\\d+)s', duration_str)\n",
    "\n",
    "    if hours:\n",
    "        total_seconds += int(hours.group(1)) * 3600\n",
    "    if minutes:\n",
    "        total_seconds += int(minutes.group(1)) * 60\n",
    "    if seconds:\n",
    "        total_seconds += int(seconds.group(1))\n",
    "\n",
    "    return total_seconds\n",
    "\n",
    "\n",
    "def process_video_mentions_batch(video_batch, current_api_client, current_db_conn):\n",
    "    \"\"\"\n",
    "    Processes a batch of videos to find mentions, update collaborations.\n",
    "    Discovers new channels via API for unknown mentions.\n",
    "    Attempts atomic processing per video using DB transactions.\n",
    "    \"\"\"\n",
    "    func_start_time = time.time()\n",
    "    processed_count_in_batch = 0;\n",
    "    newly_found_channels_in_batch = 0;\n",
    "    updated_edges_in_batch = 0;\n",
    "    all_unknown_logins_in_batch = set();\n",
    "    temp_video_data = {}\n",
    "\n",
    "    if VERBOSE_MODE: print(f\"  Batch Start: {len(video_batch)} videos to process.\")\n",
    "\n",
    "    # Pass 1: Extract mentions and identify all unique unknown logins for the batch\n",
    "    for video_row in video_batch:\n",
    "         video_id = video_row['id']; title = video_row['title'] or ''; desc = video_row['description'] or ''\n",
    "         text_to_scan = f\"{title} {desc}\"\n",
    "         mentioned_logins = network_utils.extract_mentions(text_to_scan)\n",
    "         temp_video_data[video_id] = {\n",
    "             'owner_id': video_row['channel_id'], 'mentions': mentioned_logins,\n",
    "             'published_at': video_row['published_at'], 'duration': video_row['duration']\n",
    "         }\n",
    "         if mentioned_logins:\n",
    "             try:\n",
    "                 _, not_found_now = network_utils.find_mentioned_channel_ids(mentioned_logins, current_db_conn);\n",
    "                 all_unknown_logins_in_batch.update(not_found_now)\n",
    "             except Exception as e:\n",
    "                 logging.error(f\"Error checking mentions in DB during Pass 1 for video {video_id}: {e}\")\n",
    "\n",
    "    # Pass 2: Fetch details for unknown mentioned logins via batched API calls\n",
    "    newly_discovered_ids_this_pass = {}\n",
    "    if all_unknown_logins_in_batch:\n",
    "        unknown_logins_list = list(all_unknown_logins_in_batch)\n",
    "        if VERBOSE_MODE: print(f\"  Pass 2: Checking {len(unknown_logins_list)} unique unknown logins via API...\")\n",
    "        num_api_batches = (len(unknown_logins_list) + 99) // 100\n",
    "        for i in range(num_api_batches):\n",
    "            batch_logins = unknown_logins_list[i * 100:(i + 1) * 100]\n",
    "            if VERBOSE_MODE: print(f\"   -> API Batch {i + 1}/{num_api_batches} for {len(batch_logins)} logins...\")\n",
    "            time.sleep(0.2)\n",
    "            user_details_list = current_api_client.get_user_details(user_logins=batch_logins);\n",
    "            api_call_succeeded = user_details_list is not None\n",
    "            if api_call_succeeded and user_details_list:\n",
    "                for user_data in user_details_list:\n",
    "                     try:\n",
    "                         database.save_channel_details(current_db_conn, user_data);\n",
    "                         login_lower = user_data['login'].lower();\n",
    "                         user_id = user_data['id']\n",
    "                         newly_discovered_ids_this_pass[login_lower] = user_id;\n",
    "                         newly_found_channels_in_batch += 1\n",
    "                     except Exception as e:\n",
    "                         print(f\"Error saving newly discovered channel {user_data.get('login')}: {e}\")\n",
    "            time.sleep(0.1)\n",
    "        if VERBOSE_MODE: print(f\"  Pass 2 Complete. Discovered and saved {newly_found_channels_in_batch} new channels.\")\n",
    "\n",
    "    # Pass 3: Process each video within its own database transaction\n",
    "    for idx, (video_id, video_data) in enumerate(temp_video_data.items()):\n",
    "        channel_id_A = video_data['owner_id'];\n",
    "        mentioned_logins = video_data['mentions']\n",
    "        published_at = video_data['published_at'];\n",
    "        duration_str = video_data['duration']\n",
    "        is_processed_successfully_this_video = False\n",
    "\n",
    "        try:\n",
    "            published_at_dt = published_at\n",
    "            if not isinstance(published_at_dt, datetime):\n",
    "                published_at_dt = pd.to_datetime(published_at, errors='coerce', utc=True)\n",
    "\n",
    "            if pd.isna(published_at_dt):\n",
    "                logging.warning(f\"Invalid timestamp for video {video_id}. Skipping edges. Marking processed.\")\n",
    "                current_db_conn.execute('BEGIN')\n",
    "                database.mark_video_mentions_processed(current_db_conn, video_id)\n",
    "                current_db_conn.commit()\n",
    "                processed_count_in_batch += 1\n",
    "                is_processed_successfully_this_video = True\n",
    "\n",
    "            elif not mentioned_logins:\n",
    "                current_db_conn.execute('BEGIN')\n",
    "                database.mark_video_mentions_processed(current_db_conn, video_id)\n",
    "                current_db_conn.commit()\n",
    "                processed_count_in_batch += 1\n",
    "                is_processed_successfully_this_video = True\n",
    "            else:\n",
    "                current_known_ids, _ = network_utils.find_mentioned_channel_ids(mentioned_logins, current_db_conn)\n",
    "                for login, user_id_new in newly_discovered_ids_this_pass.items():\n",
    "                     if login in mentioned_logins:\n",
    "                         current_known_ids[login] = user_id_new\n",
    "\n",
    "                duration_sec = parse_duration_for_collab(duration_str)\n",
    "                edges_for_this_video = 0\n",
    "                mentions_to_add_list = []\n",
    "\n",
    "                current_db_conn.execute('BEGIN IMMEDIATE')\n",
    "                for login, channel_id_B in current_known_ids.items():\n",
    "                    if channel_id_A != channel_id_B:\n",
    "                        database.upsert_collaboration_edge(current_db_conn, channel_id_A, channel_id_B, published_at_dt, duration_sec)\n",
    "                        mentions_to_add_list.append((channel_id_A, channel_id_B, video_id, published_at_dt))\n",
    "                        edges_for_this_video += 1\n",
    "\n",
    "                if mentions_to_add_list:\n",
    "                    database.add_mentions(current_db_conn, mentions_to_add_list)\n",
    "\n",
    "                database.mark_video_mentions_processed(current_db_conn, video_id)\n",
    "                current_db_conn.commit()\n",
    "\n",
    "                processed_count_in_batch += 1\n",
    "                updated_edges_in_batch += edges_for_this_video\n",
    "                is_processed_successfully_this_video = True\n",
    "\n",
    "        except Exception as e:\n",
    "            if not is_processed_successfully_this_video:\n",
    "                 try:\n",
    "                     current_db_conn.rollback()\n",
    "                 except sqlite3.Error as rb_err:\n",
    "                     logging.error(f\"Error during rollback for video {video_id}: {rb_err}\")\n",
    "                 logging.error(f\"Error processing video {video_id} within transaction\", exc_info=True)\n",
    "\n",
    "    return processed_count_in_batch, newly_found_channels_in_batch, updated_edges_in_batch"
   ],
   "id": "975b454ea1069823",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 4: Mention Processing Loop\n",
    "print(f\"\\n--- Starting Mention Processing Phase at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "BATCH_SIZE = config.MENTION_PROC_BATCH_SIZE\n",
    "MAX_BATCHES_PER_RUN = config.MENTION_PROC_MAX_BATCHES\n",
    "total_videos_processed_run = 0; total_new_channels_run = 0\n",
    "total_edges_updated_run = 0; # Removed context counter\n",
    "batches_processed_run = 0\n",
    "mention_loop_start_time = time.time()\n",
    "batch_processing_times_mention_loop = []\n",
    "\n",
    "# Indication of total unprocessed videos at the START\n",
    "try:\n",
    "    unproc_cursor = db_conn.cursor()\n",
    "    unproc_cursor.execute(\"SELECT COUNT(*) FROM Videos WHERE mentions_processed_at IS NULL\")\n",
    "    initial_total_unprocessed_videos = unproc_cursor.fetchone()[0]\n",
    "    print(f\"Estimated total unprocessed videos at start of run: {initial_total_unprocessed_videos}\")\n",
    "    if initial_total_unprocessed_videos > 0:\n",
    "        initial_total_expected_batches = (initial_total_unprocessed_videos + BATCH_SIZE -1) // BATCH_SIZE\n",
    "        print(f\"Expecting around {initial_total_expected_batches} batches in total (this run will process up to {MAX_BATCHES_PER_RUN}).\")\n",
    "except sqlite3.Error as e_count:\n",
    "    print(f\"Could not get initial count of unprocessed videos: {e_count}\")\n",
    "    initial_total_expected_batches = MAX_BATCHES_PER_RUN\n",
    "\n",
    "while batches_processed_run < MAX_BATCHES_PER_RUN:\n",
    "    batch_loop_start_time = time.time()\n",
    "    print(f\"\\nFetching mention processing batch {batches_processed_run + 1}/{MAX_BATCHES_PER_RUN} (Batch size: {BATCH_SIZE})...\")\n",
    "    videos_to_process = database.get_unprocessed_videos_batch(db_conn, BATCH_SIZE)\n",
    "\n",
    "    if not videos_to_process: print(\"No more videos found needing mention processing.\"); break\n",
    "\n",
    "    print(f\"Processing mentions for {len(videos_to_process)} videos...\")\n",
    "    try:\n",
    "        count, new_chans, edges = process_video_mentions_batch(videos_to_process, api_client, db_conn)\n",
    "\n",
    "        total_videos_processed_run += count\n",
    "        total_new_channels_run += new_chans\n",
    "        total_edges_updated_run += edges\n",
    "        batches_processed_run += 1\n",
    "        batch_duration = time.time() - batch_loop_start_time\n",
    "        batch_processing_times_mention_loop.append(batch_duration)\n",
    "\n",
    "        print(f\"Batch {batches_processed_run} complete in {batch_duration:.2f}s. Processed: {count} videos, Found: {new_chans} new channels, Upserted: {edges} edges.\")\n",
    "\n",
    "        if VERBOSE_MODE and batches_processed_run < MAX_BATCHES_PER_RUN and batch_processing_times_mention_loop and len(videos_to_process) == BATCH_SIZE:\n",
    "            avg_time_per_batch = sum(batch_processing_times_mention_loop) / len(batch_processing_times_mention_loop)\n",
    "            remaining_batches_in_run = MAX_BATCHES_PER_RUN - batches_processed_run\n",
    "            batches_for_eta = remaining_batches_in_run\n",
    "            if 'initial_total_expected_batches' in locals() and initial_total_expected_batches > batches_processed_run:\n",
    "                 batches_for_eta = min(remaining_batches_in_run, initial_total_expected_batches - batches_processed_run)\n",
    "\n",
    "            est_remaining_run_secs = batches_for_eta * avg_time_per_batch\n",
    "            if est_remaining_run_secs > 0:\n",
    "                est_run_mins, est_run_s = divmod(int(est_remaining_run_secs), 60)\n",
    "                print(f\"Est. time remaining for *this run* (up to {MAX_BATCHES_PER_RUN} batches, or fewer if DB empties): {est_run_mins}m {est_run_s}s\")\n",
    "\n",
    "        time.sleep(max(0.2, 1.0 - (0.05 * count / BATCH_SIZE if BATCH_SIZE > 0 else 1)))\n",
    "\n",
    "    except Exception as e: print(f\"Error during mention processing batch: {e}\"); logging.error(\"Error in mention processing loop\", exc_info=True); print(\"Stopping mention processing due to error.\"); break\n",
    "\n",
    "# UPDATED: Removed context from final summary\n",
    "print(f\"\\n--- Mention Processing Phase Finished (for this run) in {time.time() - mention_loop_start_time:.2f}s ---\")\n",
    "print(f\"Total videos marked as processed in this run: {total_videos_processed_run}\")\n",
    "print(f\"Total new channels discovered via mentions in this run: {total_new_channels_run}\")\n",
    "print(f\"Total collaboration edge instances upserted in this run: {total_edges_updated_run}\")\n",
    "\n",
    "try:\n",
    "    final_unproc_cursor = db_conn.cursor()\n",
    "    final_unproc_cursor.execute(\"SELECT COUNT(*) FROM Videos WHERE mentions_processed_at IS NULL\")\n",
    "    final_unprocessed_videos = final_unproc_cursor.fetchone()[0]\n",
    "    print(f\"\\nVideos remaining to be processed in database: {final_unprocessed_videos}\")\n",
    "except sqlite3.Error as e_count_end:\n",
    "    print(f\"Could not get final count of unprocessed videos: {e_count_end}\")\n",
    "\n",
    "print(\"-\" * 30)"
   ],
   "id": "9b8a33530f48500e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Referesh Cycle",
   "id": "89a825b0f8352143"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 5: Refresh Function\n",
    "\n",
    "def run_refresh_cycle(current_api_client, current_db_conn, num_channels_to_refresh):\n",
    "    \"\"\"\n",
    "    Refreshes details and fetches recent videos for a prioritized subset of\n",
    "    the \"stalest\" channels (those not updated in the longest time). Includes follower count.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Prioritized Channel Refresh Cycle at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "    overall_refresh_start_time = time.time()\n",
    "    processed_count = 0; new_videos_found_total = 0\n",
    "    channel_refresh_times = []\n",
    "\n",
    "    try:\n",
    "        # Get prioritized list instead of random\n",
    "        print(f\"Fetching up to {num_channels_to_refresh} of the stalest channels from the database...\")\n",
    "        ids_to_refresh = database.get_stale_channels_for_refresh(current_db_conn, num_channels_to_refresh)\n",
    "        actual_to_refresh_count = len(ids_to_refresh)\n",
    "\n",
    "        if not ids_to_refresh:\n",
    "            print(\"No channels found to refresh.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Prioritized list created. Refreshing {actual_to_refresh_count} channels...\")\n",
    "\n",
    "        for i, channel_row in enumerate(ids_to_refresh):\n",
    "            channel_refresh_start_time = time.time()\n",
    "            time.sleep(0.4) # API courtesy\n",
    "            channel_id = channel_row['id']\n",
    "            channel_log_name = channel_row['login']\n",
    "\n",
    "            print(f\"\\n ({i + 1}/{actual_to_refresh_count}) Refreshing channel: {channel_log_name}...\")\n",
    "\n",
    "            # 1. Refresh Channel Details (one-by-one)\n",
    "            details_list = current_api_client.get_user_details(user_ids=[channel_id])\n",
    "\n",
    "            if details_list:\n",
    "                user_data = details_list[0]\n",
    "                # Get follower count\n",
    "                follower_count = current_api_client.get_channel_follower_count(broadcaster_id=channel_id)\n",
    "                if follower_count is not None:\n",
    "                    user_data['follower_count'] = follower_count\n",
    "\n",
    "                try:\n",
    "                    database.save_channel_details(current_db_conn, user_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"  -> DB Error saving details for channel {channel_id}: {e}\")\n",
    "\n",
    "            elif details_list is None:\n",
    "                print(f\"  -> API call failed fetching details for channel {channel_id}. Skipping.\")\n",
    "                continue # Skip this channel if we can't get its details\n",
    "\n",
    "            # 2. Fetch New Videos\n",
    "            if details_list is not None:\n",
    "                latest_stored_date = database.get_latest_video_date_for_channel(current_db_conn, channel_id)\n",
    "\n",
    "                new_videos = current_api_client.get_channel_videos(\n",
    "                    channel_id,\n",
    "                    video_type='archive',\n",
    "                    # This limit prevents a single channel from dominating the refresh cycle.\n",
    "                    # The database grows incrementally over multiple runs.\n",
    "                    limit=100,\n",
    "                    after_date=latest_stored_date\n",
    "                )\n",
    "\n",
    "                if new_videos:\n",
    "                    if VERBOSE_MODE: print(f\" -> Found {len(new_videos)} new archive videos.\")\n",
    "                    database.save_videos(current_db_conn, new_videos)\n",
    "                    new_videos_found_total += len(new_videos)\n",
    "                elif new_videos is None:\n",
    "                    print(f\" -> API call failed fetching videos for {channel_log_name}.\")\n",
    "\n",
    "                if new_videos is not None:\n",
    "                    database.update_channel_video_fetch_time(current_db_conn, channel_id)\n",
    "\n",
    "            processed_count += 1\n",
    "\n",
    "            # Time estimation logic\n",
    "            channel_refresh_duration = time.time() - channel_refresh_start_time\n",
    "            channel_refresh_times.append(channel_refresh_duration)\n",
    "            if VERBOSE_MODE and channel_refresh_times and actual_to_refresh_count > 0:\n",
    "                avg_time_per_refresh = sum(channel_refresh_times) / len(channel_refresh_times)\n",
    "                remaining_refreshes = actual_to_refresh_count - (i + 1)\n",
    "                est_remaining_refresh_secs = remaining_refreshes * avg_time_per_refresh\n",
    "                if est_remaining_refresh_secs > 0 and i < actual_to_refresh_count - 1:\n",
    "                    est_ref_mins, est_ref_s = divmod(int(est_remaining_refresh_secs), 60)\n",
    "                    print(f\" -> Refreshed in {channel_refresh_duration:.2f}s. Est. time remaining for refresh cycle: {est_ref_mins}m {est_ref_s}s\")\n",
    "                else:\n",
    "                    print(f\" -> Refreshed in {channel_refresh_duration:.2f}s.\")\n",
    "\n",
    "        print(f\"\\n--- Channel Refresh Cycle Finished in {time.time() - overall_refresh_start_time:.2f}s ---\")\n",
    "        print(f\"Attempted refresh for {processed_count} channels.\")\n",
    "        print(f\"Found {new_videos_found_total} new videos in total during refresh.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during refresh cycle: {e}\")\n",
    "        logging.error(\"Error in refresh cycle\", exc_info=True)"
   ],
   "id": "5bc8ba422b2e5eb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 6: Run Channel Refresh Cycle\n",
    "print(f\"\\n--- Executing Channel Refresh Cycle at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "run_refresh_cycle(api_client, db_conn, num_channels_to_refresh=config.REFRESH_CYCLE_CHANNELS)\n",
    "print(\"-\" * 30)\n",
    "print(\"Channel Refresh Phase Done (for this run).\")\n",
    "print(\"-\" * 30)\n"
   ],
   "id": "924394808b835266",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Exploration\n",
    "\n",
    "Load the collected data from the database into pandas DataFrames and perform basic analysis and visualization.\n"
   ],
   "id": "27a245d7daeec98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 7: Load Data from Database\n",
    "print(f\"\\n--- Loading Data for Exploration at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "try:\n",
    "    channel_date_cols = ['created_at', 'first_seen', 'last_fetched_details', 'last_fetched_videos']\n",
    "    video_date_cols = ['published_at', 'created_at_api', 'fetched_at', 'mentions_processed_at']\n",
    "    category_date_cols = ['last_scanned_top_streams']\n",
    "\n",
    "    print(\"Loading Channels table...\")\n",
    "    channels_df = pd.read_sql_query(\"SELECT * FROM Channels\", db_conn)\n",
    "    print(\"Loading Videos table...\")\n",
    "    videos_df = pd.read_sql_query(\"SELECT * FROM Videos\", db_conn)\n",
    "    print(\"Loading Categories table...\")\n",
    "    categories_df = pd.read_sql_query(\"SELECT * FROM Categories\", db_conn)\n",
    "\n",
    "    # Convert timestamp columns\n",
    "    print(\"Converting timestamp columns...\")\n",
    "    for col in channel_date_cols:\n",
    "        if col in channels_df.columns: channels_df[col] = pd.to_datetime(channels_df[col], errors='coerce', utc=True)\n",
    "    for col in video_date_cols:\n",
    "        if col in videos_df.columns: videos_df[col] = pd.to_datetime(videos_df[col], errors='coerce', utc=True)\n",
    "    for col in category_date_cols:\n",
    "         if col in categories_df.columns: categories_df[col] = pd.to_datetime(categories_df[col], errors='coerce', utc=True)\n",
    "\n",
    "    # Convert numeric columns, coercing errors\n",
    "    print(\"Converting numeric columns and parsing durations...\")\n",
    "    channels_df['view_count'] = pd.to_numeric(channels_df['view_count'], errors='coerce')\n",
    "    channels_df['follower_count'] = pd.to_numeric(channels_df['follower_count'], errors='coerce') # Add follower_count\n",
    "    channels_df['display_name'] = channels_df['display_name'].fillna(channels_df['login'])\n",
    "    videos_df['view_count'] = pd.to_numeric(videos_df['view_count'], errors='coerce')\n",
    "    videos_df['duration_seconds'] = videos_df['duration'].apply(parse_duration_for_collab).fillna(0).astype(int)\n",
    "\n",
    "    print(f\"Loaded {len(channels_df)} channels, {len(videos_df)} videos, and {len(categories_df)} categories.\")\n",
    "    print(\"--- Data Loading Complete ---\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from database: {e}\")\n",
    "    raise SystemExit(\"Stopping notebook due to data loading failure.\")"
   ],
   "id": "889d3742ca50c522",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 8: Display Sample Data\n",
    "print(\"\\nSample Channels Data:\")\n",
    "display(channels_df.head())\n",
    "print(\"\\nSample Videos Data:\")\n",
    "display(\n",
    "    videos_df[['id', 'channel_id', 'title', 'published_at', 'view_count', 'duration_seconds', 'mentions_processed_at',\n",
    "               'game_name']].head())\n",
    "print(\"\\nSample Categories Data:\")\n",
    "display(categories_df.head())\n"
   ],
   "id": "8246499271cdb108",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 9: Basic Statistics\n",
    "print(\"\\n--- Basic Statistics ---\")\n",
    "print(\"\\nChannel Statistics:\")\n",
    "print(f\"Total Channels: {len(channels_df)}\")\n",
    "print(\"\\nBroadcaster Types:\")\n",
    "print(channels_df['broadcaster_type'].value_counts(dropna=False))\n",
    "print(\"\\nChannel Follower Count Summary:\")\n",
    "print(channels_df['follower_count'].describe())\n",
    "print(f\"Channels missing details: {channels_df['last_fetched_details'].isnull().sum()}\")\n",
    "\n",
    "print(\"\\nVideo Statistics:\")\n",
    "print(f\"Total Videos: {len(videos_df)}\")\n",
    "print(\"\\nVideo Types:\")\n",
    "print(videos_df['type'].value_counts(dropna=False))\n",
    "print(\"\\nVideo View Count Summary:\")\n",
    "print(videos_df['view_count'].describe())\n",
    "print(\"\\nVideo Duration (seconds) Summary:\")\n",
    "print(videos_df['duration_seconds'].describe())\n",
    "print(f\"Videos with mentions processed: {videos_df['mentions_processed_at'].notnull().sum()}\")\n",
    "print(f\"Videos pending mention processing: {videos_df['mentions_processed_at'].isnull().sum()}\")"
   ],
   "id": "6416a94fbeb6693c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Visualization (Channels & Videos)\n",
   "id": "7aa877841300cbbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 10: Visualizations - Channels\n",
    "print(\"\\n--- Channel Visualizations ---\")\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# --- UPDATED: Histogram of Channel Follower Counts ---\n",
    "print(\"Generating Channel Follower Count Histogram...\")\n",
    "plt.figure(figsize=(10, 5));\n",
    "# Use the 'follower_count' column now, and drop any rows where it might be NaN\n",
    "followers_positive = channels_df.dropna(subset=['follower_count'])\n",
    "followers_positive = followers_positive[followers_positive['follower_count'] > 0]['follower_count']\n",
    "\n",
    "if not followers_positive.empty:\n",
    "    # Determine if log scale is needed by checking the data range\n",
    "    should_use_log = (followers_positive.max() / followers_positive.min() > 100) if followers_positive.min() > 0 else False\n",
    "\n",
    "    # Conditionally set the plotting arguments\n",
    "    plot_kwargs = {'bins': 40, 'kde': False}\n",
    "    if should_use_log:\n",
    "        plot_kwargs['log_scale'] = True # Set to True, not a variable that could be False\n",
    "        title = 'Distribution of Channel Follower Counts (Log Scale)'\n",
    "        xlabel = 'Total Follower Count (Log Scale)'\n",
    "    else:\n",
    "        # Do not pass log_scale when it's not needed\n",
    "        title = 'Distribution of Channel Follower Counts'\n",
    "        xlabel = 'Total Follower Count'\n",
    "\n",
    "    sns.histplot(followers_positive, **plot_kwargs)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Number of Channels')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No positive channel follower data to plot histogram yet.\")\n",
    "\n",
    "print(\"Generating Broadcaster Types Bar Chart...\")\n",
    "plt.figure(figsize=(8, 5));\n",
    "channel_types = channels_df['broadcaster_type'].fillna('N/A').replace('', 'N/A')\n",
    "sns.countplot(y=channel_types, order=channel_types.value_counts().index, palette='viridis');\n",
    "plt.title('Channel Broadcaster Types')\n",
    "plt.xlabel('Number of Channels')\n",
    "plt.ylabel('Broadcaster Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "8fddecb20676a379",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 11: Visualizations - Videos\n",
    "print(\"\\n--- Video Visualizations ---\")\n",
    "\n",
    "print(\"Generating Video View Count Histogram...\")\n",
    "plt.figure(figsize=(10, 5));\n",
    "views_positive_vid = videos_df.dropna(subset=['view_count'])\n",
    "views_positive_vid = views_positive_vid[views_positive_vid['view_count'] > 0]['view_count']\n",
    "\n",
    "if not views_positive_vid.empty:\n",
    "    # Determine if log scale is needed\n",
    "    should_use_log_vid = (views_positive_vid.max() / views_positive_vid.min() > 100) if views_positive_vid.min() > 0 else False\n",
    "\n",
    "    # Conditionally set the plotting arguments\n",
    "    plot_kwargs_vid = {'bins': 40, 'kde': False}\n",
    "    if should_use_log_vid:\n",
    "        plot_kwargs_vid['log_scale'] = True\n",
    "        title_vid = 'Distribution of Video View Counts (Log Scale)'\n",
    "        xlabel_vid = 'Video View Count (Log Scale)'\n",
    "    else:\n",
    "        title_vid = 'Distribution of Video View Counts'\n",
    "        xlabel_vid = 'Video View Count'\n",
    "\n",
    "    sns.histplot(views_positive_vid, **plot_kwargs_vid)\n",
    "    plt.title(title_vid)\n",
    "    plt.xlabel(xlabel_vid)\n",
    "    plt.ylabel('Number of Videos')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No positive video view data to plot histogram.\")\n",
    "\n",
    "print(\"Generating Video Types Bar Chart...\")\n",
    "plt.figure(figsize=(8, 5));\n",
    "video_types = videos_df['type'].fillna('N/A');\n",
    "sns.countplot(y=video_types, order=video_types.value_counts().index, palette='magma');\n",
    "plt.title('Video Types');\n",
    "plt.xlabel('Number of Videos');\n",
    "plt.ylabel('Type');\n",
    "plt.tight_layout();\n",
    "plt.show()\n",
    "\n",
    "# --- Video Publication Time Series ---\n",
    "print(\"Generating Video Publication Time Series...\")\n",
    "plt.figure(figsize=(12, 6));\n",
    "video_pub_dates = videos_df.dropna(subset=['published_at'])\n",
    "\n",
    "if not video_pub_dates.empty and len(video_pub_dates) > 1:\n",
    "    # --- Calculate the 5th percentile date to set the start range ---\n",
    "    start_date = video_pub_dates['published_at'].quantile(0.05)\n",
    "    print(f\"Displaying time series from {start_date.strftime('%Y-%m-%d')} onwards (showing 95% of the data).\")\n",
    "\n",
    "    # Filter the DataFrame to this new date range\n",
    "    plotting_df = video_pub_dates[video_pub_dates['published_at'] >= start_date]\n",
    "\n",
    "    # --- Use the filtered plotting_df for the rest of the logic ---\n",
    "    if not plotting_df.empty:\n",
    "        time_range_days = (plotting_df['published_at'].max() - plotting_df['published_at'].min()).days if len(plotting_df) > 1 else 0\n",
    "        resample_freq = 'ME' if time_range_days > 90 else 'D' # Resample by Month or Day\n",
    "        plot_title = 'Number of Videos Published (' + ('Monthly' if resample_freq == 'ME' else 'Daily') + ')'\n",
    "\n",
    "        # Plot the resampled data\n",
    "        plotting_df.set_index('published_at')['id'].resample(resample_freq).count().plot(marker='.' if resample_freq == 'D' else 'o', linestyle='-');\n",
    "\n",
    "        plt.title(plot_title);\n",
    "        plt.ylabel('Number of Videos');\n",
    "        plt.xlabel('Publication Date');\n",
    "        plt.tight_layout();\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No video data remains after filtering by start date.\")\n",
    "else:\n",
    "    print(\"Not enough video publication date data to plot a meaningful time series.\")"
   ],
   "id": "9f967ce2b74eb455",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Note on \"Streaming Together\" Feature\n",
    "\n",
    "While Twitch has features like \"Squad Streams\" or \"Guest Star\" allowing multiple creators to stream simultaneously on one channel, the participant data for these features **does not appear to be reliably available** via the standard Twitch API for *past* streams or VODs (as of June 2025).\n",
    "\n",
    "Therefore, the collaboration detection in this notebook relies primarily on identifying `@mentions` within video titles and descriptions.\n"
   ],
   "id": "8d17c13ab5a5f533"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Collaboration Network Exploration (Filtered)\n",
    "\n",
    "Exploring the collaboration network. Data is filtered IN MEMORY based on\n",
    "thresholds in `config.py` BEFORE analysis to improve performance.\n",
    "The underlying database remains complete.\n"
   ],
   "id": "cd855c40a6302025"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 12: Load and Filter Data for Network Analysis\n",
    "print(f\"\\n--- Loading and Filtering Data for Network Analysis at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "channels_df_net = pd.DataFrame()  # For channels passing all filters\n",
    "collab_df_net = pd.DataFrame()   # For edges passing all filters\n",
    "G_filtered = nx.Graph()          # The filtered graph for analysis\n",
    "degree_df_filtered = pd.DataFrame() # For degree stats\n",
    "\n",
    "try:\n",
    "    # --- 1. Load base data ---\n",
    "    collab_df_full = pd.read_sql_query(\"SELECT * FROM Collaborations\", db_conn)\n",
    "    collab_df_full['collaboration_count'] = pd.to_numeric(collab_df_full['collaboration_count'], errors='coerce').fillna(0).astype(int)\n",
    "    collab_df_full['total_collaboration_duration_seconds'] = pd.to_numeric(collab_df_full['total_collaboration_duration_seconds'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "\n",
    "    # --- 2. Filter Channels ---\n",
    "    print(f\"Filtering channels by follower count (>= {config.NETWORK_MIN_FOLLOWER_COUNT})...\")\n",
    "    channels_df_filtered_fc = channels_df[channels_df['follower_count'] >= config.NETWORK_MIN_FOLLOWER_COUNT]\n",
    "    print(f\" -> Channels after follower count filter: {len(channels_df_filtered_fc)}\")\n",
    "\n",
    "    if not videos_df.empty:\n",
    "        print(f\"Filtering channels by video count (>= {config.NETWORK_MIN_CHANNEL_VIDEO_COUNT})...\")\n",
    "        video_counts_per_channel = videos_df['channel_id'].value_counts()\n",
    "        channels_with_enough_videos = video_counts_per_channel[video_counts_per_channel >= config.NETWORK_MIN_CHANNEL_VIDEO_COUNT].index.tolist()\n",
    "        channels_df_net = channels_df_filtered_fc[channels_df_filtered_fc['id'].isin(channels_with_enough_videos)]\n",
    "        print(f\" -> Channels after video count filter: {len(channels_df_net)}\")\n",
    "    else:\n",
    "        print(\"Warning: videos_df not available for filtering by video count. Using only follower count filter for channels.\")\n",
    "        channels_df_net = channels_df_filtered_fc\n",
    "\n",
    "    valid_channel_ids_for_network = set(channels_df_net['id'])\n",
    "    print(f\"Total channels passing node filters: {len(valid_channel_ids_for_network)}\")\n",
    "\n",
    "    # --- 3. Filter Collaborations (Edges) ---\n",
    "    if not collab_df_full.empty:\n",
    "        print(f\"Filtering collaboration edges by count (>= {config.NETWORK_MIN_COLLABORATION_COUNT})...\")\n",
    "        collab_df_filtered_count = collab_df_full[collab_df_full['collaboration_count'] >= config.NETWORK_MIN_COLLABORATION_COUNT]\n",
    "        print(f\" -> Edges after count filter: {len(collab_df_filtered_count)}\")\n",
    "\n",
    "        print(\"Filtering edges to ensure both connected channels passed node filters...\")\n",
    "        collab_df_net = collab_df_filtered_count[\n",
    "            collab_df_filtered_count['channel_id_1'].isin(valid_channel_ids_for_network) &\n",
    "            collab_df_filtered_count['channel_id_2'].isin(valid_channel_ids_for_network)\n",
    "        ].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "        print(f\" -> Edges after node validity filter: {len(collab_df_net)}\")\n",
    "\n",
    "        # --- Cap outlier durations before creating graph ---\n",
    "        duration_threshold_weeks = config.NETWORK_DURATION_OUTLIER_WEEKS\n",
    "        duration_threshold_seconds = duration_threshold_weeks * 7 * 24 * 3600\n",
    "\n",
    "        outlier_edges = collab_df_net['total_collaboration_duration_seconds'] > duration_threshold_seconds\n",
    "        outlier_count = outlier_edges.sum()\n",
    "\n",
    "        if outlier_count > 0:\n",
    "            # The f-string now uses the config variable directly for accurate reporting\n",
    "            print(f\"Capping duration for {outlier_count} edge(s) with duration > {duration_threshold_weeks} week(s) for network analysis.\")\n",
    "\n",
    "            # Use .loc to safely modify the DataFrame\n",
    "            collab_df_net.loc[outlier_edges, 'total_collaboration_duration_seconds'] = duration_threshold_seconds\n",
    "\n",
    "    else:\n",
    "        print(\"Full collaboration data (collab_df_full) is empty. Filtered collaboration data will be empty.\")\n",
    "        collab_df_net = pd.DataFrame()\n",
    "\n",
    "    # --- 4. Create Filtered NetworkX Graph ---\n",
    "    if not collab_df_net.empty:\n",
    "        print(\"Creating NetworkX graph G_filtered from filtered data...\")\n",
    "        G_filtered = nx.from_pandas_edgelist(\n",
    "            collab_df_net,\n",
    "            'channel_id_1',\n",
    "            'channel_id_2',\n",
    "            edge_attr=['collaboration_count', 'total_collaboration_duration_seconds', 'latest_collaboration_timestamp']\n",
    "        )\n",
    "        G_filtered.add_nodes_from(valid_channel_ids_for_network)\n",
    "        print(f\"Filtered graph G_filtered created with {G_filtered.number_of_nodes()} nodes and {G_filtered.number_of_edges()} edges.\")\n",
    "    else:\n",
    "        print(\"No edges passed all filters. Filtered graph G_filtered will be empty or contain only isolated nodes.\")\n",
    "        if valid_channel_ids_for_network: G_filtered.add_nodes_from(valid_channel_ids_for_network)\n",
    "        print(f\"Filtered graph G_filtered has {G_filtered.number_of_nodes()} nodes and {G_filtered.number_of_edges()} edges.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or filtering data for network analysis: {e}\")\n",
    "    logging.error(\"Error in network data prep:\", exc_info=True)\n",
    "    if 'G_filtered' not in locals(): G_filtered = nx.Graph()\n",
    "\n",
    "print(\"--- Network Data Preparation Complete ---\")"
   ],
   "id": "e24a067f29d07663",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 13: Display Sample Filtered Collaboration Data\n",
    "print(\"\\nSample Filtered Collaboration Edges (collab_df_net):\")\n",
    "if not collab_df_net.empty:\n",
    "    display(collab_df_net.head())\n",
    "else:\n",
    "    print(\"No collaboration data in collab_df_net (all edges filtered out or none exist).\")\n"
   ],
   "id": "503c6eba174063ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 14: Filtered Collaboration Network Statistics\n",
    "if 'G_filtered' in locals() and G_filtered.number_of_nodes() > 0:\n",
    "    print(\"\\n--- Filtered Collaboration Network Statistics ---\")\n",
    "    print(f\"Total Edges in Filtered Network: {G_filtered.number_of_edges()}\")\n",
    "    print(f\"Total Nodes in Filtered Network: {G_filtered.number_of_nodes()}\")\n",
    "\n",
    "    if G_filtered.number_of_nodes() > 0:\n",
    "        degree_sequence = [d for n, d in G_filtered.degree()]\n",
    "        degree_df_filtered = pd.DataFrame({'channel_id': list(G_filtered.nodes()), 'degree': degree_sequence})\n",
    "\n",
    "        print(\"\\nDegree Distribution Summary (Filtered Network):\")\n",
    "        print(degree_df_filtered['degree'].describe())\n",
    "\n",
    "        # Merge with channel names for context\n",
    "        channels_for_labels_df = channels_df[['id', 'login', 'display_name']].rename(columns={'id': 'channel_id'})\n",
    "        degree_df_filtered = pd.merge(degree_df_filtered, channels_for_labels_df, on='channel_id', how='left')\n",
    "\n",
    "        print(\"\\nTop 10 Channels by Degree (Filtered Network):\")\n",
    "        print(degree_df_filtered.nlargest(10, 'degree'))\n",
    "    else:\n",
    "        print(\"Filtered graph has no nodes to calculate degree.\")\n",
    "\n",
    "    # Summary for edges in collab_df_net\n",
    "    if not collab_df_net.empty:\n",
    "        print(\"\\nCollaboration Count per Edge Summary (Filtered Network):\")\n",
    "        print(collab_df_net['collaboration_count'].describe())\n",
    "\n",
    "        # --- Human-Readable Duration Summary ---\n",
    "        print(\"\\nTotal Collaboration Duration per Edge Summary (Filtered Network):\")\n",
    "        duration_stats = collab_df_net['total_collaboration_duration_seconds'].describe()\n",
    "\n",
    "        # Create and print a formatted summary\n",
    "        formatted_stats = {\n",
    "            'count': f\"{duration_stats['count']:.0f}\",\n",
    "            'mean': format_seconds_to_hm(duration_stats['mean']),\n",
    "            'std dev': f\"~{format_seconds_to_hm(duration_stats['std'])}\",\n",
    "            'min': format_seconds_to_hm(duration_stats['min']),\n",
    "            '25%': format_seconds_to_hm(duration_stats['25%']),\n",
    "            '50% (median)': format_seconds_to_hm(duration_stats['50%']),\n",
    "            '75%': format_seconds_to_hm(duration_stats['75%']),\n",
    "            'max': format_seconds_to_hm(duration_stats['max'])\n",
    "        }\n",
    "\n",
    "        for key, value in formatted_stats.items():\n",
    "            # Left-align the key, right-align the value for clean output\n",
    "            print(f\"{key:<15} {value:>15}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No edges in collab_df_net to summarize.\")\n",
    "else:\n",
    "    print(\"\\nFiltered graph G_filtered is empty. No statistics to display.\")"
   ],
   "id": "dc914e19f65da2d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 15: Filtered Collaboration Network Visualizations\n",
    "if 'G_filtered' in locals() and G_filtered.number_of_nodes() > 0 and 'degree_df_filtered' in locals() and not degree_df_filtered.empty:\n",
    "    print(\"\\n--- Filtered Collaboration Network Visualizations ---\")\n",
    "\n",
    "    # --- 1. Degree Distribution Histogram ---\n",
    "    print(\"Generating Degree Distribution Histogram for the filtered network...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    degrees_filtered = degree_df_filtered[degree_df_filtered['degree'] > 0]['degree']\n",
    "    if not degrees_filtered.empty:\n",
    "        # Log-log scale is common for degree distributions to check for power-law behavior\n",
    "        sns.histplot(degrees_filtered, log_scale=True, bins=30)\n",
    "        plt.title('Degree Distribution of Filtered Collaboration Network (Log-Log Scale)')\n",
    "        plt.xlabel('Degree (Number of Unique Collaborators) - Log Scale')\n",
    "        plt.ylabel('Number of Channels - Log Scale')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No nodes with degree > 0 found to plot.\")\n",
    "\n",
    "    # --- 2. Edge Weight (Collaboration Count) Plot ---\n",
    "    if not collab_df_net.empty:\n",
    "        print(\"\\nGenerating Edge Weight (Collaboration Count) Plot...\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        collab_counts_filtered = collab_df_net[collab_df_net['collaboration_count'] > 0]['collaboration_count']\n",
    "\n",
    "        if not collab_counts_filtered.empty:\n",
    "            # A countplot is more direct and robust for this type of discrete integer data\n",
    "            ax = sns.countplot(x=collab_counts_filtered, palette='viridis', order = sorted(collab_counts_filtered.unique()))\n",
    "\n",
    "            # Add text labels on top of each bar\n",
    "            for p in ax.patches:\n",
    "                ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                            ha='center', va='center', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "            plt.title('Distribution of Collaboration Counts per Edge (Filtered Network)')\n",
    "            plt.xlabel('Number of Collaborations on a Single Edge')\n",
    "\n",
    "            # Conditionally apply log scale to y-axis only if counts are high\n",
    "            max_count = collab_counts_filtered.value_counts().max()\n",
    "            if max_count > 10:\n",
    "                plt.yscale('log')\n",
    "                plt.ylabel('Number of Edges (Log Scale)')\n",
    "            else:\n",
    "                plt.ylabel('Number of Edges (Linear Scale)')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No collaborations with count > 0 found to plot.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nFiltered graph or degree data insufficient for visualization. Skipping.\")"
   ],
   "id": "2c402ce1f749e696",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Community Detection in Collaboration Network",
   "id": "a74cb851323c91af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 16: Community Detection Setup and Execution\n",
    "import community as community_louvain  # python-louvain library\n",
    "\n",
    "communities_detected = False\n",
    "partition = {}\n",
    "communities = {}\n",
    "\n",
    "if 'G_filtered' in locals() and G_filtered.number_of_nodes() > 0:\n",
    "    print(\n",
    "        f\"\\n--- Performing Louvain Community Detection on FILTERED graph at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "    print(f\"Using 'collaboration_count' as edge weight for community detection.\")\n",
    "\n",
    "    for u, v, data in G_filtered.edges(data=True):\n",
    "        if 'weight' not in data:\n",
    "            G_filtered.edges[u, v]['weight'] = data.get('collaboration_count', 1)\n",
    "\n",
    "    start_time_community = time.time()\n",
    "    try:\n",
    "        partition = community_louvain.best_partition(G_filtered, weight='weight', random_state=42)\n",
    "        modularity = community_louvain.modularity(partition, G_filtered, weight='weight')\n",
    "\n",
    "        for node, community_id in partition.items():\n",
    "            if community_id not in communities: communities[community_id] = []\n",
    "            communities[community_id].append(node)\n",
    "\n",
    "        num_communities = len(communities)\n",
    "        print(f\"Louvain Community Detection complete in {time.time() - start_time_community:.2f}s.\")\n",
    "        print(f\"Found {num_communities} communities.\")\n",
    "        print(f\"Modularity of the partition: {modularity:.4f}\")\n",
    "\n",
    "        sorted_communities_by_size = sorted(communities.items(), key=lambda item: len(item[1]), reverse=True)\n",
    "        print(\"\\nTop 5 Largest Communities:\")\n",
    "        for i in range(min(5, len(sorted_communities_by_size))):\n",
    "            cid, nodes = sorted_communities_by_size[i]\n",
    "            member_names = [\n",
    "                channels_for_labels_df.loc[channels_for_labels_df['channel_id'] == node_id, 'login'].iloc[0]\n",
    "                for node_id in nodes[:3]\n",
    "                if not channels_for_labels_df[channels_for_labels_df['channel_id'] == node_id].empty\n",
    "            ]\n",
    "            print(\n",
    "                f\"  Community {cid} (Size rank {i + 1}): {len(nodes)} members. Examples: {', '.join(member_names[:3])}...\")\n",
    "        communities_detected = True\n",
    "    except Exception as e:\n",
    "        print(f\"Error during community detection: {e}\")\n",
    "else:\n",
    "    print(\"\\nFiltered graph G_filtered not found or is empty. Skipping community detection.\")\n"
   ],
   "id": "23b293ec0907b0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualizing Communities",
   "id": "cd49e319fa29ca8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell 17: Visualize Subgraph with Community Colors\n",
    "if communities_detected and 'G_filtered' in locals() and G_filtered.number_of_nodes() > 0:\n",
    "    print(\"\\n--- NetworkX Subgraph Visualization with Community Colors ---\")\n",
    "    try:\n",
    "        # Use the same subgraph logic as before to select nodes for visualization\n",
    "        top_nodes_filtered = degree_df_filtered.nlargest(config.NETWORK_VIZ_TOP_N_CHANNELS_BY_DEGREE, 'degree')['channel_id'].tolist()\n",
    "        subgraph_nodes_set = set(top_nodes_filtered)\n",
    "        max_subgraph_nodes = config.NETWORK_VIZ_MAX_SUBGRAPH_NODES\n",
    "\n",
    "        for node in list(subgraph_nodes_set):\n",
    "            if G_filtered.has_node(node) and len(subgraph_nodes_set) < max_subgraph_nodes:\n",
    "                neighbors = list(G_filtered.neighbors(node))\n",
    "                needed = max_subgraph_nodes - len(subgraph_nodes_set)\n",
    "                subgraph_nodes_set.update(neighbors[:needed])\n",
    "\n",
    "        subgraph = G_filtered.subgraph(list(subgraph_nodes_set))\n",
    "        print(f\"Creating subgraph visualization with {subgraph.number_of_nodes()} nodes and {subgraph.number_of_edges()} edges, clustered by community.\")\n",
    "\n",
    "        if subgraph.number_of_nodes() > 0:\n",
    "            # --- NEW: Create a temporary graph for community-aware layout ---\n",
    "            # We will modify edge weights in this copy to influence the layout\n",
    "            layout_graph = subgraph.copy()\n",
    "\n",
    "            # Increase weight for intra-community edges, decrease for inter-community\n",
    "            # This makes the \"springs\" inside a community much stronger.\n",
    "            for u, v in layout_graph.edges():\n",
    "                if partition.get(u) == partition.get(v):\n",
    "                    # Strengthen connection if nodes are in the same community\n",
    "                    layout_graph.edges[u,v]['weight'] = 5  # High weight for strong attraction\n",
    "                else:\n",
    "                    # Weaken connection if nodes are in different communities\n",
    "                    layout_graph.edges[u,v]['weight'] = 0.1 # Low weight for weak attraction\n",
    "\n",
    "            # --- Calculate positions using the modified layout_graph ---\n",
    "            pos_subgraph = nx.spring_layout(layout_graph, weight='weight', k=0.4, iterations=50, seed=42)\n",
    "\n",
    "            # --- Visualization logic remains mostly the same, but uses the new positions ---\n",
    "\n",
    "            # Get community colors for nodes in the subgraph\n",
    "            unique_community_ids_in_subgraph = sorted(list(set(partition[node] for node in subgraph.nodes() if node in partition)))\n",
    "            community_to_color_idx = {comm_id: i for i, comm_id in enumerate(unique_community_ids_in_subgraph)}\n",
    "            num_distinct_colors_needed = len(unique_community_ids_in_subgraph)\n",
    "\n",
    "            node_colors_subgraph = 'grey'\n",
    "            if num_distinct_colors_needed > 0:\n",
    "                cmap = plt.cm.get_cmap('tab20', num_distinct_colors_needed) if num_distinct_colors_needed <= 20 else plt.cm.get_cmap('viridis', num_distinct_colors_needed)\n",
    "                node_colors_subgraph = [cmap(community_to_color_idx.get(partition.get(node), -1)) for node in subgraph.nodes()]\n",
    "\n",
    "            # Get node sizes and labels based on the original data\n",
    "            node_data_subgraph = channels_df[channels_df['id'].isin(subgraph.nodes())].set_index('id')\n",
    "            subgraph_node_sizes = [math.log10(max(1, node_data_subgraph.loc[node, 'follower_count'] if node in node_data_subgraph.index else 1)+1) * 300 + 100 for node in subgraph.nodes()]\n",
    "            node_labels_subgraph = pd.merge(pd.DataFrame({'channel_id': list(subgraph.nodes())}), channels_for_labels_df, on='channel_id', how='left').set_index('channel_id')['login'].to_dict()\n",
    "\n",
    "            # Use original collaboration count for visual edge thickness\n",
    "            edge_widths_subgraph = [math.log10(max(0, d.get('collaboration_count',0)) + 1) * 1.5 + 0.5 for u, v, d in subgraph.edges(data=True)]\n",
    "\n",
    "            # --- Drawing ---\n",
    "            plt.figure(figsize=(18, 15))\n",
    "\n",
    "            # Draw using the original subgraph data but the new positions (pos_subgraph)\n",
    "            nx.draw_networkx_nodes(subgraph, pos_subgraph, node_size=subgraph_node_sizes, node_color=node_colors_subgraph, alpha=0.9)\n",
    "            nx.draw_networkx_edges(subgraph, pos_subgraph, width=edge_widths_subgraph, alpha=0.2, edge_color='gray')\n",
    "            nx.draw_networkx_labels(subgraph, pos_subgraph, labels=node_labels_subgraph, font_size=8)\n",
    "\n",
    "            plt.title(f'Collaboration Subgraph with Community Colors (Clustered Layout)', fontsize=16)\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Subgraph is empty, cannot visualize communities.\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"\\n`python-louvain` (community) library not found. Skipping community detection visualization.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during community visualization: {e}\")\n",
    "        logging.error(\"Community visualization error\", exc_info=True)\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo communities detected or filtered graph is empty. Skipping community visualization.\")"
   ],
   "id": "d9ddeff2aec48950",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Collaboration Topic Modeling with BERTopic\n",
    "\n",
    "Here, we derive context by analyzing the content of video titles. We use BERTopic to automatically discover topics from the titles of videos belonging to channels in our filtered collaboration network. This helps us understand what collaborating streamers talk about or play, without relying on pre-defined categories.\n"
   ],
   "id": "6e24edeb30abcc84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell: BERTopic Pre-processing: Assign Topic ID to All Relevant Videos\n",
    "\n",
    "# This cell trains the BERTopic model on a large sample of video titles\n",
    "# and then assigns a topic ID to each video in a new DataFrame.\n",
    "# This is a time-consuming step that enables fast context lookups later.\n",
    "\n",
    "# Initialize global-like variables to hold the results\n",
    "topic_model = None\n",
    "videos_with_topics_df = None\n",
    "\n",
    "print(\"\\n--- Starting Collaboration Topic Modeling with BERTopic ---\")\n",
    "\n",
    "# --- 1. Main Gate: Check if we have a filtered graph to work with ---\n",
    "if 'G_filtered' not in locals() or G_filtered.number_of_nodes() == 0:\n",
    "    print(\"\\nFiltered graph G_filtered not found or is empty. Skipping topic modeling.\")\n",
    "\n",
    "else:\n",
    "    # --- 2. If graph exists, proceed with data preparation and modeling ---\n",
    "    try:\n",
    "        from bertopic import BERTopic\n",
    "        import nltk\n",
    "        from nltk.corpus import stopwords\n",
    "        import re\n",
    "\n",
    "        # Setup NLTK stopwords\n",
    "        try:\n",
    "            stopwords.words('english')\n",
    "        except LookupError:\n",
    "            print(\"NLTK stopwords not found. Downloading...\")\n",
    "            nltk.download('stopwords')\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        mention_regex = re.compile(r'@([a-zA-Z0-9_]{4,25})')\n",
    "\n",
    "        def clean_title(title):\n",
    "            if not isinstance(title, str): return \"\"\n",
    "            text = mention_regex.sub('', title)\n",
    "            text = re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n",
    "            text = text.lower()\n",
    "            text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "            return text\n",
    "\n",
    "        print(\"Preparing data for topic modeling...\")\n",
    "        nodes_in_network = list(G_filtered.nodes())\n",
    "        videos_for_modeling_df = videos_df[videos_df['channel_id'].isin(nodes_in_network)].copy()\n",
    "        print(f\"Found {len(videos_for_modeling_df)} videos from {len(nodes_in_network)} channels in the filtered network.\")\n",
    "\n",
    "        print(f\"Filtering for English language videos...\")\n",
    "        english_videos_df = videos_for_modeling_df[videos_for_modeling_df['language'] == 'en'].copy()\n",
    "        print(f\" -> Found {len(english_videos_df)} English videos to model.\")\n",
    "\n",
    "        print(\"Cleaning video titles (removing stopwords and mentions)...\")\n",
    "        english_videos_df['cleaned_title'] = english_videos_df['title'].dropna().apply(clean_title)\n",
    "\n",
    "        docs_df = english_videos_df[english_videos_df['cleaned_title'].str.len() > 0].copy()\n",
    "        docs = docs_df['cleaned_title'].tolist()\n",
    "\n",
    "        # --- 3. Check if we have enough documents to proceed ---\n",
    "        if len(docs) < 50:\n",
    "            print(f\"Not enough video titles found ({len(docs)}) after cleaning to perform topic modeling.\")\n",
    "\n",
    "        else:\n",
    "            # --- 4. This is where the actual modeling happens ---\n",
    "            MAX_DOCS_FOR_MODELING = 50000\n",
    "            if len(docs) > MAX_DOCS_FOR_MODELING:\n",
    "                print(f\"Sampling {MAX_DOCS_FOR_MODELING} titles for topic modeling to ensure performance...\")\n",
    "                docs_df = docs_df.sample(n=MAX_DOCS_FOR_MODELING, random_state=42)\n",
    "                docs = docs_df['cleaned_title'].tolist()\n",
    "\n",
    "            print(f\"Training BERTopic model on {len(docs)} cleaned video titles. This may take several minutes...\")\n",
    "            print(\"(The first run will download pre-trained language models).\")\n",
    "\n",
    "            topic_model = BERTopic(\n",
    "                min_topic_size=20,\n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "            topics, _ = topic_model.fit_transform(docs)\n",
    "\n",
    "            print(\"Mapping topic results back to videos...\")\n",
    "            docs_df['topic_id'] = topics\n",
    "            videos_with_topics_df = docs_df # Assign to the main variable for other cells to use\n",
    "\n",
    "            print(\"\\n--- BERTopic Analysis Complete ---\")\n",
    "\n",
    "            print(\"\\nTop discovered topics from cleaned titles:\")\n",
    "            display(topic_model.get_topic_info())\n",
    "\n",
    "            print(\"\\nVisualizing top topics (barchart):\")\n",
    "            display(topic_model.visualize_barchart(top_n_topics=10))\n",
    "\n",
    "            print(\"\\nVisualizing inter-topic distance map:\")\n",
    "            display(topic_model.visualize_topics())\n",
    "\n",
    "    # --- 5. Handle potential errors for the entire block ---\n",
    "    except ImportError:\n",
    "        print(\"\\nCould not perform topic modeling. Please install required libraries:\")\n",
    "        print(\"pip install bertopic sentence-transformers scikit-learn nltk\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during topic modeling: {e}\")\n",
    "        logging.error(\"BERTopic analysis failed\", exc_info=True)"
   ],
   "id": "e4b6a8bbb9db61d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Interactive Collaboration Network Snippet (Searchable)\n",
    "\n",
    "Type part of a channel name (login or display name) and click \"Search & Visualize\" to see its immediate collaboration network.\n",
    "\n",
    "- If multiple channels match your search, select the specific one from the second dropdown.\n",
    "- Node size is proportional to the channel's total follower count (log scale).\n",
    "- Edge thickness is proportional to the number of collaboration instances found (log scale).\n",
    "- Edge labels show the most frequent game category for the collaboration and its percentage.\n",
    "\n",
    "*(Note: Embedding channel profile pictures directly within nodes is complex with this plotting method and is omitted.)*"
   ],
   "id": "cba11b95edda95a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 20: Setup for Searchable Interactive Visualization\n",
    "print(f\"\\n--- Setting up Interactive Channel Network Visualization at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "# Widgets\n",
    "search_input = Text(description=\"Channel Name:\", placeholder=\"Enter login or display name (min 3 chars)\",\n",
    "                    style={'description_width': 'initial'})\n",
    "search_button = Button(description=\"Search & Visualize\", button_style='info',\n",
    "                       tooltip='Search for the channel and display its network')\n",
    "results_dropdown = Dropdown(description=\"Select Match:\", options=[(\"---\", None)], disabled=True,\n",
    "                            style={'description_width': 'initial'}, layout={'width': 'max-content'})\n",
    "message_output = Output(layout={'margin': '10px 0 0 0'})\n",
    "plot_output = Output()\n",
    "\n"
   ],
   "id": "b9c3ac68400abf02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 21: Visualization Function for Interactive Widget\n",
    "def visualize_channel_neighborhood(selected_channel_id):\n",
    "    \"\"\"\n",
    "    Queries data and draws the network neighborhood for the selected channel,\n",
    "    using pre-processed BERTopic results for accurate edge context labels.\n",
    "    \"\"\"\n",
    "    # Load Mentions table for context lookup\n",
    "    try:\n",
    "        mentions_df = pd.read_sql_query(\"SELECT * FROM Mentions\", db_conn)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load Mentions table: {e}\")\n",
    "        mentions_df = pd.DataFrame()\n",
    "\n",
    "    with plot_output:\n",
    "        plot_output.clear_output(wait=True);\n",
    "        with message_output: message_output.clear_output()\n",
    "        if not selected_channel_id: return\n",
    "\n",
    "        selected_channel_row = channels_df[channels_df['id'] == selected_channel_id]\n",
    "        if selected_channel_row.empty: print(f\"Selected channel ID {selected_channel_id} not found.\"); return\n",
    "        selected_channel_name_for_title = selected_channel_row['display_name'].iloc[0]\n",
    "\n",
    "        print(f\"Generating visualization for: {selected_channel_name_for_title} ({selected_channel_id})...\")\n",
    "        edges = collab_df_full[(collab_df_full['channel_id_1'] == selected_channel_id) | (collab_df_full['channel_id_2'] == selected_channel_id)].copy()\n",
    "\n",
    "        if edges.empty:\n",
    "            print(f\"No collaboration data found in the database for channel: {selected_channel_name_for_title}\")\n",
    "            G = nx.Graph(); G.add_node(selected_channel_id)\n",
    "            node_data = selected_channel_row.set_index('id')\n",
    "            # Use follower_count, with a fallback to a small number\n",
    "            follower_count = max(1, node_data.loc[selected_channel_id].get('follower_count', 1))\n",
    "            node_size = math.log10(follower_count + 1) * 200 + 150\n",
    "            node_label = {selected_channel_id: selected_channel_name_for_title}\n",
    "            plt.figure(figsize=(5, 5)); pos = {selected_channel_id: (0, 0)}\n",
    "            nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color='lightcoral', alpha=0.8)\n",
    "            nx.draw_networkx_labels(G, pos, labels=node_label, font_size=9)\n",
    "            plt.title(f\"No Collaborations Found For: {selected_channel_name_for_title}\", fontsize=12); plt.axis('off'); plt.tight_layout(); plt.show()\n",
    "            return\n",
    "\n",
    "        neighbor_nodes = set(edges['channel_id_1']).union(set(edges['channel_id_2']))\n",
    "        G_viz = nx.Graph(); G_viz.add_nodes_from(list(neighbor_nodes))\n",
    "        for _, edge_row in edges.iterrows(): G_viz.add_edge(edge_row['channel_id_1'], edge_row['channel_id_2'], weight=edge_row['collaboration_count'])\n",
    "        node_data_viz = channels_df[channels_df['id'].isin(list(neighbor_nodes))].set_index('id')\n",
    "\n",
    "        missing_nodes = [n for n in list(neighbor_nodes) if n not in node_data_viz.index]\n",
    "        if missing_nodes:\n",
    "            print(f\"Warning: Missing channel data for nodes: {missing_nodes}\")\n",
    "            dummy_data = pd.DataFrame({'follower_count': 1, 'display_name': 'UNKNOWN'}, index=missing_nodes)\n",
    "            node_data_viz = pd.concat([node_data_viz, dummy_data])\n",
    "\n",
    "        node_sizes_viz = [math.log10(max(1, node_data_viz.loc[node].get('follower_count', 1)) + 1) * 200 + 150 for node in G_viz.nodes()]\n",
    "\n",
    "        node_labels_viz = {node: node_data_viz.loc[node, 'display_name'] for node in G_viz.nodes()}\n",
    "        edge_widths_viz = [math.log10(max(0, G_viz.edges[u, v]['weight']) + 1) * 1.5 + 0.5 for u, v in G_viz.edges()]\n",
    "        edge_labels_viz = {}\n",
    "        print(\"Deriving edge context from BERTopic model...\")\n",
    "\n",
    "        for u, v in G_viz.edges():\n",
    "            # 1. Find all video IDs for this specific edge from the Mentions table\n",
    "            edge_mentions = mentions_df[\n",
    "                ((mentions_df['source_channel_id'] == u) & (mentions_df['target_channel_id'] == v)) |\n",
    "                ((mentions_df['source_channel_id'] == v) & (mentions_df['target_channel_id'] == u))\n",
    "            ]\n",
    "\n",
    "            video_ids_for_edge = edge_mentions['video_id'].unique().tolist()\n",
    "\n",
    "            label = \"\"\n",
    "            if video_ids_for_edge and videos_with_topics_df is not None:\n",
    "                # 2. Look up the pre-assigned topics for these specific videos\n",
    "                edge_video_topics = videos_with_topics_df[\n",
    "                    videos_with_topics_df['id'].isin(video_ids_for_edge)\n",
    "                ]\n",
    "\n",
    "                # 3. Find the most common topic for this edge\n",
    "                if not edge_video_topics.empty:\n",
    "                    # Filter out outlier topic -1\n",
    "                    known_topics = edge_video_topics[edge_video_topics['topic_id'] != -1]\n",
    "                    if not known_topics.empty:\n",
    "                        top_topic_id = known_topics['topic_id'].mode()[0]\n",
    "\n",
    "                        # 4. Get the topic name from the trained model\n",
    "                        topic_words = topic_model.get_topic(top_topic_id)\n",
    "                        if topic_words:\n",
    "                            label = \", \".join([word for word, prob in topic_words[:3]])\n",
    "\n",
    "            edge_labels_viz[(u, v)] = label\n",
    "\n",
    "        # --- Plotting logic remains the same, but now uses the new accurate labels ---\n",
    "        plt.figure(figsize=(16, 12)); pos_viz = nx.spring_layout(G_viz, k=0.5, iterations=50, seed=42)\n",
    "        nx.draw_networkx_nodes(G_viz, pos_viz, node_size=node_sizes_viz, node_color='skyblue', alpha=0.8)\n",
    "        nx.draw_networkx_edges(G_viz, pos_viz, width=edge_widths_viz, alpha=0.4, edge_color='gray')\n",
    "        nx.draw_networkx_labels(G_viz, pos_viz, labels=node_labels_viz, font_size=9)\n",
    "        nx.draw_networkx_edge_labels(G_viz, pos_viz, edge_labels=edge_labels_viz, font_size=7, font_color='darkgreen',\n",
    "                                     bbox=dict(facecolor='white', alpha=0.4, edgecolor='none', boxstyle='round,pad=0.2'))\n",
    "\n",
    "        plt.title(f\"Collaboration Network Neighborhood for: {selected_channel_name_for_title}\", fontsize=16);\n",
    "        plt.axis('off');\n",
    "        plt.tight_layout();\n",
    "        plt.show()"
   ],
   "id": "468062d3efeafd49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 22: Search and Selection Logic (for interactive viz)\n",
    "def handle_search_click(b):\n",
    "    search_term = search_input.value.strip()\n",
    "    results_dropdown.options = [(\"---\", None)];\n",
    "    results_dropdown.value = None;\n",
    "    results_dropdown.disabled = True\n",
    "    with message_output:\n",
    "        message_output.clear_output()\n",
    "    with plot_output:\n",
    "        plot_output.clear_output()\n",
    "\n",
    "    if len(search_term) < 3:\n",
    "        with message_output: print(\"Please enter at least 3 characters to search.\")\n",
    "        return\n",
    "\n",
    "    search_pattern = f\"%{search_term.lower()}%\"\n",
    "    cursor = db_conn.cursor()\n",
    "    sql = \"SELECT id, login, display_name FROM Channels WHERE LOWER(login) LIKE ? OR LOWER(display_name) LIKE ? ORDER BY display_name ASC LIMIT 50\"\n",
    "    try:\n",
    "        cursor.execute(sql, (search_pattern, search_pattern));\n",
    "        matches = cursor.fetchall()\n",
    "    except sqlite3.Error as e:\n",
    "        with message_output:\n",
    "            print(f\"Database search error: {e}\"); return\n",
    "\n",
    "    if not matches:\n",
    "        with message_output:\n",
    "            print(f\"No channels found matching '{search_term}'.\")\n",
    "    elif len(matches) == 1:\n",
    "        match = matches[0];\n",
    "        channel_id = match['id']\n",
    "        with message_output:\n",
    "            print(f\"Found 1 match: {match['display_name']} ({match['login']}). Visualizing...\")\n",
    "        visualize_channel_neighborhood(channel_id)\n",
    "    else:\n",
    "        match_options = [(\"--- Select a Match ---\", None)] + [(f\"{row['display_name']} ({row['login']})\", row['id']) for\n",
    "                                                              row in matches]\n",
    "        results_dropdown.options = match_options;\n",
    "        results_dropdown.disabled = False\n",
    "        with message_output:\n",
    "            print(f\"Found {len(matches)} matches. Please select one from the dropdown below.\")\n",
    "\n",
    "\n",
    "def handle_match_selection(change):\n",
    "    selected_id = change.get('new')\n",
    "    if selected_id:\n",
    "        visualize_channel_neighborhood(selected_id)\n",
    "    else:\n",
    "        with plot_output:\n",
    "            plot_output.clear_output()\n",
    "\n",
    "\n",
    "search_button.on_click(handle_search_click)\n",
    "results_dropdown.observe(handle_match_selection, names='value')\n"
   ],
   "id": "3c41caac0554ba8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 23: Display Interactive Widgets\n",
    "print(\"\\n--- Displaying Interactive Network Visualization Widget ---\")\n",
    "controls = VBox([search_input, search_button, results_dropdown, message_output, plot_output],\n",
    "                layout=Layout(width='100%'))\n",
    "display(controls)\n"
   ],
   "id": "4d7e1aebd0860436",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 24: Final Closing Remarks & Cleanup\n",
    "print(f\"\\n--- Notebook Processing Finished at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "print(\n",
    "    \"You can re-run cells like 'Run Data Collection Cycle', 'Mention Processing Loop', or 'Run Channel Refresh Cycle' to gather more data.\")\n",
    "print(\"Consider closing the database connection manually if you are completely finished.\")\n",
    "# Example manual close (uncomment to run):\n",
    "# if 'db_conn' in locals() and db_conn is not None:\n",
    "#     try:\n",
    "#         db_conn.close()\n",
    "#         print(\"Database connection closed.\")\n",
    "#         db_conn = None # Clear variable\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error closing database connection: {e}\")"
   ],
   "id": "1ad0552c3886910e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
