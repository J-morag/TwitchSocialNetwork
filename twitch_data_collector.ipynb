{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Twitch Insight: Data Collector & Collaboration Network Analyzer (v3 - Enhanced Prints)\n",
    "\n",
    "This notebook automates the process of collecting data from the Twitch API, store it in a local SQLite database, and perform analysis, with a special focus on understanding collaboration networks between streamers. It identifies potential collaborations by detecting `@mentions` in video titles and descriptions, and also explores community structures within these networks. The primary interface for data collection, processing, and exploration is a Jupyter Notebook.\n",
    "\n",
    "**Key Features:**\n",
    "- Fetches top streams/categories periodically.\n",
    "- Fetches channel details and video archives.\n",
    "- Processes video titles/descriptions for `@mentions`.\n",
    "- Looks up mentioned users via API if not in the database.\n",
    "- Stores collaboration data (frequency, duration, recency).\n",
    "- Processes mentions atomically per-video using DB transactions.\n",
    "- Includes a refresh cycle for updating random channels.\n",
    "- Provides data exploration for channels, videos, and the collaboration network.\n",
    "\n",
    "**Modules Used:**\n",
    "- `config.py`: Configuration (API keys, constants). Requires `.env` file.\n",
    "- `database.py`: SQLite database interactions (schema, CRUD, upserts).\n",
    "- `twitch_api.py`: Twitch Helix API communication (auth, rate limits).\n",
    "- `network_utils.py`: Mention extraction and validation."
   ],
   "id": "5e517ca68d15737a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import logging\n",
    "import time\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import re  # For parsing duration\n",
    "import math  # For log scale checks\n",
    "import random  # For refresh cycle\n",
    "import networkx as nx  # For graph analysis\n",
    "from ipywidgets import Text, Button, Dropdown, Output, VBox, Layout  # For interactive viz\n",
    "from IPython.display import display, HTML  # For displaying widgets and potentially HTML\n",
    "\n",
    "# --- Local Notebook Configuration ---\n",
    "# Set to True for detailed progress and time estimations, False for minimal output.\n",
    "VERBOSE_MODE = True\n",
    "\n",
    "# Import custom modules\n",
    "try:\n",
    "    import config\n",
    "    import database\n",
    "    import twitch_api\n",
    "    import network_utils\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing custom modules: {e}\")\n",
    "    print(\"Please ensure config.py, database.py, twitch_api.py, and network_utils.py are in the same directory.\")\n",
    "    raise SystemExit(\"Stopping notebook due to missing modules.\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(module)s - %(message)s')\n",
    "\n",
    "# --- Print Configuration ---\n",
    "config.print_config()  # Call the function from config.py\n",
    "print(f\"\\n[Notebook Settings]\\n  VERBOSE_MODE: {VERBOSE_MODE}\")\n",
    "\n",
    "# --- Initialize ---\n",
    "db_conn = None\n",
    "api_client = None\n",
    "try:\n",
    "    print(\"\\nInitializing database connection...\")\n",
    "    # Use settings from config.py\n",
    "    db_conn = database.get_db_connection(config.DATABASE_NAME)\n",
    "    database.initialize_database(db_conn)\n",
    "    print(f\"Database '{config.DATABASE_NAME}' initialized.\")\n",
    "\n",
    "    print(\"Initializing Twitch API client...\")\n",
    "    api_client = twitch_api.TwitchAPIClient(\n",
    "        client_id=config.TWITCH_CLIENT_ID,\n",
    "        client_secret=config.TWITCH_CLIENT_SECRET,\n",
    "        auth_url=config.TWITCH_AUTH_URL,\n",
    "        base_url=config.TWITCH_API_BASE_URL\n",
    "    )\n",
    "    # Trigger authentication early to check credentials\n",
    "    if not api_client._authenticate():  # Use internal method carefully or add a public check method\n",
    "        raise ConnectionError(\"Failed to authenticate with Twitch API.\")\n",
    "    print(\"API Client initialized and authenticated.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.critical(f\"Initialization failed: {e}\", exc_info=True)\n",
    "    print(f\"Initialization failed: {e}\")\n",
    "    if db_conn:\n",
    "        db_conn.close()\n",
    "    raise SystemExit(\"Stopping notebook due to initialization failure.\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Setup Complete. Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"-\" * 30)\n",
    "\n"
   ],
   "id": "e5ca9c4d759b9375",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 2: Helper Functions\n",
    "\n",
    "def parse_duration_for_collab(duration_str):\n",
    "    \"\"\"\n",
    "    Parses a Twitch duration string (e.g., \"1h30m20s\") into total seconds.\n",
    "    \"\"\"\n",
    "    if not duration_str or not isinstance(duration_str, str):\n",
    "        return 0 # Return 0 for invalid input\n",
    "\n",
    "    total_seconds = 0\n",
    "    hours = re.search(r'(\\d+)h', duration_str)\n",
    "    minutes = re.search(r'(\\d+)m', duration_str)\n",
    "    seconds = re.search(r'(\\d+)s', duration_str)\n",
    "\n",
    "    if hours:\n",
    "        total_seconds += int(hours.group(1)) * 3600\n",
    "    if minutes:\n",
    "        total_seconds += int(minutes.group(1)) * 60\n",
    "    if seconds:\n",
    "        total_seconds += int(seconds.group(1))\n",
    "\n",
    "    return total_seconds\n",
    "\n",
    "def format_seconds_to_hm(seconds):\n",
    "    \"\"\"\n",
    "    Formats a duration in seconds into a human-readable \"Xh Ym\" string.\n",
    "    \"\"\"\n",
    "    if pd.isna(seconds) or seconds < 0:\n",
    "        return \"N/A\"\n",
    "\n",
    "    seconds = int(seconds)\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "\n",
    "    return f\"{hours}h {minutes:02}m\"\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ],
   "id": "d87a6db4e65288e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Collection Seeding\n",
    "\n",
    "This section is responsible for seeding the database with channels and their videos by fetching top streams and channels. It can be run to initialize an empty database, or to discover new channels that are not a part of the social network of any known channel.\n"
   ],
   "id": "c18bda92add56995"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 2: Data Collection Cycle Function & Execution (Top Streams Focus)\n",
    "def run_collection_cycle(current_api_client, current_db_conn):\n",
    "    \"\"\"\n",
    "    Runs one cycle focused on fetching top streams, channels, and their videos.\n",
    "    Mention processing is handled separately.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Starting Top Stream Data Collection Cycle at {datetime.now().strftime('%H:%M:%S')} ===\")\n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    # Phase 1: Fetch Top Categories\n",
    "    phase_start_time = time.time()\n",
    "    print(\"\\n--- Phase 1: Fetching and Processing Top Categories ---\")\n",
    "    top_categories = current_api_client.get_top_games(config.NUM_TOP_CATEGORIES)\n",
    "    if not top_categories: print(\"Could not fetch top categories. Cycle aborted.\"); return False\n",
    "    database.save_categories(current_db_conn, top_categories)\n",
    "    print(f\"Phase 1: Processed {len(top_categories)} top categories in {time.time() - phase_start_time:.2f}s.\")\n",
    "\n",
    "    # Phase 2: Fetch Top Streams & Identify Channels\n",
    "    phase_start_time = time.time()\n",
    "    print(\"\\n--- Phase 2: Fetching Top Streams & Identifying Channels from Categories ---\")\n",
    "    channels_to_process = set()\n",
    "    categories_to_scan = database.get_categories_to_scan(current_db_conn, config.NUM_TOP_CATEGORIES)\n",
    "    total_categories_to_scan = len(categories_to_scan)\n",
    "    print(f\"Found {total_categories_to_scan} categories prioritized for scanning.\")\n",
    "    category_processing_times = []\n",
    "\n",
    "    for i, category_row in enumerate(categories_to_scan):\n",
    "        cat_start_time = time.time()\n",
    "        category_id = category_row['id']\n",
    "        category_name = category_row['name']\n",
    "        print(f\" ({i + 1}/{total_categories_to_scan}) Processing category: {category_name}...\")\n",
    "        streams = current_api_client.get_streams_for_game(category_id, config.NUM_STREAMS_PER_CATEGORY)\n",
    "        if streams:\n",
    "            stream_channel_ids = set()\n",
    "            for stream in streams:\n",
    "                if 'user_id' in stream and 'user_login' in stream and 'user_name' in stream:\n",
    "                    if database.save_channel_basic(current_db_conn, {\n",
    "                        'id': stream['user_id'], 'login': stream['user_login'], 'display_name': stream['user_name']\n",
    "                    }): stream_channel_ids.add(stream['user_id'])\n",
    "            channels_to_process.update(stream_channel_ids)\n",
    "        database.update_category_scan_time(current_db_conn, category_row['id'])\n",
    "\n",
    "        cat_duration = time.time() - cat_start_time\n",
    "        category_processing_times.append(cat_duration)\n",
    "        if VERBOSE_MODE and category_processing_times and total_categories_to_scan > 0:\n",
    "            avg_time_per_cat = sum(category_processing_times) / len(category_processing_times)\n",
    "            est_remaining_cat_secs = (total_categories_to_scan - (i + 1)) * avg_time_per_cat\n",
    "            if est_remaining_cat_secs > 0 and i < total_categories_to_scan - 1:\n",
    "                est_cat_mins, est_cat_s = divmod(int(est_remaining_cat_secs), 60)\n",
    "                print(f\" -> Processed in {cat_duration:.2f}s. Est. remaining for categories: {est_cat_mins}m {est_cat_s}s\")\n",
    "            else:\n",
    "                print(f\" -> Processed in {cat_duration:.2f}s.\")\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    print(\n",
    "        f\"\\nPhase 2: Identified {len(channels_to_process)} unique channels. Took {time.time() - phase_start_time:.2f}s.\")\n",
    "\n",
    "    # Phase 3: Fetch/Update Channel Details (MODIFIED for one-by-one fetching)\n",
    "    phase_start_time = time.time()\n",
    "    print(\"\\n--- Phase 3: Fetching/Updating Channel Details (including Followers) ---\")\n",
    "    processed_channels_details = 0\n",
    "    channels_needing_details_update = [\n",
    "        chan_id for chan_id in list(channels_to_process)\n",
    "        if database.check_channel_needs_update(current_db_conn, chan_id, config.REFETCH_CHANNEL_DETAILS_DAYS)\n",
    "    ]\n",
    "    total_to_update = len(channels_needing_details_update)\n",
    "    print(f\"{total_to_update} channels require detail fetching/updating.\")\n",
    "    detail_fetch_times = []\n",
    "\n",
    "    if total_to_update > 0:\n",
    "        for i, channel_id in enumerate(channels_needing_details_update):\n",
    "            item_start_time = time.time()\n",
    "            print(f\" ({i + 1}/{total_to_update}) Fetching details for channel ID: {channel_id}...\")\n",
    "\n",
    "            # 1. Get user details (like login, description)\n",
    "            user_details_list = current_api_client.get_user_details(user_ids=[channel_id])\n",
    "\n",
    "            if user_details_list:\n",
    "                user_data = user_details_list[0] # Get the first (and only) result\n",
    "\n",
    "                # 2. Get follower count (separate API call)\n",
    "                follower_count = current_api_client.get_channel_follower_count(broadcaster_id=channel_id)\n",
    "                if follower_count is not None:\n",
    "                    user_data['follower_count'] = follower_count\n",
    "\n",
    "                # 3. Save combined data\n",
    "                try:\n",
    "                    database.save_channel_details(current_db_conn, user_data)\n",
    "                    processed_channels_details += 1\n",
    "                except Exception as e:\n",
    "                    print(f\" -> DB Error saving details for {user_data.get('login', channel_id)}: {e}\")\n",
    "\n",
    "            elif user_details_list is None:\n",
    "                print(f\" -> API call failed for details of channel {channel_id}. Skipping.\")\n",
    "\n",
    "            # Time estimation logic\n",
    "            item_duration = time.time() - item_start_time\n",
    "            detail_fetch_times.append(item_duration)\n",
    "            if VERBOSE_MODE and detail_fetch_times:\n",
    "                avg_time = sum(detail_fetch_times) / len(detail_fetch_times)\n",
    "                est_rem_secs = (total_to_update - (i + 1)) * avg_time\n",
    "                if est_rem_secs > 0:\n",
    "                    est_mins, est_s = divmod(int(est_rem_secs), 60)\n",
    "                    print(f\" -> Processed in {item_duration:.2f}s. Est. remaining: {est_mins}m {est_s}s\")\n",
    "\n",
    "            time.sleep(0.3) # API courtesy between channels\n",
    "\n",
    "    print(f\"Phase 3: Finished. Attempted save for {processed_channels_details} channels. Took {time.time() - phase_start_time:.2f}s.\")\n",
    "\n",
    "    # Phase 4: Fetch/Update Channel Videos\n",
    "    phase_start_time = time.time()\n",
    "    print(\"\\n--- Phase 4: Checking for and Fetching New Videos ---\")\n",
    "    channels_for_video_fetch = list(channels_to_process)\n",
    "    total_channels_for_video = len(channels_for_video_fetch)\n",
    "    print(f\"Checking for new videos for {total_channels_for_video} channels from this cycle...\")\n",
    "    processed_channels_videos = 0; new_videos_found_total = 0\n",
    "    video_fetch_times = []\n",
    "\n",
    "    if total_channels_for_video > 0:\n",
    "        for i, channel_id in enumerate(channels_for_video_fetch):\n",
    "            ch_video_start_time = time.time()\n",
    "            channel_info_for_log_cursor = current_db_conn.execute(\"SELECT login FROM Channels WHERE id = ?\", (channel_id,))\n",
    "            channel_info_for_log = channel_info_for_log_cursor.fetchone()\n",
    "            channel_log_name = channel_info_for_log['login'] if channel_info_for_log else channel_id\n",
    "\n",
    "            print(f\" ({i + 1}/{total_channels_for_video}) Checking videos for channel: {channel_log_name}...\")\n",
    "            latest_stored_date = database.get_latest_video_date_for_channel(current_db_conn, channel_id)\n",
    "            # This call fetches videos for channels just found in top streams\n",
    "            new_videos = current_api_client.get_channel_videos(\n",
    "                channel_id,\n",
    "                video_type='archive',\n",
    "                # Limit how many new videos to grab for a newly seen channel.\n",
    "                # Helps prevent a single new channel from dominating the cycle time.\n",
    "                limit=100,\n",
    "                after_date=latest_stored_date\n",
    "            )\n",
    "\n",
    "            if new_videos:\n",
    "                if VERBOSE_MODE: print(f\" -> Found {len(new_videos)} new archive videos.\")\n",
    "                database.save_videos(current_db_conn, new_videos); new_videos_found_total += len(new_videos)\n",
    "            elif new_videos is None:\n",
    "                print(f\" -> API call failed fetching videos for {channel_log_name}.\")\n",
    "\n",
    "            if new_videos is not None: database.update_channel_video_fetch_time(current_db_conn, channel_id)\n",
    "            processed_channels_videos += 1\n",
    "\n",
    "            ch_video_duration = time.time() - ch_video_start_time\n",
    "            video_fetch_times.append(ch_video_duration)\n",
    "            if VERBOSE_MODE and video_fetch_times:\n",
    "                avg_time_per_ch_video = sum(video_fetch_times) / len(video_fetch_times)\n",
    "                est_remaining_vid_secs = (total_channels_for_video - (i + 1)) * avg_time_per_ch_video\n",
    "                if est_remaining_vid_secs > 0 and i < total_channels_for_video - 1:\n",
    "                    est_vid_mins, est_vid_s = divmod(int(est_remaining_vid_secs), 60)\n",
    "                    print(f\" -> Processed in {ch_video_duration:.2f}s. Est. remaining for video checks: {est_vid_mins}m {est_vid_s}s\")\n",
    "                else:\n",
    "                    print(f\" -> Processed in {ch_video_duration:.2f}s.\")\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    print(f\"\\nPhase 4: Finished. Checked {processed_channels_videos} channels, found {new_videos_found_total} new videos. Took {time.time() - phase_start_time:.2f}s.\")\n",
    "    print(f\"=== Top Stream Data Collection Cycle Finished in {time.time() - overall_start_time:.2f}s ===\")\n",
    "    return True\n",
    "\n",
    "# Run the collection cycle (you can comment this out after initial runs or run it selectively)\n",
    "print(f\"\\n--- Executing Data Collection Cycle (Top Streams) at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "run_collection_cycle(api_client, db_conn)\n",
    "print(\"-\" * 30)\n",
    "print(\"Data Collection Cycle (Top Streams) Done for this run.\")\n",
    "print(\"-\" * 30)"
   ],
   "id": "15d635ce2a507458",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Mention Processing\n",
    "\n",
    "This section processes videos to extract mentions (indicating collaborations) from their titles. New channels discovered through this process will be registered in the database, but we will not yet fetch these new channels' videos. Run this repeatedly until all videos in the database have been processed.\n"
   ],
   "id": "5d8740c021271ab1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 3: Mention Processing Function (Atomic Per-Video)\n",
    "\n",
    "# Helper function to parse duration\n",
    "def parse_duration_for_collab(duration_str):\n",
    "    \"\"\"\n",
    "    Parses a Twitch duration string (e.g., \"1h30m20s\") into total seconds.\n",
    "    \"\"\"\n",
    "    if not duration_str or not isinstance(duration_str, str):\n",
    "        return 0 # Return 0 for invalid input\n",
    "\n",
    "    total_seconds = 0\n",
    "    hours = re.search(r'(\\d+)h', duration_str)\n",
    "    minutes = re.search(r'(\\d+)m', duration_str)\n",
    "    seconds = re.search(r'(\\d+)s', duration_str)\n",
    "\n",
    "    if hours:\n",
    "        total_seconds += int(hours.group(1)) * 3600\n",
    "    if minutes:\n",
    "        total_seconds += int(minutes.group(1)) * 60\n",
    "    if seconds:\n",
    "        total_seconds += int(seconds.group(1))\n",
    "\n",
    "    return total_seconds\n",
    "\n",
    "\n",
    "def process_video_mentions_batch(video_batch, current_api_client, current_db_conn):\n",
    "    \"\"\"\n",
    "    Processes a batch of videos to find mentions, update collaborations.\n",
    "    Discovers new channels via API for unknown mentions.\n",
    "    Attempts atomic processing per video using DB transactions.\n",
    "    \"\"\"\n",
    "    func_start_time = time.time()\n",
    "    processed_count_in_batch = 0;\n",
    "    newly_found_channels_in_batch = 0;\n",
    "    updated_edges_in_batch = 0;\n",
    "    all_unknown_logins_in_batch = set();\n",
    "    temp_video_data = {}\n",
    "\n",
    "    if VERBOSE_MODE: print(f\"  Batch Start: {len(video_batch)} videos to process.\")\n",
    "\n",
    "    # Pass 1: Extract mentions and identify all unique unknown logins for the batch\n",
    "    for video_row in video_batch:\n",
    "         video_id = video_row['id']; title = video_row['title'] or ''; desc = video_row['description'] or ''\n",
    "         text_to_scan = f\"{title} {desc}\"\n",
    "         mentioned_logins = network_utils.extract_mentions(text_to_scan)\n",
    "         temp_video_data[video_id] = {\n",
    "             'owner_id': video_row['channel_id'], 'mentions': mentioned_logins,\n",
    "             'published_at': video_row['published_at'], 'duration': video_row['duration']\n",
    "         }\n",
    "         if mentioned_logins:\n",
    "             try:\n",
    "                 _, not_found_now = network_utils.find_mentioned_channel_ids(mentioned_logins, current_db_conn);\n",
    "                 all_unknown_logins_in_batch.update(not_found_now)\n",
    "             except Exception as e:\n",
    "                 logging.error(f\"Error checking mentions in DB during Pass 1 for video {video_id}: {e}\")\n",
    "\n",
    "    # Pass 2: Fetch details for unknown mentioned logins via batched API calls\n",
    "    newly_discovered_ids_this_pass = {}\n",
    "    if all_unknown_logins_in_batch:\n",
    "        unknown_logins_list = list(all_unknown_logins_in_batch)\n",
    "        if VERBOSE_MODE: print(f\"  Pass 2: Checking {len(unknown_logins_list)} unique unknown logins via API...\")\n",
    "        num_api_batches = (len(unknown_logins_list) + 99) // 100\n",
    "        for i in range(num_api_batches):\n",
    "            batch_logins = unknown_logins_list[i * 100:(i + 1) * 100]\n",
    "            if VERBOSE_MODE: print(f\"   -> API Batch {i + 1}/{num_api_batches} for {len(batch_logins)} logins...\")\n",
    "            time.sleep(0.2)\n",
    "            user_details_list = current_api_client.get_user_details(user_logins=batch_logins);\n",
    "            api_call_succeeded = user_details_list is not None\n",
    "            if api_call_succeeded and user_details_list:\n",
    "                for user_data in user_details_list:\n",
    "                     try:\n",
    "                         database.save_channel_details(current_db_conn, user_data);\n",
    "                         login_lower = user_data['login'].lower();\n",
    "                         user_id = user_data['id']\n",
    "                         newly_discovered_ids_this_pass[login_lower] = user_id;\n",
    "                         newly_found_channels_in_batch += 1\n",
    "                     except Exception as e:\n",
    "                         print(f\"Error saving newly discovered channel {user_data.get('login')}: {e}\")\n",
    "            time.sleep(0.1)\n",
    "        if VERBOSE_MODE: print(f\"  Pass 2 Complete. Discovered and saved {newly_found_channels_in_batch} new channels.\")\n",
    "\n",
    "    # Pass 3: Process each video within its own database transaction\n",
    "    for idx, (video_id, video_data) in enumerate(temp_video_data.items()):\n",
    "        channel_id_A = video_data['owner_id'];\n",
    "        mentioned_logins = video_data['mentions']\n",
    "        published_at = video_data['published_at'];\n",
    "        duration_str = video_data['duration']\n",
    "        is_processed_successfully_this_video = False\n",
    "\n",
    "        try:\n",
    "            published_at_dt = published_at\n",
    "            if not isinstance(published_at_dt, datetime):\n",
    "                published_at_dt = pd.to_datetime(published_at, errors='coerce', utc=True)\n",
    "\n",
    "            if pd.isna(published_at_dt):\n",
    "                logging.warning(f\"Invalid timestamp for video {video_id}. Skipping edges. Marking processed.\")\n",
    "                current_db_conn.execute('BEGIN')\n",
    "                database.mark_video_mentions_processed(current_db_conn, video_id)\n",
    "                current_db_conn.commit()\n",
    "                processed_count_in_batch += 1\n",
    "                is_processed_successfully_this_video = True\n",
    "\n",
    "            elif not mentioned_logins:\n",
    "                current_db_conn.execute('BEGIN')\n",
    "                database.mark_video_mentions_processed(current_db_conn, video_id)\n",
    "                current_db_conn.commit()\n",
    "                processed_count_in_batch += 1\n",
    "                is_processed_successfully_this_video = True\n",
    "            else:\n",
    "                current_known_ids, _ = network_utils.find_mentioned_channel_ids(mentioned_logins, current_db_conn)\n",
    "                for login, user_id_new in newly_discovered_ids_this_pass.items():\n",
    "                     if login in mentioned_logins:\n",
    "                         current_known_ids[login] = user_id_new\n",
    "\n",
    "                duration_sec = parse_duration_for_collab(duration_str)\n",
    "                edges_for_this_video = 0\n",
    "                mentions_to_add_list = []\n",
    "\n",
    "                current_db_conn.execute('BEGIN IMMEDIATE')\n",
    "                for login, channel_id_B in current_known_ids.items():\n",
    "                    if channel_id_A != channel_id_B:\n",
    "                        database.upsert_collaboration_edge(current_db_conn, channel_id_A, channel_id_B, published_at_dt, duration_sec)\n",
    "                        mentions_to_add_list.append((channel_id_A, channel_id_B, video_id, published_at_dt))\n",
    "                        edges_for_this_video += 1\n",
    "\n",
    "                if mentions_to_add_list:\n",
    "                    database.add_mentions(current_db_conn, mentions_to_add_list)\n",
    "\n",
    "                database.mark_video_mentions_processed(current_db_conn, video_id)\n",
    "                current_db_conn.commit()\n",
    "\n",
    "                processed_count_in_batch += 1\n",
    "                updated_edges_in_batch += edges_for_this_video\n",
    "                is_processed_successfully_this_video = True\n",
    "\n",
    "        except Exception as e:\n",
    "            if not is_processed_successfully_this_video:\n",
    "                 try:\n",
    "                     current_db_conn.rollback()\n",
    "                 except sqlite3.Error as rb_err:\n",
    "                     logging.error(f\"Error during rollback for video {video_id}: {rb_err}\")\n",
    "                 logging.error(f\"Error processing video {video_id} within transaction\", exc_info=True)\n",
    "\n",
    "    return processed_count_in_batch, newly_found_channels_in_batch, updated_edges_in_batch"
   ],
   "id": "975b454ea1069823",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 4: Mention Processing Loop\n",
    "print(f\"\\n--- Starting Mention Processing Phase at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "BATCH_SIZE = config.MENTION_PROC_BATCH_SIZE\n",
    "MAX_BATCHES_PER_RUN = config.MENTION_PROC_MAX_BATCHES\n",
    "total_videos_processed_run = 0; total_new_channels_run = 0\n",
    "total_edges_updated_run = 0; # Removed context counter\n",
    "batches_processed_run = 0\n",
    "mention_loop_start_time = time.time()\n",
    "batch_processing_times_mention_loop = []\n",
    "\n",
    "# Indication of total unprocessed videos at the START\n",
    "try:\n",
    "    unproc_cursor = db_conn.cursor()\n",
    "    unproc_cursor.execute(\"SELECT COUNT(*) FROM Videos WHERE mentions_processed_at IS NULL\")\n",
    "    initial_total_unprocessed_videos = unproc_cursor.fetchone()[0]\n",
    "    print(f\"Estimated total unprocessed videos at start of run: {initial_total_unprocessed_videos}\")\n",
    "    if initial_total_unprocessed_videos > 0:\n",
    "        initial_total_expected_batches = (initial_total_unprocessed_videos + BATCH_SIZE -1) // BATCH_SIZE\n",
    "        print(f\"Expecting around {initial_total_expected_batches} batches in total (this run will process up to {MAX_BATCHES_PER_RUN}).\")\n",
    "except sqlite3.Error as e_count:\n",
    "    print(f\"Could not get initial count of unprocessed videos: {e_count}\")\n",
    "    initial_total_expected_batches = MAX_BATCHES_PER_RUN\n",
    "\n",
    "while batches_processed_run < MAX_BATCHES_PER_RUN:\n",
    "    batch_loop_start_time = time.time()\n",
    "    print(f\"\\nFetching mention processing batch {batches_processed_run + 1}/{MAX_BATCHES_PER_RUN} (Batch size: {BATCH_SIZE})...\")\n",
    "    videos_to_process = database.get_unprocessed_videos_batch(db_conn, BATCH_SIZE)\n",
    "\n",
    "    if not videos_to_process: print(\"No more videos found needing mention processing.\"); break\n",
    "\n",
    "    print(f\"Processing mentions for {len(videos_to_process)} videos...\")\n",
    "    try:\n",
    "        count, new_chans, edges = process_video_mentions_batch(videos_to_process, api_client, db_conn)\n",
    "\n",
    "        total_videos_processed_run += count\n",
    "        total_new_channels_run += new_chans\n",
    "        total_edges_updated_run += edges\n",
    "        batches_processed_run += 1\n",
    "        batch_duration = time.time() - batch_loop_start_time\n",
    "        batch_processing_times_mention_loop.append(batch_duration)\n",
    "\n",
    "        print(f\"Batch {batches_processed_run} complete in {batch_duration:.2f}s. Processed: {count} videos, Found: {new_chans} new channels, Upserted: {edges} edges.\")\n",
    "\n",
    "        if VERBOSE_MODE and batches_processed_run < MAX_BATCHES_PER_RUN and batch_processing_times_mention_loop and len(videos_to_process) == BATCH_SIZE:\n",
    "            avg_time_per_batch = sum(batch_processing_times_mention_loop) / len(batch_processing_times_mention_loop)\n",
    "            remaining_batches_in_run = MAX_BATCHES_PER_RUN - batches_processed_run\n",
    "            batches_for_eta = remaining_batches_in_run\n",
    "            if 'initial_total_expected_batches' in locals() and initial_total_expected_batches > batches_processed_run:\n",
    "                 batches_for_eta = min(remaining_batches_in_run, initial_total_expected_batches - batches_processed_run)\n",
    "\n",
    "            est_remaining_run_secs = batches_for_eta * avg_time_per_batch\n",
    "            if est_remaining_run_secs > 0:\n",
    "                est_run_mins, est_run_s = divmod(int(est_remaining_run_secs), 60)\n",
    "                print(f\"Est. time remaining for *this run* (up to {MAX_BATCHES_PER_RUN} batches, or fewer if DB empties): {est_run_mins}m {est_run_s}s\")\n",
    "\n",
    "        time.sleep(max(0.2, 1.0 - (0.05 * count / BATCH_SIZE if BATCH_SIZE > 0 else 1)))\n",
    "\n",
    "    except Exception as e: print(f\"Error during mention processing batch: {e}\"); logging.error(\"Error in mention processing loop\", exc_info=True); print(\"Stopping mention processing due to error.\"); break\n",
    "\n",
    "# UPDATED: Removed context from final summary\n",
    "print(f\"\\n--- Mention Processing Phase Finished (for this run) in {time.time() - mention_loop_start_time:.2f}s ---\")\n",
    "print(f\"Total videos marked as processed in this run: {total_videos_processed_run}\")\n",
    "print(f\"Total new channels discovered via mentions in this run: {total_new_channels_run}\")\n",
    "print(f\"Total collaboration edge instances upserted in this run: {total_edges_updated_run}\")\n",
    "\n",
    "try:\n",
    "    final_unproc_cursor = db_conn.cursor()\n",
    "    final_unproc_cursor.execute(\"SELECT COUNT(*) FROM Videos WHERE mentions_processed_at IS NULL\")\n",
    "    final_unprocessed_videos = final_unproc_cursor.fetchone()[0]\n",
    "    print(f\"\\nVideos remaining to be processed in database: {final_unprocessed_videos}\")\n",
    "except sqlite3.Error as e_count_end:\n",
    "    print(f\"Could not get final count of unprocessed videos: {e_count_end}\")\n",
    "\n",
    "print(\"-\" * 30)"
   ],
   "id": "9b8a33530f48500e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Refresh Cycle\n",
    "\n",
    "This section periodically updates details and recent videos for a prioritized subset of channels in the database. It focuses on channels that have not been updated recently (\"stalest\" channels), ensuring the dataset remains current without re-fetching all channels every cycle. The refresh cycle:\n",
    "\n",
    "* Selects a configurable number of the stalest channels based on last update timestamps.\n",
    "* Updates each channel's details (including follower count).\n",
    "* Fetches and stores new videos for each refreshed channel.\n",
    "* Helps maintain up-to-date collaboration and content data for ongoing analysis.\n",
    "\n",
    "Run this cycle regularly to keep your database fresh and to discover new collaborations as channels continue to stream. Run it repeatedly until refreshes do not produce significant updates, at which point you can go back to the Mention Processing section."
   ],
   "id": "89a825b0f8352143"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 5: Refresh Function\n",
    "\n",
    "def run_refresh_cycle(current_api_client, current_db_conn, num_channels_to_refresh):\n",
    "    \"\"\"\n",
    "    Refreshes details and fetches recent videos for a prioritized subset of\n",
    "    the \"stalest\" channels (those not updated in the longest time). Includes follower count.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Prioritized Channel Refresh Cycle at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "    overall_refresh_start_time = time.time()\n",
    "    processed_count = 0; new_videos_found_total = 0\n",
    "    channel_refresh_times = []\n",
    "\n",
    "    try:\n",
    "        # Get prioritized list instead of random\n",
    "        print(f\"Fetching up to {num_channels_to_refresh} of the stalest channels from the database...\")\n",
    "        ids_to_refresh = database.get_stale_channels_for_refresh(current_db_conn, num_channels_to_refresh)\n",
    "        actual_to_refresh_count = len(ids_to_refresh)\n",
    "\n",
    "        if not ids_to_refresh:\n",
    "            print(\"No channels found to refresh.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Prioritized list created. Refreshing {actual_to_refresh_count} channels...\")\n",
    "\n",
    "        for i, channel_row in enumerate(ids_to_refresh):\n",
    "            channel_refresh_start_time = time.time()\n",
    "            time.sleep(0.4) # API courtesy\n",
    "            channel_id = channel_row['id']\n",
    "            channel_log_name = channel_row['login']\n",
    "\n",
    "            print(f\"\\n ({i + 1}/{actual_to_refresh_count}) Refreshing channel: {channel_log_name}...\")\n",
    "\n",
    "            # 1. Refresh Channel Details (one-by-one)\n",
    "            details_list = current_api_client.get_user_details(user_ids=[channel_id])\n",
    "\n",
    "            if details_list:\n",
    "                user_data = details_list[0]\n",
    "                # Get follower count\n",
    "                follower_count = current_api_client.get_channel_follower_count(broadcaster_id=channel_id)\n",
    "                if follower_count is not None:\n",
    "                    user_data['follower_count'] = follower_count\n",
    "\n",
    "                try:\n",
    "                    database.save_channel_details(current_db_conn, user_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"  -> DB Error saving details for channel {channel_id}: {e}\")\n",
    "\n",
    "            elif details_list is None:\n",
    "                print(f\"  -> API call failed fetching details for channel {channel_id}. Skipping.\")\n",
    "                continue # Skip this channel if we can't get its details\n",
    "\n",
    "            # 2. Fetch New Videos\n",
    "            if details_list is not None:\n",
    "                latest_stored_date = database.get_latest_video_date_for_channel(current_db_conn, channel_id)\n",
    "\n",
    "                new_videos = current_api_client.get_channel_videos(\n",
    "                    channel_id,\n",
    "                    video_type='archive',\n",
    "                    # This limit prevents a single channel from dominating the refresh cycle.\n",
    "                    # The database grows incrementally over multiple runs.\n",
    "                    limit=100,\n",
    "                    after_date=latest_stored_date\n",
    "                )\n",
    "\n",
    "                if new_videos:\n",
    "                    if VERBOSE_MODE: print(f\" -> Found {len(new_videos)} new archive videos.\")\n",
    "                    database.save_videos(current_db_conn, new_videos)\n",
    "                    new_videos_found_total += len(new_videos)\n",
    "                elif new_videos is None:\n",
    "                    print(f\" -> API call failed fetching videos for {channel_log_name}.\")\n",
    "\n",
    "                if new_videos is not None:\n",
    "                    database.update_channel_video_fetch_time(current_db_conn, channel_id)\n",
    "\n",
    "            processed_count += 1\n",
    "\n",
    "            # Time estimation logic\n",
    "            channel_refresh_duration = time.time() - channel_refresh_start_time\n",
    "            channel_refresh_times.append(channel_refresh_duration)\n",
    "            if VERBOSE_MODE and channel_refresh_times and actual_to_refresh_count > 0:\n",
    "                avg_time_per_refresh = sum(channel_refresh_times) / len(channel_refresh_times)\n",
    "                remaining_refreshes = actual_to_refresh_count - (i + 1)\n",
    "                est_remaining_refresh_secs = remaining_refreshes * avg_time_per_refresh\n",
    "                if est_remaining_refresh_secs > 0 and i < actual_to_refresh_count - 1:\n",
    "                    est_ref_mins, est_ref_s = divmod(int(est_remaining_refresh_secs), 60)\n",
    "                    print(f\" -> Refreshed in {channel_refresh_duration:.2f}s. Est. time remaining for refresh cycle: {est_ref_mins}m {est_ref_s}s\")\n",
    "                else:\n",
    "                    print(f\" -> Refreshed in {channel_refresh_duration:.2f}s.\")\n",
    "\n",
    "        print(f\"\\n--- Channel Refresh Cycle Finished in {time.time() - overall_refresh_start_time:.2f}s ---\")\n",
    "        print(f\"Attempted refresh for {processed_count} channels.\")\n",
    "        print(f\"Found {new_videos_found_total} new videos in total during refresh.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during refresh cycle: {e}\")\n",
    "        logging.error(\"Error in refresh cycle\", exc_info=True)"
   ],
   "id": "5bc8ba422b2e5eb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 6: Run Channel Refresh Cycle\n",
    "print(f\"\\n--- Executing Channel Refresh Cycle at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "run_refresh_cycle(api_client, db_conn, num_channels_to_refresh=config.REFRESH_CYCLE_CHANNELS)\n",
    "print(\"-\" * 30)\n",
    "print(\"Channel Refresh Phase Done (for this run).\")\n",
    "print(\"-\" * 30)\n"
   ],
   "id": "924394808b835266",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Exploration\n",
    "\n",
    "Load the collected data from the database into pandas DataFrames and perform basic analysis and visualization.\n"
   ],
   "id": "27a245d7daeec98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 7: Load Data from Database\n",
    "print(f\"\\n--- Loading Data for Exploration at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "try:\n",
    "    channel_date_cols = ['created_at', 'first_seen', 'last_fetched_details', 'last_fetched_videos']\n",
    "    video_date_cols = ['published_at', 'created_at_api', 'fetched_at', 'mentions_processed_at']\n",
    "    category_date_cols = ['last_scanned_top_streams']\n",
    "\n",
    "    print(\"Loading Channels table...\")\n",
    "    channels_df = pd.read_sql_query(\"SELECT * FROM Channels\", db_conn)\n",
    "    print(\"Loading Videos table...\")\n",
    "    videos_df = pd.read_sql_query(\"SELECT * FROM Videos\", db_conn)\n",
    "    print(\"Loading Categories table...\")\n",
    "    categories_df = pd.read_sql_query(\"SELECT * FROM Categories\", db_conn)\n",
    "\n",
    "    # Convert timestamp columns\n",
    "    print(\"Converting timestamp columns...\")\n",
    "    for col in channel_date_cols:\n",
    "        if col in channels_df.columns: channels_df[col] = pd.to_datetime(channels_df[col], errors='coerce', utc=True)\n",
    "    for col in video_date_cols:\n",
    "        if col in videos_df.columns: videos_df[col] = pd.to_datetime(videos_df[col], errors='coerce', utc=True)\n",
    "    for col in category_date_cols:\n",
    "         if col in categories_df.columns: categories_df[col] = pd.to_datetime(categories_df[col], errors='coerce', utc=True)\n",
    "\n",
    "    # Convert numeric columns, coercing errors\n",
    "    print(\"Converting numeric columns and parsing durations...\")\n",
    "    channels_df['view_count'] = pd.to_numeric(channels_df['view_count'], errors='coerce')\n",
    "    channels_df['follower_count'] = pd.to_numeric(channels_df['follower_count'], errors='coerce') # Add follower_count\n",
    "    channels_df['display_name'] = channels_df['display_name'].fillna(channels_df['login'])\n",
    "    videos_df['view_count'] = pd.to_numeric(videos_df['view_count'], errors='coerce')\n",
    "    videos_df['duration_seconds'] = videos_df['duration'].apply(parse_duration_for_collab).fillna(0).astype(int)\n",
    "\n",
    "    print(f\"Loaded {len(channels_df)} channels, {len(videos_df)} videos, and {len(categories_df)} categories.\")\n",
    "    print(\"--- Data Loading Complete ---\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from database: {e}\")\n",
    "    raise SystemExit(\"Stopping notebook due to data loading failure.\")"
   ],
   "id": "889d3742ca50c522",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 8: Display Sample Data\n",
    "print(\"\\nSample Channels Data:\")\n",
    "display(channels_df.head())\n",
    "print(\"\\nSample Videos Data:\")\n",
    "display(\n",
    "    videos_df[['id', 'channel_id', 'title', 'published_at', 'view_count', 'duration_seconds', 'mentions_processed_at',\n",
    "               'game_name']].head())\n",
    "print(\"\\nSample Categories Data:\")\n",
    "display(categories_df.head())\n"
   ],
   "id": "8246499271cdb108",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 9: Basic Statistics\n",
    "print(\"\\n--- Basic Statistics ---\")\n",
    "print(\"\\nChannel Statistics:\")\n",
    "print(f\"Total Channels: {len(channels_df)}\")\n",
    "print(\"\\nBroadcaster Types:\")\n",
    "print(channels_df['broadcaster_type'].value_counts(dropna=False))\n",
    "print(\"\\nChannel Follower Count Summary:\")\n",
    "print(channels_df['follower_count'].describe())\n",
    "print(f\"Channels missing details: {channels_df['last_fetched_details'].isnull().sum()}\")\n",
    "\n",
    "print(\"\\nVideo Statistics:\")\n",
    "print(f\"Total Videos: {len(videos_df)}\")\n",
    "print(\"\\nVideo Types:\")\n",
    "print(videos_df['type'].value_counts(dropna=False))\n",
    "print(\"\\nVideo View Count Summary:\")\n",
    "print(videos_df['view_count'].describe())\n",
    "print(\"\\nVideo Duration (seconds) Summary:\")\n",
    "print(videos_df['duration_seconds'].describe())\n",
    "print(f\"Videos with mentions processed: {videos_df['mentions_processed_at'].notnull().sum()}\")\n",
    "print(f\"Videos pending mention processing: {videos_df['mentions_processed_at'].isnull().sum()}\")"
   ],
   "id": "6416a94fbeb6693c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Visualization (Channels & Videos)\n",
   "id": "7aa877841300cbbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 10: Visualizations - Channels\n",
    "print(\"\\n--- Channel Visualizations ---\")\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# --- UPDATED: Histogram of Channel Follower Counts ---\n",
    "print(\"Generating Channel Follower Count Histogram...\")\n",
    "plt.figure(figsize=(10, 5));\n",
    "# Use the 'follower_count' column now, and drop any rows where it might be NaN\n",
    "followers_positive = channels_df.dropna(subset=['follower_count'])\n",
    "followers_positive = followers_positive[followers_positive['follower_count'] > 0]['follower_count']\n",
    "\n",
    "if not followers_positive.empty:\n",
    "    # Determine if log scale is needed by checking the data range\n",
    "    should_use_log = (followers_positive.max() / followers_positive.min() > 100) if followers_positive.min() > 0 else False\n",
    "\n",
    "    # Conditionally set the plotting arguments\n",
    "    plot_kwargs = {'bins': 40, 'kde': False}\n",
    "    if should_use_log:\n",
    "        plot_kwargs['log_scale'] = True # Set to True, not a variable that could be False\n",
    "        title = 'Distribution of Channel Follower Counts (Log Scale)'\n",
    "        xlabel = 'Total Follower Count (Log Scale)'\n",
    "    else:\n",
    "        # Do not pass log_scale when it's not needed\n",
    "        title = 'Distribution of Channel Follower Counts'\n",
    "        xlabel = 'Total Follower Count'\n",
    "\n",
    "    sns.histplot(followers_positive, **plot_kwargs)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Number of Channels')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No positive channel follower data to plot histogram yet.\")\n",
    "\n",
    "print(\"Generating Broadcaster Types Bar Chart...\")\n",
    "plt.figure(figsize=(8, 5));\n",
    "channel_types = channels_df['broadcaster_type'].fillna('N/A').replace('', 'N/A')\n",
    "sns.countplot(y=channel_types, order=channel_types.value_counts().index, palette='viridis');\n",
    "plt.title('Channel Broadcaster Types')\n",
    "plt.xlabel('Number of Channels')\n",
    "plt.ylabel('Broadcaster Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "8fddecb20676a379",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 11: Visualizations - Videos\n",
    "print(\"\\n--- Video Visualizations ---\")\n",
    "\n",
    "print(\"Generating Video View Count Histogram...\")\n",
    "plt.figure(figsize=(10, 5));\n",
    "views_positive_vid = videos_df.dropna(subset=['view_count'])\n",
    "views_positive_vid = views_positive_vid[views_positive_vid['view_count'] > 0]['view_count']\n",
    "\n",
    "if not views_positive_vid.empty:\n",
    "    # Determine if log scale is needed\n",
    "    should_use_log_vid = (views_positive_vid.max() / views_positive_vid.min() > 100) if views_positive_vid.min() > 0 else False\n",
    "\n",
    "    # Conditionally set the plotting arguments\n",
    "    plot_kwargs_vid = {'bins': 40, 'kde': False}\n",
    "    if should_use_log_vid:\n",
    "        plot_kwargs_vid['log_scale'] = True\n",
    "        title_vid = 'Distribution of Video View Counts (Log Scale)'\n",
    "        xlabel_vid = 'Video View Count (Log Scale)'\n",
    "    else:\n",
    "        title_vid = 'Distribution of Video View Counts'\n",
    "        xlabel_vid = 'Video View Count'\n",
    "\n",
    "    sns.histplot(views_positive_vid, **plot_kwargs_vid)\n",
    "    plt.title(title_vid)\n",
    "    plt.xlabel(xlabel_vid)\n",
    "    plt.ylabel('Number of Videos')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No positive video view data to plot histogram.\")\n",
    "\n",
    "print(\"Generating Video Types Bar Chart...\")\n",
    "plt.figure(figsize=(8, 5));\n",
    "video_types = videos_df['type'].fillna('N/A');\n",
    "sns.countplot(y=video_types, order=video_types.value_counts().index, palette='magma');\n",
    "plt.title('Video Types');\n",
    "plt.xlabel('Number of Videos');\n",
    "plt.ylabel('Type');\n",
    "plt.tight_layout();\n",
    "plt.show()\n",
    "\n",
    "# --- Video Publication Time Series ---\n",
    "print(\"Generating Video Publication Time Series...\")\n",
    "plt.figure(figsize=(12, 6));\n",
    "video_pub_dates = videos_df.dropna(subset=['published_at'])\n",
    "\n",
    "if not video_pub_dates.empty and len(video_pub_dates) > 1:\n",
    "    # --- Calculate the 5th percentile date to set the start range ---\n",
    "    start_date = video_pub_dates['published_at'].quantile(0.05)\n",
    "    print(f\"Displaying time series from {start_date.strftime('%Y-%m-%d')} onwards (showing 95% of the data).\")\n",
    "\n",
    "    # Filter the DataFrame to this new date range\n",
    "    plotting_df = video_pub_dates[video_pub_dates['published_at'] >= start_date]\n",
    "\n",
    "    # --- Use the filtered plotting_df for the rest of the logic ---\n",
    "    if not plotting_df.empty:\n",
    "        time_range_days = (plotting_df['published_at'].max() - plotting_df['published_at'].min()).days if len(plotting_df) > 1 else 0\n",
    "        resample_freq = 'ME' if time_range_days > 90 else 'D' # Resample by Month or Day\n",
    "        plot_title = 'Number of Videos Published (' + ('Monthly' if resample_freq == 'ME' else 'Daily') + ')'\n",
    "\n",
    "        # Plot the resampled data\n",
    "        plotting_df.set_index('published_at')['id'].resample(resample_freq).count().plot(marker='.' if resample_freq == 'D' else 'o', linestyle='-');\n",
    "\n",
    "        plt.title(plot_title);\n",
    "        plt.ylabel('Number of Videos');\n",
    "        plt.xlabel('Publication Date');\n",
    "        plt.tight_layout();\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No video data remains after filtering by start date.\")\n",
    "else:\n",
    "    print(\"Not enough video publication date data to plot a meaningful time series.\")"
   ],
   "id": "9f967ce2b74eb455",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Note on \"Streaming Together\" Feature\n",
    "\n",
    "While Twitch has features like \"Squad Streams\" or \"Guest Star\" allowing multiple creators to stream simultaneously on one channel, the participant data for these features **does not appear to be reliably available** via the standard Twitch API for *past* streams or VODs (as of June 2025).\n",
    "\n",
    "Therefore, the collaboration detection in this notebook relies primarily on identifying `@mentions` within video titles and descriptions.\n"
   ],
   "id": "8d17c13ab5a5f533"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Collaboration Network Exploration (Filtered)\n",
    "\n",
    "Exploring the collaboration network. Data is filtered IN MEMORY based on\n",
    "thresholds in `config.py` BEFORE analysis to improve performance.\n",
    "The underlying database remains complete.\n"
   ],
   "id": "cd855c40a6302025"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 12: Load and Filter Data for Network Analysis\n",
    "print(f\"\\n--- Loading and Filtering Data for Network Analysis at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "channels_df_net = pd.DataFrame()  # For channels passing all filters\n",
    "collab_df_net = pd.DataFrame()   # For edges passing all filters\n",
    "G_filtered = nx.Graph()          # The filtered graph for analysis\n",
    "degree_df_filtered = pd.DataFrame() # For degree stats\n",
    "\n",
    "try:\n",
    "    # --- 1. Load base data ---\n",
    "    collab_df_full = pd.read_sql_query(\"SELECT * FROM Collaborations\", db_conn)\n",
    "    collab_df_full['collaboration_count'] = pd.to_numeric(collab_df_full['collaboration_count'], errors='coerce').fillna(0).astype(int)\n",
    "    collab_df_full['total_collaboration_duration_seconds'] = pd.to_numeric(collab_df_full['total_collaboration_duration_seconds'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "\n",
    "    # --- 2. Filter Channels ---\n",
    "    print(f\"Filtering channels by follower count (>= {config.NETWORK_MIN_FOLLOWER_COUNT})...\")\n",
    "    channels_df_filtered_fc = channels_df[channels_df['follower_count'] >= config.NETWORK_MIN_FOLLOWER_COUNT]\n",
    "    print(f\" -> Channels after follower count filter: {len(channels_df_filtered_fc)}\")\n",
    "\n",
    "    if not videos_df.empty:\n",
    "        print(f\"Filtering channels by video count (>= {config.NETWORK_MIN_CHANNEL_VIDEO_COUNT})...\")\n",
    "        video_counts_per_channel = videos_df['channel_id'].value_counts()\n",
    "        channels_with_enough_videos = video_counts_per_channel[video_counts_per_channel >= config.NETWORK_MIN_CHANNEL_VIDEO_COUNT].index.tolist()\n",
    "        channels_df_net = channels_df_filtered_fc[channels_df_filtered_fc['id'].isin(channels_with_enough_videos)]\n",
    "        print(f\" -> Channels after video count filter: {len(channels_df_net)}\")\n",
    "    else:\n",
    "        print(\"Warning: videos_df not available for filtering by video count. Using only follower count filter for channels.\")\n",
    "        channels_df_net = channels_df_filtered_fc\n",
    "\n",
    "    valid_channel_ids_for_network = set(channels_df_net['id'])\n",
    "    print(f\"Total channels passing node filters: {len(valid_channel_ids_for_network)}\")\n",
    "\n",
    "    # --- 3. Filter Collaborations (Edges) ---\n",
    "    if not collab_df_full.empty:\n",
    "        print(f\"Filtering collaboration edges by count (>= {config.NETWORK_MIN_COLLABORATION_COUNT})...\")\n",
    "        collab_df_filtered_count = collab_df_full[collab_df_full['collaboration_count'] >= config.NETWORK_MIN_COLLABORATION_COUNT]\n",
    "        print(f\" -> Edges after count filter: {len(collab_df_filtered_count)}\")\n",
    "\n",
    "        print(\"Filtering edges to ensure both connected channels passed node filters...\")\n",
    "        collab_df_net = collab_df_filtered_count[\n",
    "            collab_df_filtered_count['channel_id_1'].isin(valid_channel_ids_for_network) &\n",
    "            collab_df_filtered_count['channel_id_2'].isin(valid_channel_ids_for_network)\n",
    "        ].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "        print(f\" -> Edges after node validity filter: {len(collab_df_net)}\")\n",
    "\n",
    "        # --- Cap outlier durations before creating graph ---\n",
    "        duration_threshold_weeks = config.NETWORK_DURATION_OUTLIER_WEEKS\n",
    "        duration_threshold_seconds = duration_threshold_weeks * 7 * 24 * 3600\n",
    "\n",
    "        outlier_edges = collab_df_net['total_collaboration_duration_seconds'] > duration_threshold_seconds\n",
    "        outlier_count = outlier_edges.sum()\n",
    "\n",
    "        if outlier_count > 0:\n",
    "            # The f-string now uses the config variable directly for accurate reporting\n",
    "            print(f\"Capping duration for {outlier_count} edge(s) with duration > {duration_threshold_weeks} week(s) for network analysis.\")\n",
    "\n",
    "            # Use .loc to safely modify the DataFrame\n",
    "            collab_df_net.loc[outlier_edges, 'total_collaboration_duration_seconds'] = duration_threshold_seconds\n",
    "\n",
    "    else:\n",
    "        print(\"Full collaboration data (collab_df_full) is empty. Filtered collaboration data will be empty.\")\n",
    "        collab_df_net = pd.DataFrame()\n",
    "\n",
    "    # --- 4. Create Filtered NetworkX Graph ---\n",
    "    if not collab_df_net.empty:\n",
    "        print(\"Creating NetworkX graph G_filtered from filtered data...\")\n",
    "        G_filtered = nx.from_pandas_edgelist(\n",
    "            collab_df_net,\n",
    "            'channel_id_1',\n",
    "            'channel_id_2',\n",
    "            edge_attr=['collaboration_count', 'total_collaboration_duration_seconds', 'latest_collaboration_timestamp']\n",
    "        )\n",
    "        G_filtered.add_nodes_from(valid_channel_ids_for_network)\n",
    "        print(f\"Filtered graph G_filtered created with {G_filtered.number_of_nodes()} nodes and {G_filtered.number_of_edges()} edges.\")\n",
    "    else:\n",
    "        print(\"No edges passed all filters. Filtered graph G_filtered will be empty or contain only isolated nodes.\")\n",
    "        if valid_channel_ids_for_network: G_filtered.add_nodes_from(valid_channel_ids_for_network)\n",
    "        print(f\"Filtered graph G_filtered has {G_filtered.number_of_nodes()} nodes and {G_filtered.number_of_edges()} edges.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or filtering data for network analysis: {e}\")\n",
    "    logging.error(\"Error in network data prep:\", exc_info=True)\n",
    "    if 'G_filtered' not in locals(): G_filtered = nx.Graph()\n",
    "\n",
    "print(\"--- Network Data Preparation Complete ---\")"
   ],
   "id": "e24a067f29d07663",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 13: Display Sample Filtered Collaboration Data\n",
    "print(\"\\nSample Filtered Collaboration Edges (collab_df_net):\")\n",
    "if not collab_df_net.empty:\n",
    "    display(collab_df_net.head())\n",
    "else:\n",
    "    print(\"No collaboration data in collab_df_net (all edges filtered out or none exist).\")\n"
   ],
   "id": "503c6eba174063ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 14: Filtered Collaboration Network Statistics\n",
    "if 'G_filtered' in locals() and G_filtered.number_of_nodes() > 0:\n",
    "    print(\"\\n--- Filtered Collaboration Network Statistics ---\")\n",
    "    print(f\"Total Edges in Filtered Network: {G_filtered.number_of_edges()}\")\n",
    "    print(f\"Total Nodes in Filtered Network: {G_filtered.number_of_nodes()}\")\n",
    "\n",
    "    if G_filtered.number_of_nodes() > 0:\n",
    "        degree_sequence = [d for n, d in G_filtered.degree()]\n",
    "        degree_df_filtered = pd.DataFrame({'channel_id': list(G_filtered.nodes()), 'degree': degree_sequence})\n",
    "\n",
    "        print(\"\\nDegree Distribution Summary (Filtered Network):\")\n",
    "        print(degree_df_filtered['degree'].describe())\n",
    "\n",
    "        # Merge with channel names for context\n",
    "        channels_for_labels_df = channels_df[['id', 'login', 'display_name']].rename(columns={'id': 'channel_id'})\n",
    "        degree_df_filtered = pd.merge(degree_df_filtered, channels_for_labels_df, on='channel_id', how='left')\n",
    "\n",
    "        print(\"\\nTop 10 Channels by Degree (Filtered Network):\")\n",
    "        print(degree_df_filtered.nlargest(10, 'degree'))\n",
    "    else:\n",
    "        print(\"Filtered graph has no nodes to calculate degree.\")\n",
    "\n",
    "    # Summary for edges in collab_df_net\n",
    "    if not collab_df_net.empty:\n",
    "        print(\"\\nCollaboration Count per Edge Summary (Filtered Network):\")\n",
    "        print(collab_df_net['collaboration_count'].describe())\n",
    "\n",
    "        # --- Human-Readable Duration Summary ---\n",
    "        print(\"\\nTotal Collaboration Duration per Edge Summary (Filtered Network):\")\n",
    "        duration_stats = collab_df_net['total_collaboration_duration_seconds'].describe()\n",
    "\n",
    "        # Create and print a formatted summary\n",
    "        formatted_stats = {\n",
    "            'count': f\"{duration_stats['count']:.0f}\",\n",
    "            'mean': format_seconds_to_hm(duration_stats['mean']),\n",
    "            'std dev': f\"~{format_seconds_to_hm(duration_stats['std'])}\",\n",
    "            'min': format_seconds_to_hm(duration_stats['min']),\n",
    "            '25%': format_seconds_to_hm(duration_stats['25%']),\n",
    "            '50% (median)': format_seconds_to_hm(duration_stats['50%']),\n",
    "            '75%': format_seconds_to_hm(duration_stats['75%']),\n",
    "            'max': format_seconds_to_hm(duration_stats['max'])\n",
    "        }\n",
    "\n",
    "        for key, value in formatted_stats.items():\n",
    "            # Left-align the key, right-align the value for clean output\n",
    "            print(f\"{key:<15} {value:>15}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No edges in collab_df_net to summarize.\")\n",
    "else:\n",
    "    print(\"\\nFiltered graph G_filtered is empty. No statistics to display.\")"
   ],
   "id": "dc914e19f65da2d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 15: Filtered Collaboration Network Visualizations\n",
    "if 'G_filtered' in locals() and G_filtered.number_of_nodes() > 0 and 'degree_df_filtered' in locals() and not degree_df_filtered.empty:\n",
    "    print(\"\\n--- Filtered Collaboration Network Visualizations ---\")\n",
    "\n",
    "    # --- 1. Degree Distribution Histogram ---\n",
    "    print(\"Generating Degree Distribution Histogram for the filtered network...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    degrees_filtered = degree_df_filtered[degree_df_filtered['degree'] > 0]['degree']\n",
    "    if not degrees_filtered.empty:\n",
    "        # Log-log scale is common for degree distributions to check for power-law behavior\n",
    "        sns.histplot(degrees_filtered, log_scale=True, bins=30)\n",
    "        plt.title('Degree Distribution of Filtered Collaboration Network (Log-Log Scale)')\n",
    "        plt.xlabel('Degree (Number of Unique Collaborators) - Log Scale')\n",
    "        plt.ylabel('Number of Channels - Log Scale')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No nodes with degree > 0 found to plot.\")\n",
    "\n",
    "    # --- 2. Edge Weight (Collaboration Count) Plot ---\n",
    "    if not collab_df_net.empty:\n",
    "        print(\"\\nGenerating Edge Weight (Collaboration Count) Plot...\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        collab_counts_filtered = collab_df_net[collab_df_net['collaboration_count'] > 0]['collaboration_count']\n",
    "\n",
    "        if not collab_counts_filtered.empty:\n",
    "            # A countplot is more direct and robust for this type of discrete integer data\n",
    "            ax = sns.countplot(x=collab_counts_filtered, palette='viridis', order = sorted(collab_counts_filtered.unique()))\n",
    "\n",
    "            # Add text labels on top of each bar\n",
    "            for p in ax.patches:\n",
    "                ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                            ha='center', va='center', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "            plt.title('Distribution of Collaboration Counts per Edge (Filtered Network)')\n",
    "            plt.xlabel('Number of Collaborations on a Single Edge')\n",
    "\n",
    "            # Conditionally apply log scale to y-axis only if counts are high\n",
    "            max_count = collab_counts_filtered.value_counts().max()\n",
    "            if max_count > 10:\n",
    "                plt.yscale('log')\n",
    "                plt.ylabel('Number of Edges (Log Scale)')\n",
    "            else:\n",
    "                plt.ylabel('Number of Edges (Linear Scale)')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No collaborations with count > 0 found to plot.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nFiltered graph or degree data insufficient for visualization. Skipping.\")"
   ],
   "id": "2c402ce1f749e696",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Community Detection in Collaboration Network",
   "id": "a74cb851323c91af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 16: Community Detection Setup and Execution\n",
    "import community as community_louvain  # python-louvain library\n",
    "\n",
    "communities_detected = False\n",
    "partition = {}\n",
    "communities = {}\n",
    "\n",
    "if 'G_filtered' in locals() and G_filtered.number_of_nodes() > 0:\n",
    "    print(\n",
    "        f\"\\n--- Performing Louvain Community Detection on FILTERED graph at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "    print(f\"Using 'collaboration_count' as edge weight for community detection.\")\n",
    "\n",
    "    for u, v, data in G_filtered.edges(data=True):\n",
    "        if 'weight' not in data:\n",
    "            G_filtered.edges[u, v]['weight'] = data.get('collaboration_count', 1)\n",
    "\n",
    "    start_time_community = time.time()\n",
    "    try:\n",
    "        partition = community_louvain.best_partition(G_filtered, weight='weight', random_state=42)\n",
    "        modularity = community_louvain.modularity(partition, G_filtered, weight='weight')\n",
    "\n",
    "        for node, community_id in partition.items():\n",
    "            if community_id not in communities: communities[community_id] = []\n",
    "            communities[community_id].append(node)\n",
    "\n",
    "        num_communities = len(communities)\n",
    "        print(f\"Louvain Community Detection complete in {time.time() - start_time_community:.2f}s.\")\n",
    "        print(f\"Found {num_communities} communities.\")\n",
    "        print(f\"Modularity of the partition: {modularity:.4f}\")\n",
    "\n",
    "        sorted_communities_by_size = sorted(communities.items(), key=lambda item: len(item[1]), reverse=True)\n",
    "        print(\"\\nTop 5 Largest Communities:\")\n",
    "        for i in range(min(5, len(sorted_communities_by_size))):\n",
    "            cid, nodes = sorted_communities_by_size[i]\n",
    "            member_names = [\n",
    "                channels_for_labels_df.loc[channels_for_labels_df['channel_id'] == node_id, 'login'].iloc[0]\n",
    "                for node_id in nodes[:3]\n",
    "                if not channels_for_labels_df[channels_for_labels_df['channel_id'] == node_id].empty\n",
    "            ]\n",
    "            print(\n",
    "                f\"  Community {cid} (Size rank {i + 1}): {len(nodes)} members. Examples: {', '.join(member_names[:3])}...\")\n",
    "        communities_detected = True\n",
    "    except Exception as e:\n",
    "        print(f\"Error during community detection: {e}\")\n",
    "else:\n",
    "    print(\"\\nFiltered graph G_filtered not found or is empty. Skipping community detection.\")\n"
   ],
   "id": "23b293ec0907b0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualizing Communities",
   "id": "cd49e319fa29ca8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell 17: Visualize Subgraph with Community Colors\n",
    "if communities_detected and 'G_filtered' in locals() and G_filtered.number_of_nodes() > 0:\n",
    "    print(\"\\n--- NetworkX Subgraph Visualization with Community Colors ---\")\n",
    "    try:\n",
    "        # Use the same subgraph logic as before to select nodes for visualization\n",
    "        top_nodes_filtered = degree_df_filtered.nlargest(config.NETWORK_VIZ_TOP_N_CHANNELS_BY_DEGREE, 'degree')['channel_id'].tolist()\n",
    "        subgraph_nodes_set = set(top_nodes_filtered)\n",
    "        max_subgraph_nodes = config.NETWORK_VIZ_MAX_SUBGRAPH_NODES\n",
    "\n",
    "        for node in list(subgraph_nodes_set):\n",
    "            if G_filtered.has_node(node) and len(subgraph_nodes_set) < max_subgraph_nodes:\n",
    "                neighbors = list(G_filtered.neighbors(node))\n",
    "                needed = max_subgraph_nodes - len(subgraph_nodes_set)\n",
    "                subgraph_nodes_set.update(neighbors[:needed])\n",
    "\n",
    "        subgraph = G_filtered.subgraph(list(subgraph_nodes_set))\n",
    "        print(f\"Creating subgraph visualization with {subgraph.number_of_nodes()} nodes and {subgraph.number_of_edges()} edges, clustered by community.\")\n",
    "\n",
    "        if subgraph.number_of_nodes() > 0:\n",
    "            # --- NEW: Create a temporary graph for community-aware layout ---\n",
    "            # We will modify edge weights in this copy to influence the layout\n",
    "            layout_graph = subgraph.copy()\n",
    "\n",
    "            # Increase weight for intra-community edges, decrease for inter-community\n",
    "            # This makes the \"springs\" inside a community much stronger.\n",
    "            for u, v in layout_graph.edges():\n",
    "                if partition.get(u) == partition.get(v):\n",
    "                    # Strengthen connection if nodes are in the same community\n",
    "                    layout_graph.edges[u,v]['weight'] = 5  # High weight for strong attraction\n",
    "                else:\n",
    "                    # Weaken connection if nodes are in different communities\n",
    "                    layout_graph.edges[u,v]['weight'] = 0.1 # Low weight for weak attraction\n",
    "\n",
    "            # --- Calculate positions using the modified layout_graph ---\n",
    "            pos_subgraph = nx.spring_layout(layout_graph, weight='weight', k=0.4, iterations=50, seed=42)\n",
    "\n",
    "            # --- Visualization logic remains mostly the same, but uses the new positions ---\n",
    "\n",
    "            # Get community colors for nodes in the subgraph\n",
    "            unique_community_ids_in_subgraph = sorted(list(set(partition[node] for node in subgraph.nodes() if node in partition)))\n",
    "            community_to_color_idx = {comm_id: i for i, comm_id in enumerate(unique_community_ids_in_subgraph)}\n",
    "            num_distinct_colors_needed = len(unique_community_ids_in_subgraph)\n",
    "\n",
    "            node_colors_subgraph = 'grey'\n",
    "            if num_distinct_colors_needed > 0:\n",
    "                cmap = plt.cm.get_cmap('tab20', num_distinct_colors_needed) if num_distinct_colors_needed <= 20 else plt.cm.get_cmap('viridis', num_distinct_colors_needed)\n",
    "                node_colors_subgraph = [cmap(community_to_color_idx.get(partition.get(node), -1)) for node in subgraph.nodes()]\n",
    "\n",
    "            # Get node sizes and labels based on the original data\n",
    "            node_data_subgraph = channels_df[channels_df['id'].isin(subgraph.nodes())].set_index('id')\n",
    "            subgraph_node_sizes = [math.log10(max(1, node_data_subgraph.loc[node, 'follower_count'] if node in node_data_subgraph.index else 1)+1) * 300 + 100 for node in subgraph.nodes()]\n",
    "            node_labels_subgraph = pd.merge(pd.DataFrame({'channel_id': list(subgraph.nodes())}), channels_for_labels_df, on='channel_id', how='left').set_index('channel_id')['login'].to_dict()\n",
    "\n",
    "            # Use original collaboration count for visual edge thickness\n",
    "            edge_widths_subgraph = [math.log10(max(0, d.get('collaboration_count',0)) + 1) * 1.5 + 0.5 for u, v, d in subgraph.edges(data=True)]\n",
    "\n",
    "            # --- Drawing ---\n",
    "            plt.figure(figsize=(18, 15))\n",
    "\n",
    "            # Draw using the original subgraph data but the new positions (pos_subgraph)\n",
    "            nx.draw_networkx_nodes(subgraph, pos_subgraph, node_size=subgraph_node_sizes, node_color=node_colors_subgraph, alpha=0.9)\n",
    "            nx.draw_networkx_edges(subgraph, pos_subgraph, width=edge_widths_subgraph, alpha=0.2, edge_color='gray')\n",
    "            nx.draw_networkx_labels(subgraph, pos_subgraph, labels=node_labels_subgraph, font_size=8)\n",
    "\n",
    "            plt.title(f'Collaboration Subgraph with Community Colors (Clustered Layout)', fontsize=16)\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Subgraph is empty, cannot visualize communities.\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"\\n`python-louvain` (community) library not found. Skipping community detection visualization.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during community visualization: {e}\")\n",
    "        logging.error(\"Community visualization error\", exc_info=True)\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo communities detected or filtered graph is empty. Skipping community visualization.\")"
   ],
   "id": "d9ddeff2aec48950",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Collaboration Topic Modeling with BERTopic\n",
    "\n",
    "Here, we derive context by analyzing the content of video titles. We use BERTopic to automatically discover topics from the titles of videos belonging to channels in our filtered collaboration network. This helps us understand what collaborating streamers talk about or play, without relying on pre-defined categories.\n"
   ],
   "id": "6e24edeb30abcc84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell: BERTopic Pre-processing: Assign Topic ID to All Relevant Videos\n",
    "\n",
    "# This cell trains the BERTopic model on a large sample of video titles\n",
    "# and then assigns a topic ID to each video in a new DataFrame.\n",
    "# This is a time-consuming step that enables fast context lookups later.\n",
    "\n",
    "# Initialize global-like variables to hold the results\n",
    "topic_model = None\n",
    "videos_with_topics_df = None\n",
    "\n",
    "print(\"\\n--- Starting Collaboration Topic Modeling with BERTopic ---\")\n",
    "\n",
    "# --- 1. Main Gate: Check if we have a filtered graph to work with ---\n",
    "if 'G_filtered' not in locals() or G_filtered.number_of_nodes() == 0:\n",
    "    print(\"\\nFiltered graph G_filtered not found or is empty. Skipping topic modeling.\")\n",
    "\n",
    "else:\n",
    "    # --- 2. If graph exists, proceed with data preparation and modeling ---\n",
    "    try:\n",
    "        from bertopic import BERTopic\n",
    "        import nltk\n",
    "        from nltk.corpus import stopwords\n",
    "        import re\n",
    "\n",
    "        # Setup NLTK stopwords\n",
    "        try:\n",
    "            stopwords.words('english')\n",
    "        except LookupError:\n",
    "            print(\"NLTK stopwords not found. Downloading...\")\n",
    "            nltk.download('stopwords')\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        mention_regex = re.compile(r'@([a-zA-Z0-9_]{4,25})')\n",
    "\n",
    "        def clean_title(title):\n",
    "            if not isinstance(title, str): return \"\"\n",
    "            text = mention_regex.sub('', title)\n",
    "            text = re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n",
    "            text = text.lower()\n",
    "            text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "            return text\n",
    "\n",
    "        print(\"Preparing data for topic modeling...\")\n",
    "        nodes_in_network = list(G_filtered.nodes())\n",
    "        videos_for_modeling_df = videos_df[videos_df['channel_id'].isin(nodes_in_network)].copy()\n",
    "        print(f\"Found {len(videos_for_modeling_df)} videos from {len(nodes_in_network)} channels in the filtered network.\")\n",
    "\n",
    "        print(f\"Filtering for English language videos...\")\n",
    "        english_videos_df = videos_for_modeling_df[videos_for_modeling_df['language'] == 'en'].copy()\n",
    "        print(f\" -> Found {len(english_videos_df)} English videos to model.\")\n",
    "\n",
    "        print(\"Cleaning video titles (removing stopwords and mentions)...\")\n",
    "        english_videos_df['cleaned_title'] = english_videos_df['title'].dropna().apply(clean_title)\n",
    "\n",
    "        docs_df = english_videos_df[english_videos_df['cleaned_title'].str.len() > 0].copy()\n",
    "        docs = docs_df['cleaned_title'].tolist()\n",
    "\n",
    "        # --- 3. Check if we have enough documents to proceed ---\n",
    "        if len(docs) < 50:\n",
    "            print(f\"Not enough video titles found ({len(docs)}) after cleaning to perform topic modeling.\")\n",
    "\n",
    "        else:\n",
    "            # --- 4. This is where the actual modeling happens ---\n",
    "            MAX_DOCS_FOR_MODELING = 50000\n",
    "            if len(docs) > MAX_DOCS_FOR_MODELING:\n",
    "                print(f\"Sampling {MAX_DOCS_FOR_MODELING} titles for topic modeling to ensure performance...\")\n",
    "                docs_df = docs_df.sample(n=MAX_DOCS_FOR_MODELING, random_state=42)\n",
    "                docs = docs_df['cleaned_title'].tolist()\n",
    "\n",
    "            print(f\"Training BERTopic model on {len(docs)} cleaned video titles. This may take several minutes...\")\n",
    "            print(\"(The first run will download pre-trained language models).\")\n",
    "\n",
    "            topic_model = BERTopic(\n",
    "                min_topic_size=20,\n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "            topics, _ = topic_model.fit_transform(docs)\n",
    "\n",
    "            print(\"Mapping topic results back to videos...\")\n",
    "            docs_df['topic_id'] = topics\n",
    "            videos_with_topics_df = docs_df # Assign to the main variable for other cells to use\n",
    "\n",
    "            print(\"\\n--- BERTopic Analysis Complete ---\")\n",
    "\n",
    "            print(\"\\nTop discovered topics from cleaned titles:\")\n",
    "            display(topic_model.get_topic_info())\n",
    "\n",
    "            print(\"\\nVisualizing top topics (barchart):\")\n",
    "            display(topic_model.visualize_barchart(top_n_topics=10))\n",
    "\n",
    "            print(\"\\nVisualizing inter-topic distance map:\")\n",
    "            display(topic_model.visualize_topics())\n",
    "\n",
    "    # --- 5. Handle potential errors for the entire block ---\n",
    "    except ImportError:\n",
    "        print(\"\\nCould not perform topic modeling. Please install required libraries:\")\n",
    "        print(\"pip install bertopic sentence-transformers scikit-learn nltk\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during topic modeling: {e}\")\n",
    "        logging.error(\"BERTopic analysis failed\", exc_info=True)"
   ],
   "id": "e4b6a8bbb9db61d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell: Diagnostic for BERTopic Results\n",
    "\n",
    "if 'topic_model' in locals() and topic_model is not None:\n",
    "    print(\"--- BERTopic Model Diagnostics ---\")\n",
    "\n",
    "    # Get the overview of topics\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "\n",
    "    # Check the size of the outlier topic (-1)\n",
    "    outlier_info = topic_info[topic_info['Topic'] == -1]\n",
    "    if not outlier_info.empty:\n",
    "        total_docs = topic_info['Count'].sum()\n",
    "        outlier_count = outlier_info['Count'].iloc[0]\n",
    "        outlier_percentage = (outlier_count / total_docs) * 100\n",
    "        print(f\"\\nOutlier Topic (-1) contains {outlier_count} videos ({outlier_percentage:.1f}% of the total).\")\n",
    "\n",
    "    print(\"\\nOverview of the top 10 most frequent topics (excluding outliers):\")\n",
    "    display(topic_info[topic_info['Topic'] != -1].head(10))\n",
    "\n",
    "else:\n",
    "    print(\"BERTopic model has not been trained yet.\")"
   ],
   "id": "73f4c30a2859734c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Interactive Collaboration Network Snippet (Searchable)\n",
    "\n",
    "Type part of a channel name (login or display name) and click \"Search & Visualize\" to see its immediate collaboration network.\n",
    "\n",
    "- If multiple channels match your search, select the specific one from the second dropdown.\n",
    "- Node size is proportional to the channel's total follower count (log scale).\n",
    "- Edge thickness is proportional to the number of collaboration instances found (log scale).\n",
    "- Edge labels show the most frequent game category for the collaboration and its percentage.\n",
    "\n",
    "*(Note: Embedding channel profile pictures directly within nodes is complex with this plotting method and is omitted.)*"
   ],
   "id": "cba11b95edda95a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell: Setup for Searchable Interactive Visualization\n",
    "from ipywidgets import Text, Button, Dropdown, Output, VBox, HBox, Layout\n",
    "from IPython.display import display\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# This variable will hold the ID of the last channel successfully visualized\n",
    "# so we can re-plot when the degree of separation is changed.\n",
    "currently_visualized_channel_id = None\n",
    "\n",
    "# --- Ensure data is loaded from previous cells ---\n",
    "# This check ensures that if you only run the bottom part of the notebook, data is available.\n",
    "if 'channels_df' not in locals() or channels_df.empty:\n",
    "    print(\"Warning: Full 'channels_df' not loaded. Reloading for search widget...\")\n",
    "    try:\n",
    "        channels_df = pd.read_sql_query(\"SELECT id, login, display_name, follower_count FROM Channels\", db_conn)\n",
    "        channels_df['follower_count'] = pd.to_numeric(channels_df['follower_count'], errors='coerce').fillna(1)\n",
    "        channels_df['display_name'] = channels_df['display_name'].fillna(channels_df['login'])\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to reload channel data: {e}\")\n",
    "        channels_df = pd.DataFrame(columns=['id', 'login', 'display_name', 'follower_count'])\n",
    "\n",
    "if 'collab_df_full' not in locals() or collab_df_full.empty:\n",
    "    print(\"Warning: Full collaboration data not loaded. Reloading...\")\n",
    "    try:\n",
    "        collab_df_full = pd.read_sql_query(\"SELECT * FROM Collaborations\", db_conn)\n",
    "        collab_df_full['collaboration_count'] = pd.to_numeric(collab_df_full['collaboration_count'], errors='coerce').fillna(0).astype(int)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to reload collaboration data: {e}\")\n",
    "        collab_df_full = pd.DataFrame(columns=['channel_id_1', 'channel_id_2', 'collaboration_count'])\n",
    "\n",
    "if 'videos_with_topics_df' not in locals():\n",
    "     print(\"Warning: BERTopic data ('videos_with_topics_df') not found. Edge labels will be empty.\")\n",
    "     videos_with_topics_df = None # Ensure variable exists to prevent errors\n",
    "\n",
    "# --- Create Widgets ---\n",
    "search_input = Text(\n",
    "    description=\"Channel Name:\",\n",
    "    placeholder=\"Enter login or display name (min 3 chars)\",\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "search_button = Button(\n",
    "    description=\"Search\",\n",
    "    button_style='info',\n",
    "    tooltip='Search for the channel and display its network',\n",
    ")\n",
    "\n",
    "# --- NEW: Dropdown for degrees of separation ---\n",
    "degree_selector = Dropdown(\n",
    "    options=[\n",
    "        ('1 (Direct Neighbors)', 1),\n",
    "        ('2 (Neighbors of Neighbors)', 2),\n",
    "        ('3', 3),\n",
    "        ('4', 4)\n",
    "    ],\n",
    "    value=1, # Default to 1 degree\n",
    "    description='Degrees:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "results_dropdown = Dropdown(\n",
    "    description=\"Select Match:\",\n",
    "    options=[(\"---\", None)],\n",
    "    disabled=True,\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': 'max-content'}\n",
    ")\n",
    "\n",
    "message_output = Output(layout={'margin': '10px 0 0 0'})\n",
    "plot_output = Output()"
   ],
   "id": "b9c3ac68400abf02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell: Helper function for BERTopic Context\n",
    "\n",
    "def get_shared_topic_context(channel_id_A, channel_id_B, videos_with_topics, topic_model, top_n=1):\n",
    "    \"\"\"\n",
    "    Infers a shared context between two channels based on the dominant topics\n",
    "    of their video titles, using the pre-generated CustomName or keywords.\n",
    "    \"\"\"\n",
    "    # Check if the BERTopic model and results are available\n",
    "    if videos_with_topics is None or topic_model is None:\n",
    "        return \"\" # Not ready yet\n",
    "\n",
    "    try:\n",
    "        # Get topics for each channel's videos from the pre-processed DataFrame\n",
    "        videos_A = videos_with_topics[videos_with_topics['channel_id'] == channel_id_A]\n",
    "        videos_B = videos_with_topics[videos_with_topics['channel_id'] == channel_id_B]\n",
    "\n",
    "        if videos_A.empty or videos_B.empty:\n",
    "            return \"\" # Not enough video data for one of the channels\n",
    "\n",
    "        # Get the frequency of each topic for each channel (ignoring outlier topic -1)\n",
    "        freq_A = videos_A[videos_A['topic_id'] != -1]['topic_id'].value_counts(normalize=True)\n",
    "        freq_B = videos_B[videos_B['topic_id'] != -1]['topic_id'].value_counts(normalize=True)\n",
    "\n",
    "        if freq_A.empty or freq_B.empty:\n",
    "            return \"\" # No non-outlier topics found\n",
    "\n",
    "        # Find common topics and calculate a simple \"shared interest\" score by multiplying their frequencies\n",
    "        common_topics = freq_A.index.intersection(freq_B.index)\n",
    "\n",
    "        if len(common_topics) == 0:\n",
    "            return \"\" # No topics in common\n",
    "\n",
    "        shared_scores = freq_A[common_topics] * freq_B[common_topics]\n",
    "\n",
    "        # Get the top N shared topics based on this score\n",
    "        top_shared_topic_ids = shared_scores.nlargest(top_n).index\n",
    "\n",
    "        labels = []\n",
    "        for topic_id in top_shared_topic_ids:\n",
    "            # Check if a CustomName was generated by an LLM\n",
    "            if 'CustomName' in videos_with_topics.columns:\n",
    "                topic_name_row = videos_with_topics[videos_with_topics['topic_id'] == topic_id]\n",
    "                if not topic_name_row.empty:\n",
    "                    custom_name = topic_name_row['CustomName'].iloc[0]\n",
    "                    # Check if the custom name is valid and not the default\n",
    "                    if pd.notna(custom_name) and not custom_name.lower().startswith(\"topic\"):\n",
    "                        labels.append(custom_name)\n",
    "                        continue # Move to next topic if we found a good custom name\n",
    "\n",
    "            # Fallback to using keywords if no valid CustomName exists\n",
    "            topic_words = topic_model.get_topic(topic_id)\n",
    "            if topic_words:\n",
    "                # Take the first 3 keywords and join them for a concise label\n",
    "                label = \", \".join([word for word, prob in topic_words[:3]])\n",
    "                labels.append(label)\n",
    "\n",
    "        return \"\\n\".join(labels) if labels else \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error getting shared topic for {channel_id_A}-{channel_id_B}: {e}\")\n",
    "        return \"\" # Return empty string on any error\n",
    "\n",
    "print(\"Helper function 'get_shared_topic_context' is now defined.\")"
   ],
   "id": "b7811adf9cc66f5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell: Visualization Function for Interactive Widget (Corrected)\n",
    "\n",
    "def visualize_channel_neighborhood(selected_channel_id, degrees):\n",
    "    \"\"\"\n",
    "    Queries data and draws the network neighborhood for the selected channel\n",
    "    up to a specified degree of separation.\n",
    "    \"\"\"\n",
    "    global currently_visualized_channel_id # Use the global-like variable\n",
    "    with plot_output:\n",
    "        plot_output.clear_output(wait=True);\n",
    "        with message_output: message_output.clear_output()\n",
    "        if not selected_channel_id: return\n",
    "\n",
    "        selected_channel_row = channels_df[channels_df['id'] == selected_channel_id]\n",
    "        if selected_channel_row.empty:\n",
    "            with message_output: print(f\"Selected channel ID {selected_channel_id} not found in loaded channel data.\")\n",
    "            return\n",
    "        selected_channel_name_for_title = selected_channel_row['display_name'].iloc[0]\n",
    "\n",
    "        print(f\"Generating {degrees}-degree network for: {selected_channel_name_for_title}...\")\n",
    "\n",
    "        # Build a full graph object from the complete collaboration data to traverse it\n",
    "        global G_full_for_viz\n",
    "        if 'G_full_for_viz' not in globals() or not isinstance(G_full_for_viz, nx.Graph):\n",
    "            print(\" -> Creating full collaboration graph for traversal (one-time)...\")\n",
    "            G_full_for_viz = nx.from_pandas_edgelist(\n",
    "                collab_df_full, 'channel_id_1', 'channel_id_2',\n",
    "                edge_attr=['collaboration_count']\n",
    "            )\n",
    "\n",
    "        if not G_full_for_viz.has_node(selected_channel_id):\n",
    "            with message_output: print(f\"No collaboration data found for channel: {selected_channel_name_for_title}\")\n",
    "            return\n",
    "\n",
    "        # Create the subgraph using the specified radius (degrees)\n",
    "        subgraph = nx.ego_graph(G_full_for_viz, selected_channel_id, radius=degrees)\n",
    "\n",
    "        # Performance Warning for large subgraphs\n",
    "        if subgraph.number_of_nodes() > config.NETWORK_VIZ_MAX_SUBGRAPH_NODES:\n",
    "            with message_output:\n",
    "                display(HTML(f\"<p style='color: orange;'><b>Warning:</b> The {degrees}-degree neighborhood has {subgraph.number_of_nodes()} nodes, which is more than the display limit of {config.NETWORK_VIZ_MAX_SUBGRAPH_NODES}. The graph may be slow and hard to read.</p>\"))\n",
    "\n",
    "        # --- Visualization logic ---\n",
    "        # Get data for nodes in the subgraph\n",
    "        node_data_viz = channels_df[channels_df['id'].isin(list(subgraph.nodes()))].set_index('id')\n",
    "\n",
    "        missing_nodes = [n for n in list(subgraph.nodes()) if n not in node_data_viz.index]\n",
    "        if missing_nodes:\n",
    "            print(f\"Warning: Missing channel data for nodes: {missing_nodes}\")\n",
    "            dummy_data = pd.DataFrame({'follower_count': 1, 'display_name': 'UNKNOWN'}, index=missing_nodes)\n",
    "            node_data_viz = pd.concat([node_data_viz, dummy_data])\n",
    "\n",
    "        # Calculate visual properties using the correct `subgraph` variable\n",
    "        node_sizes_viz = [math.log10(max(1, node_data_viz.loc[node].get('follower_count', 1)) + 1) * 200 + 150 for node in subgraph.nodes()]\n",
    "        node_labels_viz = {node: node_data_viz.loc[node, 'display_name'] for node in subgraph.nodes()}\n",
    "        edge_widths_viz = [math.log10(max(0, d.get('weight', 0)) + 1) * 1.5 + 0.5 for u, v, d in subgraph.edges(data=True)]\n",
    "\n",
    "        edge_labels_viz = {}\n",
    "        print(\"Deriving edge context from BERTopic model...\")\n",
    "        for u, v in subgraph.edges():\n",
    "            shared_topic_label = get_shared_topic_context(\n",
    "                u, v,\n",
    "                videos_with_topics=videos_with_topics_df,\n",
    "                topic_model=topic_model\n",
    "            )\n",
    "            edge_labels_viz[(u, v)] = shared_topic_label\n",
    "\n",
    "        # --- Plotting ---\n",
    "        plt.figure(figsize=(16, 12));\n",
    "        pos_viz = nx.spring_layout(subgraph, k=0.5, iterations=50, seed=42)\n",
    "\n",
    "        # Use `subgraph` for all drawing functions\n",
    "        nx.draw_networkx_nodes(subgraph, pos_viz, node_size=node_sizes_viz, node_color='skyblue', alpha=0.8)\n",
    "        nx.draw_networkx_edges(subgraph, pos_viz, width=edge_widths_viz, alpha=0.4, edge_color='gray')\n",
    "        nx.draw_networkx_labels(subgraph, pos_viz, labels=node_labels_viz, font_size=9)\n",
    "        nx.draw_networkx_edge_labels(subgraph, pos_viz, edge_labels=edge_labels_viz, font_size=7, font_color='darkgreen',\n",
    "                                     bbox=dict(facecolor='white', alpha=0.4, edgecolor='none', boxstyle='round,pad=0.2'))\n",
    "\n",
    "        plt.title(f\"{degrees}-Degree Collaboration Network for: {selected_channel_name_for_title}\", fontsize=16);\n",
    "        plt.axis('off');\n",
    "        plt.tight_layout();\n",
    "        plt.show()\n",
    "\n",
    "        # When visualization is successful, store the channel ID\n",
    "        currently_visualized_channel_id = selected_channel_id\n",
    "        print(\"Done.\")"
   ],
   "id": "468062d3efeafd49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell: Search and Selection Logic (Updated for Degrees of Separation)\n",
    "\n",
    "def handle_search_click(b):\n",
    "    \"\"\"Function called when the search button is clicked.\"\"\"\n",
    "    global currently_visualized_channel_id\n",
    "    search_term = search_input.value.strip()\n",
    "    # Reset state\n",
    "    results_dropdown.options = [(\"---\", None)];\n",
    "    results_dropdown.value = None;\n",
    "    results_dropdown.disabled = True\n",
    "    with message_output: message_output.clear_output()\n",
    "    with plot_output: plot_output.clear_output()\n",
    "    currently_visualized_channel_id = None\n",
    "\n",
    "    if len(search_term) < 3:\n",
    "        with message_output: print(\"Please enter at least 3 characters to search.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Searching for channels matching: '{search_term}'...\")\n",
    "    search_pattern = f\"%{search_term.lower()}%\"\n",
    "    cursor = db_conn.cursor()\n",
    "    # Updated SQL to sort by follower count, showing more relevant results first\n",
    "    sql = \"SELECT id, login, display_name FROM Channels WHERE LOWER(login) LIKE ? OR LOWER(display_name) LIKE ? ORDER BY follower_count DESC LIMIT 50\"\n",
    "    try:\n",
    "        cursor.execute(sql, (search_pattern, search_pattern));\n",
    "        matches = cursor.fetchall()\n",
    "    except sqlite3.Error as e:\n",
    "        with message_output: print(f\"Database search error: {e}\"); return\n",
    "\n",
    "    if not matches:\n",
    "        with message_output: print(f\"No channels found matching '{search_term}'.\")\n",
    "    elif len(matches) == 1:\n",
    "        match = matches[0];\n",
    "        channel_id = match['id']\n",
    "        with message_output: print(f\"Found 1 match: {match['display_name']} ({match['login']}). Visualizing...\")\n",
    "        visualize_channel_neighborhood(channel_id, degree_selector.value)\n",
    "    else:\n",
    "        match_options = [(\"--- Select a Match ---\", None)] + [(f\"{row['display_name']} ({row['login']})\", row['id']) for row in matches]\n",
    "        results_dropdown.options = match_options;\n",
    "        results_dropdown.disabled = False\n",
    "        with message_output: print(f\"Found {len(matches)} matches. Please select one from the dropdown below.\")\n",
    "\n",
    "\n",
    "def handle_match_selection(change):\n",
    "    \"\"\"Function called when a channel is selected from the results dropdown.\"\"\"\n",
    "    selected_id = change.get('new')\n",
    "    if selected_id:\n",
    "        visualize_channel_neighborhood(selected_id, degree_selector.value)\n",
    "    else:\n",
    "        with plot_output: plot_output.clear_output()\n",
    "\n",
    "# --- NEW: Handler for changing the degree of separation ---\n",
    "def handle_degree_change(change):\n",
    "    \"\"\"Function called when the degree selector dropdown changes.\"\"\"\n",
    "    new_degree = change.get('new')\n",
    "    # If a channel is already selected/visualized, re-run the visualization with the new degree\n",
    "    if currently_visualized_channel_id:\n",
    "        visualize_channel_neighborhood(currently_visualized_channel_id, new_degree)\n",
    "\n",
    "# Link events to handlers\n",
    "search_button.on_click(handle_search_click)\n",
    "results_dropdown.observe(handle_match_selection, names='value')\n",
    "degree_selector.observe(handle_degree_change, names='value')"
   ],
   "id": "3c41caac0554ba8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Cell: Display Interactive Widgets\n",
    "print(\"\\n--- Displaying Interactive Network Visualization Widget ---\")\n",
    "\n",
    "# Arrange search input and button horizontally\n",
    "search_box = HBox([search_input, search_button])\n",
    "# Arrange degree and results dropdowns horizontally\n",
    "selector_box = HBox([degree_selector, results_dropdown])\n",
    "\n",
    "# Arrange all controls vertically for a clean layout\n",
    "controls = VBox([\n",
    "    search_box,\n",
    "    selector_box,\n",
    "    message_output,\n",
    "    plot_output\n",
    "    ], layout=Layout(width='100%'))\n",
    "\n",
    "display(controls)"
   ],
   "id": "4d7e1aebd0860436",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 24: Final Closing Remarks & Cleanup\n",
    "print(f\"\\n--- Notebook Processing Finished at {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "print(\n",
    "    \"You can re-run cells like 'Run Data Collection Cycle', 'Mention Processing Loop', or 'Run Channel Refresh Cycle' to gather more data.\")\n",
    "print(\"Consider closing the database connection manually if you are completely finished.\")\n",
    "# Example manual close (uncomment to run):\n",
    "# if 'db_conn' in locals() and db_conn is not None:\n",
    "#     try:\n",
    "#         db_conn.close()\n",
    "#         print(\"Database connection closed.\")\n",
    "#         db_conn = None # Clear variable\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error closing database connection: {e}\")"
   ],
   "id": "1ad0552c3886910e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
